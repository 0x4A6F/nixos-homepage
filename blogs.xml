<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
	<title>NixOS Planet</title>
	<link>https://planet.nixos.org</link>
	<language>en</language>
	<description>NixOS Planet - https://planet.nixos.org</description>
	<atom:link rel="self" href="https://planet.nixos.org/rss20.xml" type="application/rss+xml"/>

<item>
	<title>Sander van der Burg: On using Nix and Docker as deployment automation solutions: similarities and differences</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-1397115249631682228.post-1542344007221596374</guid>
	<link>http://sandervanderburg.blogspot.com/2020/07/on-using-nix-and-docker-as-deployment.html</link>
	<description>As frequent readers of my blog may probably already know, I have been using &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/01/nix-package-manager.html&quot;&gt;Nix-related&lt;/a&gt; tools for quite some time to solve many of my deployment automation problems.&lt;br /&gt;&lt;br /&gt;Although I have worked in environments in which Nix and its &lt;a href=&quot;https://sandervanderburg.blogspot.com/2016/03/the-nixos-project-and-deploying-systems.html&quot;&gt;related sub projects&lt;/a&gt; are well-known, when I show some of Nix's use cases to larger groups of DevOps-minded people, a frequent answer I that have been hearing is that it looks very similar to &lt;a href=&quot;https://www.docker.com&quot;&gt;Docker&lt;/a&gt;. People also often ask me what advantages Nix has over Docker.&lt;br /&gt;&lt;br /&gt;So far, I have not even covered Docker once on my blog, despite its popularity, including very popular sister projects such as &lt;a href=&quot;https://docs.docker.com/compose&quot;&gt;docker-compose&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;The main reason why I never wrote anything about Docker is not because I do not know about it or how to use it, but simply because I never had any notable use cases that would lead to something publishable -- most of my problems for which Docker could be a solution, I solved it by other means, typically by using a Nix-based solution somewhere in the solution stack.&lt;br /&gt;&lt;br /&gt;Docker is a container-based deployment solution, that was not the first (&lt;a href=&quot;https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016&quot;&gt;neither in the Linux world, nor in the UNIX-world in general&lt;/a&gt;), but since its introduction in 2013 it has grown very rapidly in popularity. I believe its popularity can be mainly attributed to its ease of usage and its extendable images ecosystem: &lt;a href=&quot;https://hub.docker.com&quot;&gt;Docker Hub&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;In fact, Docker (and Kubernetes, a container orchestration solution that incorporates Docker) have become so popular, that they have set a new standard when it comes to organizing systems and automating deployment -- today, in many environments, I have the feeling that it is no longer the question what kind of deployment solution is best for a particular system and organization, but rather: &quot;how do we get it into containers?&quot;.&lt;br /&gt;&lt;br /&gt;The same thing applies to the &quot;&lt;a href=&quot;https://martinfowler.com/articles/microservices.html&quot;&gt;microservices paradigm&lt;/a&gt;&quot; that should facilitate modular systems. If I compare the characteristics of microservices with the definition a &quot;software component&quot; by &lt;a href=&quot;https://dl.acm.org/doi/book/10.5555/515228&quot;&gt;Clemens Szyperski's Component Software book&lt;/a&gt;, then I would argue that they have more in common than they are different.&lt;br /&gt;&lt;br /&gt;One of the reasons why I think microservices are considered to be a success (or at least considered moderately more successful by some over older concepts, such as web services and software components) is because they easily map to a container, that can be conveniently managed with Docker. For some people, a microservice and a Docker container are pretty much the same things.&lt;br /&gt;&lt;br /&gt;Modular software systems have all kinds of advantages, but its biggest disadvantage is that the deployment of a system becomes more complicated as the amount of components and dependencies grow. With Docker containers this problem can be (somewhat) addressed in a convenient way.&lt;br /&gt;&lt;br /&gt;In this blog post, I will provide my view on Nix and Docker -- I will elaborate about some of their key concepts, explain in what ways they are different and similar, and I will show some use-cases in which both solutions can be combined to achieve interesting results.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Application domains&lt;/h2&gt;&lt;br /&gt;Nix and Docker are both deployment solutions for slightly different, but also somewhat overlapping, application domains.&lt;br /&gt;&lt;br /&gt;The Nix package manager (on the recently &lt;a href=&quot;https://nixos.org&quot;&gt;revised homepage&lt;/a&gt;) advertises itself as follows:&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;Nix is a powerful package manager for Linux and other Unix systems that makes package management reliable and reproducible. Share your development and build environments across different machines. &lt;/blockquote&gt;&lt;br /&gt;whereas Docker advertises itself as follows (in the &lt;a href=&quot;https://docs.docker.com/get-started/overview/&quot;&gt;getting started guide&lt;/a&gt;):&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;Docker is an open platform for developing, shipping, and running applications. &lt;/blockquote&gt;&lt;br /&gt;To summarize my interpretations of the descriptions:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Nix's chief responsibility is as its description implies: &lt;strong&gt;package management&lt;/strong&gt; and provides a collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer's operating system in a consistent manner.&lt;br /&gt;&lt;br /&gt;There are two properties that set Nix apart from most other package management solutions. First, Nix is also a &lt;strong&gt;source-based&lt;/strong&gt; package manager -- it can be used as a tool to construct packages from source code and their dependencies, by invoking build scripts in &quot;pure build environments&quot;.&lt;br /&gt;&lt;br /&gt;Moreover, &lt;a href=&quot;https://sandervanderburg.blogspot.com/2012/11/an-alternative-explaination-of-nix.html&quot;&gt;it borrows concepts from purely functional programming languages&lt;/a&gt; to make deployments reproducible, reliable and efficient.&lt;/li&gt;&lt;li&gt;Docker's chief responsibility is much broader than package management -- Docker facilitates full process/service &lt;strong&gt;life-cycle management&lt;/strong&gt;. Package management can be considered to be a sub problem of this domain, as I will explain later in this blog post.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Although both solutions map to slightly different domains, there is one prominent objective that both solutions have in common. They both facilitate &lt;strong&gt;reproducible deployment&lt;/strong&gt;.&lt;br /&gt;&lt;br /&gt;With Nix the goal is that if you build a package from source code and a set of dependencies and perform the same build with the same inputs on a different machine, their build results should be (nearly) bit-identical.&lt;br /&gt;&lt;br /&gt;With Docker, the objective is to facilitate reproducible environments for running applications -- when running an application container on one machine that provides Docker, and running the same application container on another machine, they both should work in an identical way.&lt;br /&gt;&lt;br /&gt;Although both solutions facilitate reproducible deployments, their reproducibility properties are based on different kinds of concepts. I will explain more about them in the next sections.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Nix concepts&lt;/h2&gt;&lt;br /&gt;As explained earlier, Nix is a source-based package manager that borrows concepts from purely functional programming languages. Packages are built from build recipes called Nix expressions, such as:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;with import &amp;lt;nixpkgs&amp;gt; {};&lt;br /&gt;&lt;br /&gt;stdenv.mkDerivation {&lt;br /&gt;  name = &quot;file-5.38&quot;;&lt;br /&gt;&lt;br /&gt;  src = fetchurl {&lt;br /&gt;    url = &quot;ftp://ftp.astron.com/pub/file/file-5.38.tar.gz&quot;;&lt;br /&gt;    sha256 = &quot;0d7s376b4xqymnrsjxi3nsv3f5v89pzfspzml2pcajdk5by2yg2r&quot;;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  buildInputs = [ zlib ];&lt;br /&gt;&lt;br /&gt;  meta = {&lt;br /&gt;    homepage = https://darwinsys.com/file;&lt;br /&gt;    description = &quot;A program that shows the type of files&quot;;&lt;br /&gt;  };&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above Nix expression invokes the function: &lt;i&gt;stdenv.mkDerivation&lt;/i&gt; that creates a build environment in which we build the package: &lt;a href=&quot;https://www.darwinsys.com/file&quot;&gt;&lt;i&gt;file&lt;/i&gt;&lt;/a&gt; from source code:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The &lt;i&gt;name&lt;/i&gt; parameter provides the package name.&lt;/li&gt;&lt;li&gt;The &lt;i&gt;src&lt;/i&gt; parameter invokes the &lt;i&gt;fetchurl&lt;/i&gt; function that specifies where to download the source tarball from.&lt;/li&gt;&lt;li&gt;&lt;i&gt;buildInputs&lt;/i&gt; refers to the build-time dependencies that the package needs. The file package only uses one dependency: &lt;a href=&quot;https://zlib.net&quot;&gt;&lt;i&gt;zlib&lt;/i&gt;&lt;/a&gt; that provides deflate compression support.&lt;br /&gt;&lt;br /&gt;The &lt;i&gt;buildInputs&lt;/i&gt; parameter is used to automatically configure the build environment in such a way that &lt;i&gt;zlib&lt;/i&gt; can be found as a library dependency by the build script.&lt;/li&gt;&lt;li&gt;The &lt;i&gt;meta&lt;/i&gt; parameter specifies the package's meta data. Meta data is used by Nix to provide information about the package, but it is not used by the build script.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The Nix expression does not specify any build instructions -- if no build instructions were provided, the &lt;i&gt;stdenv.mkDerivation&lt;/i&gt; function will execute the standard &lt;a href=&quot;https://www.sourceware.org/autobook&quot;&gt;GNU Autotools&lt;/a&gt; build procedure: &lt;i&gt;./configure; make; make install&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;Nix combines several concepts to make builds more reliable and reproducible.&lt;br /&gt;&lt;br /&gt;Foremost, packages managed by Nix are stored in a so-called &lt;strong&gt;Nix store&lt;/strong&gt; (&lt;i&gt;/nix/store&lt;/i&gt;) in which every package build resides in its own directory.&lt;br /&gt;&lt;br /&gt;When we build the above Nix expression with the following command:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ nix-build file.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;then we may get the following Nix store path as output:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;/nix/store/6rcg0zgqyn2v1ypd46hlvngaf5lgqk9g-file-5.38&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Each entry in the Nix store has a SHA256 hash prefix (e.g. &lt;i&gt;ypag3bh7y7i15xf24zihr343wi6x5i6g&lt;/i&gt;) that is derived from all &lt;strong&gt;build inputs&lt;/strong&gt; used to build a package.&lt;br /&gt;&lt;br /&gt;If we would build &lt;i&gt;file&lt;/i&gt;, for example, with a different build script or different version of &lt;i&gt;zlib&lt;/i&gt; then the resulting Nix store prefix will be different. As a result, we can safely store multiple versions and variants of the same package next to each other, because they will never share the same name.&lt;br /&gt;&lt;br /&gt;Because each package resides in its own directory in the Nix store, rather than global directories that are commonly used on conventional Linux systems, such as &lt;i&gt;/bin&lt;/i&gt; and &lt;i&gt;/lib&lt;/i&gt;, we get stricter &lt;strong&gt;purity&lt;/strong&gt; guarantees -- dependencies can typically not be found if they have not been specified in any of the search environment variables (e.g. &lt;i&gt;PATH&lt;/i&gt;) or provided as build parameters.&lt;br /&gt;&lt;br /&gt;In conventional Linux systems, package builds might still accidentally succeed if they unknowingly use an undeclared dependency. When deploying such a package to another system that does not have this undeclared dependency installed, the package might not work properly or not all.&lt;br /&gt;&lt;br /&gt;In simple single-user Nix installations, builds typically get executed in an environment in which most environment variables (including search path environment variables, such as &lt;i&gt;PATH&lt;/i&gt;) are &lt;strong&gt;cleared&lt;/strong&gt; or set to dummy variables.&lt;br /&gt;&lt;br /&gt;Build abstraction functions (such as &lt;i&gt;stdenv.mkDerivation&lt;/i&gt;) will populate the search path environment variables (e.g. &lt;i&gt;PATH&lt;/i&gt;, &lt;i&gt;CLASSPATH&lt;/i&gt;, &lt;i&gt;PYTHONPATH&lt;/i&gt; etc.) and configure build parameters to ensure that the dependencies in the Nix store can be found.&lt;br /&gt;&lt;br /&gt;Builds are only allowed to write in the build directory or designated output folders in the Nix store.&lt;br /&gt;&lt;br /&gt;When a build has completed successfully, their results are made immutable (by removing their write permission bits in the Nix store) and their timestamps are reset to 1 second after the &lt;a href=&quot;https://en.wikipedia.org/wiki/Unix_time&quot;&gt;epoch&lt;/a&gt; (to improve build determinism).&lt;br /&gt;&lt;br /&gt;Storing packages in isolation and providing an environment with cleared environment variables is obviously not a guarantee that builds will be pure. For example, build scripts may still have hard-coded absolute paths to executables on the host system, such as &lt;i&gt;/bin/install&lt;/i&gt; and a C compiler may still implicitly search for headers in &lt;i&gt;/usr/include&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;To alleviate the problem with hard-coded global directory references, some common build utilities, such as GCC, deployed by Nix have been patched to ignore global directories, such as &lt;i&gt;/usr/include&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;When using &lt;a href=&quot;https://sandervanderburg.blogspot.com/2013/06/setting-up-multi-user-nix-installation.html&quot;&gt;Nix in multi-user mode&lt;/a&gt;, extra precautions have been taken to ensure build purity:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Each build will run as an &lt;strong&gt;unprivileged&lt;/strong&gt; user, that do not have any write access to any directory but its own build directory and the designated output Nix store paths.&lt;/li&gt;&lt;li&gt;On Linux, optionally a build can run in a &lt;a href=&quot;https://en.wikipedia.org/wiki/Chroot&quot;&gt;&lt;strong&gt;chroot&lt;/strong&gt;&lt;/a&gt; environment, that completely disables access to all global directories in the build process. In addition, all Nix store paths of all dependencies will be &lt;strong&gt;bind mounted&lt;/strong&gt;, preventing the build process to still access undeclared dependencies in the Nix store (changes will be slim that you encounter such a build, but still...)&lt;/li&gt;&lt;li&gt;On Linux kernels that support &lt;a href=&quot;https://man7.org/linux/man-pages/man7/namespaces.7.html&quot;&gt;&lt;strong&gt;namespaces&lt;/strong&gt;&lt;/a&gt;, the Nix build environment will use them to improve build purity.&lt;br /&gt;&lt;br /&gt;The network namespace helps the Nix builder to prevent a build process from accessing the network -- when a build process downloads an undeclared dependency from a remote location, we cannot be sure that we get a predictable result.&lt;br /&gt;&lt;br /&gt;In Nix, only builds that are so-called &lt;strong&gt;fixed output derivations&lt;/strong&gt; (whose output hashes need to be known in advance) are allowed to download files from remote locations, because their output results can be verified.&lt;br /&gt;&lt;br /&gt;(As a sidenote: namespaces are also intensively used by Docker containers, as I will explain in the next section.)&lt;/li&gt;&lt;li&gt;On macOS, builds can optionally be executed in an &lt;a href=&quot;https://developer.apple.com/library/archive/documentation/Security/Conceptual/AppSandboxDesignGuide/AboutAppSandbox/AboutAppSandbox.html&quot;&gt;&lt;strong&gt;app sandbox&lt;/strong&gt;&lt;/a&gt;, that can also be used to restrict access to various kinds of shared resources, such as network access.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Besides isolation, using hash code prefixes have another advantage. Because every build with the same hash code is (nearly) bit identical, it also provides a nice optimization feature.&lt;br /&gt;&lt;br /&gt;When we evaluate a Nix expression and the resulting hash code is identical to a valid Nix store path, then we do not have to build the package again -- because it is bit identical, we can simply return the Nix store path of the package that is already in the Nix store.&lt;br /&gt;&lt;br /&gt;This property is also used by Nix to facilitate transparent &lt;strong&gt;binary package deployments&lt;/strong&gt;. If we want to build a package with a certain hash prefix, and we know that another machine or binary cache already has this package in its Nix store, then we can download a binary &lt;strong&gt;substitute&lt;/strong&gt;.&lt;br /&gt;  &lt;br /&gt;Another interesting benefit of using hash codes is that we can also identify the &lt;strong&gt;runtime dependencies&lt;/strong&gt; that a package needs -- if a Nix store path contains references to other Nix store paths, then we know that these are runtime dependencies of the corresponding package.&lt;br /&gt;&lt;br /&gt;Scanning for Nix store paths may sound scary, but there is a very slim change that a hash code string represents something else. In practice, it works really well.&lt;br /&gt;&lt;br /&gt;For example, the following shell command shows all the runtime dependencies of the &lt;i&gt;file&lt;/i&gt; package:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ nix-store -qR /nix/store/6rcg0zgqyn2v1ypd46hlvngaf5lgqk9g-file-5.38&lt;br /&gt;/nix/store/y8n2b9nwjrgfx3kvi3vywvfib2cw5xa6-libunistring-0.9.10&lt;br /&gt;/nix/store/fhg84pzckx2igmcsvg92x1wpvl1dmybf-libidn2-2.3.0&lt;br /&gt;/nix/store/bqbg6hb2jsl3kvf6jgmgfdqy06fpjrrn-glibc-2.30&lt;br /&gt;/nix/store/5x6l9xm5dp6v113dpfv673qvhwjyb7p5-zlib-1.2.11&lt;br /&gt;/nix/store/6rcg0zgqyn2v1ypd46hlvngaf5lgqk9g-file-5.38&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;If we query the dependencies of another package that is built from the same Nix packages set, such as &lt;i&gt;cpio&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ nix-store -qR /nix/store/bzm0mszhvbr6hp4gmar4czsn52hz07q1-cpio-2.13&lt;br /&gt;/nix/store/y8n2b9nwjrgfx3kvi3vywvfib2cw5xa6-libunistring-0.9.10&lt;br /&gt;/nix/store/fhg84pzckx2igmcsvg92x1wpvl1dmybf-libidn2-2.3.0&lt;br /&gt;/nix/store/bqbg6hb2jsl3kvf6jgmgfdqy06fpjrrn-glibc-2.30&lt;br /&gt;/nix/store/bzm0mszhvbr6hp4gmar4czsn52hz07q1-cpio-2.13&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;When looking at the outputs above, you may probably notice that both &lt;i&gt;bash&lt;/i&gt; and &lt;i&gt;cpio&lt;/i&gt; share the same kinds of dependencies (e.g. &lt;i&gt;libidn2&lt;/i&gt;, &lt;i&gt;libunisting&lt;/i&gt; and &lt;i&gt;glibc&lt;/i&gt;), with the same hash code prefixes. Because they are same Nix store paths, they are shared on disk (and in RAM, because the operating system caches the same files in memory) leading to more efficient disk and RAM usage.&lt;br /&gt;&lt;br /&gt;The fact that we can detect references to Nix store paths is because packages in the Nix package repository use an unorthodox form of &lt;strong&gt;static linking&lt;/strong&gt;.&lt;br /&gt;&lt;br /&gt;For example, ELF executables built with Nix have the store paths of their library dependencies in their &lt;i&gt;RPATH&lt;/i&gt; header values (the &lt;i&gt;ld&lt;/i&gt; command in Nixpkgs has been wrapped to transparently augment libraries to a binary's RPATH).&lt;br /&gt;&lt;br /&gt;Python programs (and other programs written in interpreted languages) typically use wrapper scripts that set the &lt;i&gt;PYTHONPATH&lt;/i&gt; (or equivalent) environment variables to contain Nix store paths providing the dependencies.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Docker concepts&lt;/h2&gt;&lt;br /&gt;The &lt;a href=&quot;https://docs.docker.com/get-started/overview&quot;&gt;Docker overview page&lt;/a&gt; states the following about what Docker can do:&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. &lt;/blockquote&gt;&lt;br /&gt;Although you can create many kinds of objects with Docker, the two most important objects are the following:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt;. The overview page states: &quot;An image is a read-only template with instructions for creating a Docker container.&quot;.&lt;br /&gt;&lt;br /&gt;To more accurately describe what this means is that images are created from build recipes called &lt;a href=&quot;https://docs.docker.com/engine/reference/builder&quot;&gt;&lt;i&gt;Dockerfile&lt;/i&gt;s&lt;/a&gt;. They produce self-contained root file systems containing all necessary files to run a program, such as binaries, libraries, configuration files etc. The resulting image itself is immutable (read only) and cannot change after it has been built.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Containers&lt;/strong&gt;. The overview gives the following description: &quot;A container is a runnable instance of an image&quot;.&lt;br /&gt;&lt;br /&gt;More specifically, this means that the life-cycle (whether a container is in a started or stopped state) is bound to the life-cycle of a root process, that runs in a (somewhat) isolated environment using the content of a Docker image as its root file system.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Besides the object types explained above, there are many more kinds objects, such as volumes (that can mount a directory from the host file system to a path in the container), and port forwardings from the host system to a container. For more information about these remaining objects, consult the Docker documentation.&lt;br /&gt;&lt;br /&gt;Docker combines several concepts to facilitate reproducible and reliable container deployment. To be able to isolate containers from each other, it uses several kinds of Linux namespaces:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The mount namespace: This is in IMO the most important name space. After setting up a private mount namespace, every subsequent mount that we make will be visible in the container, but not to other containers/processes that are in a different mount name space.&lt;br /&gt;&lt;br /&gt;A private mount namespace is used to mount a new root file system (the contents of the Docker image) with all essential system software and other artifacts to run an application, that is different from the host system's root file system.&lt;/li&gt;&lt;li&gt;The Process ID (PID) namespace facilitates process isolation. A process/container with a private PID namespace will not be able to see or control the host system's processes (the opposite is actually possible).&lt;/li&gt;&lt;li&gt;The network namespace separates network interfaces from the host system. In a private network namespace, a container has one or more private network interfaces with their own IP addresses, port assignments and firewall settings.&lt;br /&gt;&lt;br /&gt;As a result, a service such as the Apache HTTP server in a Docker container can bind to port 80 without conflicting with another HTTP server that binds to the same port on the host system or in another container instance.&lt;/li&gt;&lt;li&gt;The Inter-Process Communication (IPC) namespace separates the ability for processes to communicate with each other via the SHM family of functions to establish a range of shared memory between the two processes.&lt;/li&gt;&lt;li&gt;The UTS namespace isolates kernel and version identifiers.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Another important concept that containers use are &lt;a href=&quot;https://man7.org/linux/man-pages/man7/cgroups.7.html&quot;&gt;&lt;strong&gt;cgroups&lt;/strong&gt;&lt;/a&gt; that can be use to limit the amount of system resources that containers can use, such as the amount of RAM.&lt;br /&gt;&lt;br /&gt;Finally, to optimize/reduce storage overhead, Docker uses layers and a union filesystem (there are a variety of file system options for this) to combine these layers by &quot;stacking&quot; them on top of each other.&lt;br /&gt;&lt;br /&gt;A running container basically mounts an image's read-only layers on top of each other, and keeps the final layer writable so that processes in the container can create and modify files on the system.&lt;br /&gt;&lt;br /&gt;Whenever you construct an image from a &lt;i&gt;Dockerfile&lt;/i&gt;, each modification operation generates a new layer. Each layer is immutable (it will never change after it has been created) and is uniquely identifiable with a hash code, similar to Nix store paths.&lt;br /&gt;&lt;br /&gt;For example, we can build an image with the following &lt;i&gt;Dockerfile&lt;/i&gt; that deploys and runs the Apache HTTP server on a &lt;a href=&quot;https://debian.org&quot;&gt;Debian&lt;/a&gt; Buster Linux distribution:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;FROM debian:buster&lt;br /&gt;&lt;br /&gt;RUN apt-get update&lt;br /&gt;RUN apt-get install -y apache2&lt;br /&gt;ADD index.html /var/www/html&lt;br /&gt;CMD [&quot;apachectl&quot;, &quot;-D&quot;, &quot;FOREGROUND&quot;]&lt;br /&gt;EXPOSE 80/tcp&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above &lt;i&gt;Dockerfile&lt;/i&gt; executes the following steps:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;It takes the &lt;i&gt;debian:buster&lt;/i&gt; image from Docker Hub as a base image.&lt;/li&gt;&lt;li&gt;It updates the Debian package database (&lt;i&gt;apt-get update&lt;/i&gt;) and installs the Apache HTTPD server package from the Debian package repository.&lt;/li&gt;&lt;li&gt;It uploads an example page (&lt;i&gt;index.html&lt;/i&gt;) to the document root folder.&lt;/li&gt;&lt;li&gt;It executes the: &lt;i&gt;apachectl -D FOREGROUND&lt;/i&gt; command-line instruction to start the Apache HTTP server in foreground mode. The container's life-cycle is bound to the life-cycle of this foreground process.&lt;/li&gt;&lt;li&gt;It informs Docker that the container listens to TCP port: 80. Connecting to port 80 makes it possible for a user to retrieve the example &lt;i&gt;index.html&lt;/i&gt; page.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;With the following command we can build the image:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker build . -t debian-apache&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Resulting in the following layers:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto; font-size: 90%;&quot;&gt;&lt;br /&gt;$ docker history debian-nginx:latest&lt;br /&gt;IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT&lt;br /&gt;a72c04bd48d6        About an hour ago   /bin/sh -c #(nop)  EXPOSE 80/tcp                0B                  &lt;br /&gt;325875da0f6d        About an hour ago   /bin/sh -c #(nop)  CMD [&quot;apachectl&quot; &quot;-D&quot; &quot;FO…   0B                  &lt;br /&gt;35d9a1dca334        About an hour ago   /bin/sh -c #(nop) ADD file:18aed37573327bee1…   129B                &lt;br /&gt;59ee7771f1bc        About an hour ago   /bin/sh -c apt-get install -y apache2           112MB               &lt;br /&gt;c355fe9a587f        2 hours ago         /bin/sh -c apt-get update                       17.4MB              &lt;br /&gt;ae8514941ea4        33 hours ago        /bin/sh -c #(nop)  CMD [&quot;bash&quot;]                 0B                  &lt;br /&gt;&amp;lt;missing&amp;gt;           33 hours ago        /bin/sh -c #(nop) ADD file:89dfd7d3ed77fd5e0…   114MB&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;As may be observed, the base Debian Buster image and every change made in the &lt;i&gt;Dockerfile&lt;/i&gt; results in a new layer with a new hash code, as shown in the &lt;i&gt;IMAGE&lt;/i&gt; column.&lt;br /&gt;&lt;br /&gt;Layers and Nix store paths share the similarity that they are immutable and they can both be identified with hash codes.&lt;br /&gt;&lt;br /&gt;They are also different -- first, a Nix store path is the result of building a package or a static artifact, whereas a layer is the result of making a filesystem modification. Second, for a Nix store path, the hash code is derived from all inputs, whereas the hash code of a layer is derived from the output: its contents.&lt;br /&gt;&lt;br /&gt;Furthermore, Nix store paths are always isolated because they reside in a unique directory (enforced by the hash prefixes), whereas a layer might have files that overlap with files in other layers. In Docker, when a conflict is encountered the files in the layer that gets added on top of it take precedence.&lt;br /&gt;&lt;br /&gt;We can construct a second image using the same Debian Linux distribution image that runs Nginx with the following &lt;i&gt;Dockerfile&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;FROM debian:buster&lt;br /&gt;&lt;br /&gt;RUN apt-get update&lt;br /&gt;RUN apt-get install -y nginx&lt;br /&gt;ADD nginx.conf /etc&lt;br /&gt;ADD index.html /var/www&lt;br /&gt;CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;, &quot;-c&quot;, &quot;/etc/nginx.conf&quot;]&lt;br /&gt;EXPOSE 80/tcp&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above &lt;i&gt;Dockerfile&lt;/i&gt; looks similar to the previous, except that we install the Nginx package from the Debian package repository and we use a different command-line instruction to start Nginx in foreground mode.&lt;br /&gt;&lt;br /&gt;When building the image, its storage will be optimized -- both images share the same base layer (the Debian Buster Linux base distribution):&lt;br /&gt;  &lt;br /&gt;&lt;pre style=&quot;overflow: auto; font-size: 90%;&quot;&gt;&lt;br /&gt;$ docker history debian-nginx:latest&lt;br /&gt;IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT&lt;br /&gt;b7ae6f38ae77        2 hours ago         /bin/sh -c #(nop)  EXPOSE 80/tcp                0B                  &lt;br /&gt;17027888ce23        2 hours ago         /bin/sh -c #(nop)  CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon…   0B                  &lt;br /&gt;41a50a3fa73c        2 hours ago         /bin/sh -c #(nop) ADD file:18aed37573327bee1…   129B                &lt;br /&gt;0f5b2fdcb207        2 hours ago         /bin/sh -c #(nop) ADD file:f18afd18cfe2728b3…   189B                &lt;br /&gt;e49bbb46138b        2 hours ago         /bin/sh -c apt-get install -y nginx             64.2MB              &lt;br /&gt;c355fe9a587f        2 hours ago         /bin/sh -c apt-get update                       17.4MB              &lt;br /&gt;ae8514941ea4        33 hours ago        /bin/sh -c #(nop)  CMD [&quot;bash&quot;]                 0B                  &lt;br /&gt;&amp;lt;missing&amp;gt;           33 hours ago        /bin/sh -c #(nop) ADD file:89dfd7d3ed77fd5e0…   114MB&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;If you compare the above output with the previous &lt;i&gt;docker history&lt;/i&gt; output, then you will notice that the bottom layer (last row) refers to the same layer using the same hash code behind the &lt;i&gt;ADD file:&lt;/i&gt; statement in the &lt;i&gt;CREATED BY&lt;/i&gt; column.&lt;br /&gt;&lt;br /&gt;This ability to share the base distribution prevents us from storing another 114MB Debian Buster image, saving us storage and RAM.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Some common misconceptions&lt;/h2&gt;&lt;br /&gt;What I have noticed is that quite a few people compare containers to virtual machines (and even give containers that name, incorrectly suggesting that they are the same thing!).&lt;br /&gt;&lt;br /&gt;A container is not a virtual machine, because it does not emulate or virtualize hardware -- virtual machines have a virtual CPU, virtual memory, virtual disk etc. that have similar capabilities and limitations as real hardware.&lt;br /&gt;&lt;br /&gt;Furthermore, containers do not run a full operating system -- they run processes managed by the host system's Linux kernel. As a result, Docker containers will only deploy software that runs on Linux, and not software that was built for other operating systems.&lt;br /&gt;&lt;br /&gt;(As a sidenote: Docker can also be used on Windows and macOS -- on these non-Linux platforms, &lt;a href=&quot;https://docs.docker.com/docker-for-windows/troubleshoot/#virtualization&quot;&gt;a virtualized Linux system&lt;/a&gt; is used for hosting the containers, but the containers themselves are not separated by using virtualization).&lt;br /&gt;&lt;br /&gt;Containers cannot even be considered &quot;light weight virtual machines&quot;.&lt;br /&gt;&lt;br /&gt;The means to isolate containers from each other only apply to a limited number of potentially shared resources. For example, a resource that could not be unshared is the system's clock, although this may change in the near future, because in March 2020 a &lt;a href=&quot;https://lore.kernel.org/lkml/158016896588.31887.14143226032971732742.tglx@nanos.tec.linutronix.de/&quot;&gt;time namespace has been added&lt;/a&gt; to the newest Linux kernel version. I believe this namespace is not yet offered as a generally available feature in Docker.&lt;br /&gt;&lt;br /&gt;Moreover, namespaces, that normally provide separation/isolation between containers, &lt;a href=&quot;https://blog.jessfraz.com/post/containers-zones-jails-vms/&quot;&gt;are objects&lt;/a&gt; and these objects can also be shared among multiple container instances (this is a uncommon use-case, because by default every container has its own private namespaces).&lt;br /&gt;&lt;br /&gt;For example, it is also possible for two containers to share the same IPC namespace -- then processes in both containers will be able to communicate with each other with a shared-memory IPC mechanism, but they cannot do any IPC with processes on the host system or containers not sharing the same namespace.&lt;br /&gt;&lt;br /&gt;Finally, certain system resources are not constrained by default unlike a virtual machine -- for example, a container is allowed to consume all the RAM of the host machine unless a RAM restriction has been configured. An unrestricted container could potentially affect the machine's stability as a whole and other containers running on the same machine.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;A comparison of use cases&lt;/h2&gt;&lt;br /&gt;As mentioned in the introduction, when I show people Nix, then I often get a remark that it looks very similar to Docker.&lt;br /&gt;&lt;br /&gt;In this section, I will compare some of their common use cases.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Managing services&lt;/h3&gt;&lt;br /&gt;In addition to building a Docker image, I believe the most common use case for Docker is to &lt;strong&gt;manage services&lt;/strong&gt;, such as custom REST API services (that are self-contained processes with an embedded web server), web servers or database management systems.&lt;br /&gt;&lt;br /&gt;For example, after building an Nginx Docker image (as shown in the section about Docker concepts), we can also launch a container instance using the previously constructed image to serve our example HTML page:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker run -p 8080:80 --name nginx-container -it debian-nginx&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above command create a new container instance using our Nginx image as a root file system and then starts the container in interactive mode -- the command's execution will block and display the output of the Nginx process on the terminal.&lt;br /&gt;&lt;br /&gt;If we would omit the &lt;i&gt;-it&lt;/i&gt; parameters then the container will run in the background.&lt;br /&gt;&lt;br /&gt;The &lt;i&gt;-p&lt;/i&gt; parameter configures a port forwarding from the host system to the container: traffic to the host system's port 8080 gets forwarded to port 80 in the container where the Nginx server listens to.&lt;br /&gt;&lt;br /&gt;We should be able to see the example HTML page, by opening the following URL in a web browser:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;http://localhost:8080&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;After stopping the container, its state will be retained. We can remove the container permanently, by running:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker rm nginx-container&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The Nix package manager has no equivalent use case for manging running processes, because its purpose is package management and not process/service life-cycle management.&lt;br /&gt;&lt;br /&gt;However, some projects based on Nix, such as &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/01/nixos-purely-functional-linux.html&quot;&gt;NixOS&lt;/a&gt;: a Linux distribution built around the Nix package manager using a single declarative configuration file to capture a machine's configuration, generates &lt;a href=&quot;https://www.freedesktop.org/wiki/Software/systemd&quot;&gt;&lt;i&gt;systemd&lt;/i&gt;&lt;/a&gt; unit files to manage services' life-cycles.&lt;br /&gt;&lt;br /&gt;The Nix package manager can also be used on other operating systems, such as conventional Linux distributions, macOS and other UNIX-like systems. There is no universal solution that allows you to complement Nix with service manage support on all platforms that Nix supports.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Experimenting with packages&lt;/h3&gt;&lt;br /&gt;Another common use case is using Docker to experiment with packages that should not remain permanently installed on a system.&lt;br /&gt;&lt;br /&gt;One way of doing this is by directly pulling a Linux distribution image (such as Debian Buster):&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker pull debian:buster&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;and then starting a container in an interactive shell session, in which we install the packages that we want to experiment with:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker run --name myexperiment -it debian:buster /bin/sh&lt;br /&gt;$ apt-get update&lt;br /&gt;$ apt-get install -y file&lt;br /&gt;# file --version&lt;br /&gt;file-5.22&lt;br /&gt;magic file from /etc/magic:/usr/share/misc/magic&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above example suffices to experiment with the &lt;i&gt;file&lt;/i&gt; package, but its deployment is not guaranteed to be reproducible.&lt;br /&gt;&lt;br /&gt;For example, the result of running my &lt;i&gt;apt-get&lt;/i&gt; instructions shown above is file version 5.22. If I would, for example, run the same instructions a week later, then I might get a different version (e.g. 5.23).&lt;br /&gt;&lt;br /&gt;The Docker-way of making such a deployment scenario reproducible, is by installing the packages in a &lt;i&gt;Dockerfile&lt;/i&gt; as part of the container's image construction process:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;FROM debian:buster&lt;br /&gt;&lt;br /&gt;RUN apt-get update&lt;br /&gt;RUN apt-get install -y file&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;we can build the container image, with our &lt;i&gt;file&lt;/i&gt; package as follows:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker build . -t file-experiment&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;and then deploy a container that uses that image:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker run --name myexperiment -it debian:buster /bin/sh&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;As long as we deploy a container with the same image, we will always have the same version of the &lt;i&gt;file&lt;/i&gt; executable:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker run --name myexperiment -it file-experiment /bin/sh&lt;br /&gt;# file --version&lt;br /&gt;file-5.22&lt;br /&gt;magic file from /etc/magic:/usr/share/misc/magic&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;With Nix, &lt;a href=&quot;https://sandervanderburg.blogspot.com/2013/12/using-nix-while-doing-development.html&quot;&gt;generating reproducible development environments with packages&lt;/a&gt; is a first-class feature.&lt;br /&gt;&lt;br /&gt;For example, to launch a shell session providing the &lt;i&gt;file&lt;/i&gt; package from the Nixpkgs collection, we can simply run:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;&lt;br /&gt;$ nix-shell -p file&lt;br /&gt;$ file --version&lt;br /&gt;file-5.39&lt;br /&gt;magic file from /nix/store/j4jj3slm15940mpmympb0z99a2ghg49q-file-5.39/share/misc/magic&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;As long as the Nix expression sources remain the same (e.g. the Nix channel is not updated, or &lt;i&gt;NIX_PATH&lt;/i&gt; is hardwired to a certain Git revision of Nixpkgs), the deployment of the development environment is reproducible -- we should always get the same &lt;i&gt;file&lt;/i&gt; package with the same Nix store path.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Building development projects/arbitrary packages&lt;/h2&gt;&lt;br /&gt;As shown in the section about Nix's concepts, one of Nix's key features is to generate build environments for building packages and other software projects. I have shown that with a simple Nix expression consisting of only a few lines of code, we can build the &lt;i&gt;file&lt;/i&gt; package from source code and its build dependencies in such a dedicated build environment.&lt;br /&gt;&lt;br /&gt;In Docker, only building images is a first-class concept. However, building arbitrary software projects and packages is also something you can do by using Docker containers in a specific way.&lt;br /&gt;&lt;br /&gt;For example, we can create a bash script that builds the same example package (&lt;i&gt;file&lt;/i&gt;) shown in the section that explains Nix's concepts:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;#!/bin/bash -e&lt;br /&gt;&lt;br /&gt;mkdir -p /build&lt;br /&gt;cd /build&lt;br /&gt;&lt;br /&gt;wget ftp://ftp.astron.com/pub/file/file-5.38.tar.gz&lt;br /&gt;&lt;br /&gt;tar xfv file-5.38.tar.gz&lt;br /&gt;cd file-5.38&lt;br /&gt;./configure --prefix=/opt/file&lt;br /&gt;make&lt;br /&gt;make install&lt;br /&gt;&lt;br /&gt;tar cfvz /out/file-5.38-binaries.tar.gz /opt/file&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Compared to its Nix expression counterpart, the build script above does not use any abstractions -- as a consequence, we have to explicitly write all steps that executes the required build steps to build the package:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Create a dedicated build directory.&lt;/li&gt;&lt;li&gt;Download the source tarball from the FTP server&lt;/li&gt;&lt;li&gt;Unpack the tarball&lt;/li&gt;&lt;li&gt;Execute the standard GNU Autotools build procedure: &lt;i&gt;./configure; make; make install&lt;/i&gt; and install the binaries in an isolated folder (&lt;i&gt;/opt/file&lt;/i&gt;).&lt;/li&gt;&lt;li&gt;Create a binary tarball from the &lt;i&gt;/opt/file&lt;/i&gt; folder and store it in the &lt;i&gt;/out&lt;/i&gt; directory (that is a volume shared between the container and the host system).&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;To create a container that runs the build script and to provide its dependencies in a reproducible way, we need to construct an image from the following &lt;i&gt;Dockerfile&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;FROM debian:buster&lt;br /&gt;&lt;br /&gt;RUN apt-get update&lt;br /&gt;RUN apt-get install -y wget gcc make libz-dev&lt;br /&gt;ADD ./build.sh /&lt;br /&gt;CMD /build.sh&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above Dockerfile builds an image using the Debian Buster Linux distribution, installs all mandatory build utilities (&lt;i&gt;wget&lt;/i&gt;, &lt;i&gt;gcc&lt;/i&gt;, and &lt;i&gt;make&lt;/i&gt;) and library dependencies (&lt;i&gt;libz-dev&lt;/i&gt;), and executes the build script shown above.&lt;br /&gt;&lt;br /&gt;With the following command, we can build the image:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker build . -t buildenv&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;and with the following command, we can create and launch the container that executes the build script (and automatically discard it as soon as it finishes its task):&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker run -v $(pwd)/out:/out --rm -t buildenv&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;To make sure that we can keep our resulting binary tarball after the container gets discarded, we have created a shared volume that maps the &lt;i&gt;out&lt;/i&gt; directory in our current working directory onto the &lt;i&gt;/out&lt;/i&gt; directory in the container.&lt;br /&gt;&lt;br /&gt;When the build script finishes, the output directory should contain our generated binary tarball:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;ls out/&lt;br /&gt;file-5.38-binaries.tar.gz&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Although both Nix and Docker both can provide reproducible environments for building packages (in the case of Docker, we need to make sure that all dependencies are provided by the Docker image), builds performed in a Docker container are not guaranteed to be pure, because it does not take the same precautions that Nix takes:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;In the build script, we download the source tarball without checking its integrity. This might cause an impurity, because the tarball on the remote server could change (this could happen for non-mallicious as well as mallicous reasons).&lt;/li&gt;&lt;li&gt;While running the build, we have unrestricted network access. The build script might unknowingly download all kinds of undeclared/unknown dependencies from external sites whose results are not deterministic.&lt;/li&gt;&lt;li&gt;We do not reset any timestamps -- as a result, when performing the same build twice in a row, the second result might be slightly different because of the timestamps integrated in the build product.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Coping with these impurities in a Docker workflow is the responsibility of the build script implementer. With Nix, most of it is transparently handled for you.&lt;br /&gt;&lt;br /&gt;Moreover, the build script implementer is also responsible to retrieve the build artifact and store it somewhere, e.g. in a directory outside the container or uploading it to a remote artifactory repository.&lt;br /&gt;&lt;br /&gt;In Nix, the result of a build process is automatically stored in isolation in the Nix store. We can also quite easily turn a Nix store into a binary cache and let other Nix consumers download from it, e.g. by installing &lt;a href=&quot;https://github.com/edolstra/nix-serve&quot;&gt;&lt;i&gt;nix-serve&lt;/i&gt;&lt;/a&gt;, &lt;a href=&quot;https://sandervanderburg.blogspot.com/2013/04/setting-up-hydra-build-cluster-for.html&quot;&gt;Hydra: the Nix-based continuous integration service&lt;/a&gt;, &lt;a href=&quot;https://cachix.org/&quot;&gt;cachix&lt;/a&gt;, or by &lt;a href=&quot;https://sandervanderburg.blogspot.com/2016/10/push-and-pull-deployment-of-nix-packages.html&quot;&gt;manually generating a static binary cache&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Beyond the ability to execute builds, Nix has another great advantage for building packages from source code. On Linux systems, the Nixpkgs collection is entirely &lt;strong&gt;bootstrapped&lt;/strong&gt;, except for the bootstrap binaries -- this provides us almost full traceability of all dependencies and transitive dependencies used at build-time.&lt;br /&gt;&lt;br /&gt;With Docker you typically do not have such insights -- images get constructed from binaries obtained from arbitrary locations (e.g. binary packages that originate from Linux distributions' package repositories). As a result, it is impossible to get any insights on how these package dependencies were constructed from source code.&lt;br /&gt;&lt;br /&gt;For most people, knowing exactly from which sources a package has been built is not considered important, but it can still be useful for more specialized use cases. For example, to determine if your system is constructed from trustable/audited sources and &lt;a href=&quot;https://sandervanderburg.blogspot.com/2012/04/dynamic-analysis-of-build-processes-to.html&quot;&gt;whether you did not violate a license of a third-party library&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Combined use cases&lt;/h2&gt;&lt;br /&gt;As explained earlier in this blog post, Nix and Docker are deployment solutions for sightly different application domains.&lt;br /&gt;&lt;br /&gt;There are quite a few solutions developed by the Nix community that can combine Nix and Docker in interesting ways.&lt;br /&gt;&lt;br /&gt;In this section, I will show some of them.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Experimenting with the Nix package manager in a Docker container&lt;/h3&gt;&lt;br /&gt;Since Docker is such a common solution to provide environments in which users can experiment with packages, the Nix community also provides a &lt;a href=&quot;https://hub.docker.com/r/nixos/nix&quot;&gt;Nix Docker image&lt;/a&gt;, that allows you to conveniently experiment with the Nix package manager in a Docker container.&lt;br /&gt;&lt;br /&gt;We can pull this image as follows:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker pull nixos/nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Then launch a container interactively:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker run -it nixos/nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;And finally, pull the package specifications from the Nix channel and install any Nix package that we want in the container:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto; font-size: 90%;&quot;&gt;&lt;br /&gt;$ nix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs&lt;br /&gt;$ nix-channel --update&lt;br /&gt;$ nix-env -f '&amp;lt;nixpkgs&amp;gt;' -iA file&lt;br /&gt;$ file --version&lt;br /&gt;file-5.39&lt;br /&gt;magic file from /nix/store/bx9l7vrcb9izgjgwkjwvryxsdqdd5zba-file-5.39/share/misc/magic&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h3&gt;Using the Nix package manager to deliver the required packages to construct an image&lt;/h3&gt;&lt;br /&gt;In the examples that construct Docker images for Nginx and the Apache HTTP server, I use the Debian Buster Linux distribution as base images in which I add the required packages to run the services from the Debian package repository.&lt;br /&gt;&lt;br /&gt;This is a common practice to construct Docker images -- as I have already explained in section that covers its concepts, package management is a sub problem of the process/service life-cycle management problem, but Docker leaves solving this problem to the Linux distribution's package manager.&lt;br /&gt;&lt;br /&gt;Instead of using conventional Linux distributions and their package management solutions, such as Debian, Ubuntu (using &lt;i&gt;apt-get&lt;/i&gt;), Fedora (using &lt;i&gt;yum&lt;/i&gt;) or Alpine Linux (using &lt;i&gt;apk&lt;/i&gt;), it is also possible to use Nix.&lt;br /&gt;&lt;br /&gt;The following &lt;i&gt;Dockerfile&lt;/i&gt; can be used to create an image that uses Nginx deployed by the Nix package manager:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto; font-size: 90%;&quot;&gt;&lt;br /&gt;FROM nixos/nix&lt;br /&gt;&lt;br /&gt;RUN nix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs&lt;br /&gt;RUN nix-channel --update&lt;br /&gt;RUN nix-env -f '&amp;lt;nixpkgs&amp;gt;' -iA nginx&lt;br /&gt;&lt;br /&gt;RUN mkdir -p /var/log/nginx /var/cache/nginx /var/www&lt;br /&gt;ADD nginx.conf /etc&lt;br /&gt;ADD index.html /var/www&lt;br /&gt;&lt;br /&gt;CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;, &quot;-c&quot;, &quot;/etc/nginx.conf&quot;]&lt;br /&gt;EXPOSE 80/tcp&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h3&gt;Using Nix to build Docker images&lt;/h3&gt;&lt;br /&gt;Ealier, I have shown that the Nix package manager can also be used in a &lt;i&gt;Dockerfile&lt;/i&gt; to obtain all required packages to run a service.&lt;br /&gt;&lt;br /&gt;In addition to building software packages, Nix can also build all kinds of static artifacts, such as disk images, DVD ROM ISO  images, and virtual machine configurations.&lt;br /&gt;&lt;br /&gt;The Nixpkgs repository also contains &lt;a href=&quot;http://lethalman.blogspot.com/2016/04/cheap-docker-images-with-nix_15.html&quot;&gt;an abstraction function to build Docker images&lt;/a&gt; that does not require any Docker utilities.&lt;br /&gt;&lt;br /&gt;For example, with the following Nix expression, we can build a Docker image that deploys Nginx:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;with import &amp;lt;nixpkgs&amp;gt; {};&lt;br /&gt;&lt;br /&gt;dockerTools.buildImage {&lt;br /&gt;  name = &quot;nginxexp&quot;;&lt;br /&gt;  tag = &quot;test&quot;;&lt;br /&gt;&lt;br /&gt;  contents = nginx;&lt;br /&gt;&lt;br /&gt;  runAsRoot = ''&lt;br /&gt;    ${dockerTools.shadowSetup}&lt;br /&gt;    groupadd -r nogroup&lt;br /&gt;    useradd -r nobody -g nogroup -d /dev/null&lt;br /&gt;    mkdir -p /var/log/nginx /var/cache/nginx /var/www&lt;br /&gt;    cp ${./index.html} /var/www/index.html&lt;br /&gt;  '';&lt;br /&gt;&lt;br /&gt;  config = {&lt;br /&gt;    Cmd = [ &quot;${nginx}/bin/nginx&quot; &quot;-g&quot; &quot;daemon off;&quot; &quot;-c&quot; ./nginx.conf ];&lt;br /&gt;    Expose = {&lt;br /&gt;      &quot;80/tcp&quot; = {};&lt;br /&gt;    };&lt;br /&gt;  };&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above expression propagates the following parameters to the &lt;i&gt;dockerTools.buildImage&lt;/i&gt; function:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The name of the image is: &lt;i&gt;nginxexp&lt;/i&gt; using the tag: &lt;i&gt;test&lt;/i&gt;.&lt;/li&gt;&lt;li&gt;The &lt;i&gt;contents&lt;/i&gt; parameter specifies all Nix packages that should be installed in the Docker image.&lt;/li&gt;&lt;li&gt;The &lt;i&gt;runAsRoot&lt;/i&gt; refers to a script that runs as root user in a QEMU virtual machine. This virtual machine is used to provide the dynamic parts of a Docker image, setting up user accounts and configuring the state of the Nginx service.&lt;/li&gt;&lt;li&gt;The &lt;i&gt;config&lt;/i&gt; parameter specifies image configuration properties, such as the command to execute and which TCP ports should be exposed.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Running the following command:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto; font-size: 90%;&quot;&gt;&lt;br /&gt;$ nix-build&lt;br /&gt;/nix/store/qx9cpvdxj78d98rwfk6a5z2qsmqvgzvk-docker-image-nginxexp.tar.gz&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Produces a compressed tarball that contains all files belonging to the Docker image. We can load the image into Docker with the following command:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker load -i \&lt;br /&gt;  /nix/store/qx9cpvdxj78d98rwfk6a5z2qsmqvgzvk-docker-image-nginxexp.tar.gz&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;and then launch a container instance that uses the Nix-generated image:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker run -p 8080:80/tcp -it nginxexp:test&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;When we look at the Docker images overview:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-size: 90%; overflow: auto;&quot;&gt;&lt;br /&gt;$ docker images&lt;br /&gt;REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE&lt;br /&gt;nginxexp            test                cde8298f025f        50 years ago        61MB&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;There are two properties that stand out when you compare the Nix generated Docker image to conventional Docker images:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The first odd property is that the overview says that the image that was created 50 years ago. This is explainable: to make Nix builds pure and deterministic, time stamps are typically reset to 1 second after the epoch (Januarty 1st 1970), to ensure that we always get the same bit-identical build result.&lt;/li&gt;&lt;li&gt;The second property is the size of the image: 61MB is considerably smaller than our Debian-based Docker image.&lt;br /&gt;&lt;br /&gt;To give you a comparison: the &lt;i&gt;docker history&lt;/i&gt; command-line invocation (shown earlier in this blog post) that displays the layers of which the Debian-based Nginx image consists, shows that the base Linux distribution image consumes 114 MB, the update layer 17.4 MB and the layer that provides the Nginx package is 64.2 MB.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The reason why Nix-generated images are so small is because Nix exactly knows all runtime dependencies required to run Nginx. As a result, we can restrict the image to only contain Nginx and its required runtime dependencies, leaving all unnecessary software out.&lt;br /&gt;&lt;br /&gt;The Debian-based Nginx container is much bigger, because it also contains a base Debian Linux system with all kinds of command-line utilities and libraries, that are not required to run Nginx.&lt;br /&gt;&lt;br /&gt;The same limitation also applies to the Nix Docker image shown in the previous sections -- the Nix Docker image was constructed from an Alpine Linux image and contains a small, but fully functional Linux distribution. As a result, it is bigger than the Docker image directly generated from a Nix expression.&lt;br /&gt;&lt;br /&gt;Although a Nix-generated Docker image is smaller than most conventional images, one of its disadvantages is that the image consists of only one single layer -- as we have seen in the section about Nix concepts, many services typically share the same runtime dependencies (such as &lt;i&gt;glibc&lt;/i&gt;). Because these common dependencies are not in a reusable layer, they cannot be shared.&lt;br /&gt;&lt;br /&gt;To optimize reuse, it is also possible to &lt;a href=&quot;https://grahamc.com/blog/nix-and-layered-docker-images&quot;&gt;build layered Docker images with Nix&lt;/a&gt;:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;with import &amp;lt;nixpkgs&amp;gt; {};&lt;br /&gt;&lt;br /&gt;dockerTools.buildLayeredImage {&lt;br /&gt;  name = &quot;nginxexp&quot;;&lt;br /&gt;  tag = &quot;test&quot;;&lt;br /&gt;&lt;br /&gt;  contents = nginx;&lt;br /&gt;&lt;br /&gt;  maxLayers = 100;&lt;br /&gt;&lt;br /&gt;  extraCommands = ''&lt;br /&gt;    mkdir -p var/log/nginx var/cache/nginx var/www&lt;br /&gt;    cp ${./index.html} var/www/index.html&lt;br /&gt;  '';&lt;br /&gt;&lt;br /&gt;  config = {&lt;br /&gt;    Cmd = [ &quot;${nginx}/bin/nginx&quot; &quot;-g&quot; &quot;daemon off;&quot; &quot;-c&quot; ./nginx.conf ];&lt;br /&gt;    Expose = {&lt;br /&gt;      &quot;80/tcp&quot; = {};&lt;br /&gt;    };&lt;br /&gt;  };&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above Nix expression is similar to the previous. but uses &lt;i&gt;dockerTools.buildLayeredImage&lt;/i&gt; to construct a layered image.&lt;br /&gt;&lt;br /&gt;We can build and load the image as follows:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;&lt;br /&gt;$ docker load -i $(nix-build layered.nix)&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;When we retieve the history of the image, then we will see the following:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto; font-size: 90%;&quot;&gt;&lt;br /&gt;$ docker history nginxexp:test&lt;br /&gt;IMAGE               CREATED             CREATED BY          SIZE                COMMENT&lt;br /&gt;b91799a04b99        50 years ago                            1.47kB              store paths: ['/nix/store/snxpdsksd4wxcn3niiyck0fry3wzri96-nginxexp-customisation-layer']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            200B                store paths: ['/nix/store/6npz42nl2hhsrs98bq45aqkqsndpwvp1-nginx-root.conf']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            1.79MB              store paths: ['/nix/store/qsq6ni4lxd8i4g9g4dvh3y7v1f43fqsp-nginx-1.18.0']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            71.3kB              store paths: ['/nix/store/n14bjnksgk2phl8n69m4yabmds7f0jj2-source']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            166kB               store paths: ['/nix/store/jsqrk045m09i136mgcfjfai8i05nq14c-source']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            1.3MB               store paths: ['/nix/store/4w2zbpv9ihl36kbpp6w5d1x33gp5ivfh-source']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            492kB               store paths: ['/nix/store/kdrdxhswaqm4dgdqs1vs2l4b4md7djma-pcre-8.44']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            4.17MB              store paths: ['/nix/store/6glpgx3pypxzb09wxdqyagv33rrj03qp-openssl-1.1.1g']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            385kB               store paths: ['/nix/store/7n56vmgraagsl55aarx4qbigdmcvx345-libxslt-1.1.34']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            324kB               store paths: ['/nix/store/1f8z1lc748w8clv1523lma4w31klrdpc-geoip-1.6.12']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            429kB               store paths: ['/nix/store/wnrjhy16qzbhn2qdxqd6yrp76yghhkrg-gd-2.3.0']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            1.22MB              store paths: ['/nix/store/hqd0i3nyb0717kqcm1v80x54ipkp4bv6-libwebp-1.0.3']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            327kB               store paths: ['/nix/store/79nj0nblmb44v15kymha0489sw1l7fa0-fontconfig-2.12.6-lib']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            1.7MB               store paths: ['/nix/store/6m9isbbvj78pjngmh0q5qr5cy5y1kzyw-libxml2-2.9.10']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            580kB               store paths: ['/nix/store/2xmw4nxgfximk8v1rkw74490rfzz2gjp-libtiff-4.1.0']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            404kB               store paths: ['/nix/store/vbxifzrl7i5nvh3h505kyw325da9k47n-giflib-5.2.1']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            79.8kB              store paths: ['/nix/store/jc5bd71qcjshdjgzx9xdfrnc9hsi2qc3-fontconfig-2.12.6']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            236kB               store paths: ['/nix/store/9q5gjvrabnr74vinmjzkkljbpxi8zk5j-expat-2.2.8']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            482kB               store paths: ['/nix/store/0d6vl8gzwqc3bdkgj5qmmn8v67611znm-xz-5.2.5']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            6.28MB              store paths: ['/nix/store/rmn2n2sycqviyccnhg85zangw1qpidx0-gcc-9.3.0-lib']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            1.98MB              store paths: ['/nix/store/fnhsqz8a120qwgyyaiczv3lq4bjim780-freetype-2.10.2']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            757kB               store paths: ['/nix/store/9ifada2prgfg7zm5ba0as6404rz6zy9w-dejavu-fonts-minimal-2.37']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            1.51MB              store paths: ['/nix/store/yj40ch9rhkqwyjn920imxm1zcrvazsn3-libjpeg-turbo-2.0.4']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            79.8kB              store paths: ['/nix/store/1lxskkhsfimhpg4fd7zqnynsmplvwqxz-bzip2-1.0.6.0.1']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            255kB               store paths: ['/nix/store/adldw22awj7n65688smv19mdwvi1crsl-libpng-apng-1.6.37']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            123kB               store paths: ['/nix/store/5x6l9xm5dp6v113dpfv673qvhwjyb7p5-zlib-1.2.11']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            30.9MB              store paths: ['/nix/store/bqbg6hb2jsl3kvf6jgmgfdqy06fpjrrn-glibc-2.30']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            209kB               store paths: ['/nix/store/fhg84pzckx2igmcsvg92x1wpvl1dmybf-libidn2-2.3.0']&lt;br /&gt;&amp;lt;missing&amp;gt;           50 years ago                            1.63MB              store paths: ['/nix/store/y8n2b9nwjrgfx3kvi3vywvfib2cw5xa6-libunistring-0.9.10']&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;As you may notice, all Nix store paths are in their own layers. If we would also build a layered Docker image for the Apache HTTP service, we end up using less disk space (because common dependencies such as &lt;i&gt;glibc&lt;/i&gt; can be reused), and less RAM (because these common dependencies can be shared in RAM).&lt;br /&gt;&lt;br /&gt;Mapping Nix store paths onto layers obviously has limitations -- there is a maximum number of layers that Docker can use (in the Nix expression, I have imposed a limit of 100 layers, recent versions of Docker support a somewhat higher number).&lt;br /&gt;&lt;br /&gt;Complex systems packaged with Nix typically have much more dependencies than the number of layers that Docker can mount. To cope with this limitation, the &lt;i&gt;dockerTools.buildLayerImage&lt;/i&gt; abstraction function tries to merge infrequently used dependencies into a shared layers. More information about this process can be found in Graham Christensen's blog post.&lt;br /&gt;&lt;br /&gt;Besides the use cases shown in the examples above, &lt;a href=&quot;https://nixos.org/nixpkgs/manual/#sec-pkgs-dockerTools&quot;&gt;there is much more you can do with the &lt;i&gt;dockerTools&lt;/i&gt; functions in Nixpkgs&lt;/a&gt; -- you can also pull images from Docker Hub (with the &lt;i&gt;dockerTools.pullImage&lt;/i&gt; function) and use the &lt;i&gt;dockerTools.buildImage&lt;/i&gt; function to use existing Docker images as a basis to create hybrids combining conventional Linux software with Nix packages.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;br /&gt;In this blog post, I have elaborated about using Nix and Docker as deployment solutions.&lt;br /&gt;&lt;br /&gt;What they both have in common is that they facilitate reliable and reproducible deployment.&lt;br /&gt;&lt;br /&gt;They can be used for a variety of use cases in two different domains (package management and process/service management). Some of these use cases are common to both Nix and Docker.&lt;br /&gt;&lt;br /&gt;Nix and Docker can also be combined in several interesting ways -- Nix can be used as a package manager to deliver package dependencies in the construction process of an image, and Nix can also be used directly to build images, as a replacement for Dockerfiles.&lt;br /&gt;&lt;br /&gt;This table summarizes the conceptual differences between Nix and Docker covered in this blog post:&lt;br /&gt;&lt;br /&gt;&lt;table style=&quot;border-style: solid; border-width: 1px;&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;&lt;/th&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Nix&lt;/th&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Docker&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Application domain&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Package management&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Process/service management&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Storage units&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Package build results&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;File system changes&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Storage model&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Isolated Nix store paths&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Layers + union file system&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Component addressing&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Hashes computed from inputs&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Hashes computed from a layer's contents&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Service/process management&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Unsupported&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;First-class feature&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Package management&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;First class support&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Delegated responsibility to a distro's package manager&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Development environments&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;&lt;i&gt;nix-shell&lt;/i&gt;&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Create image with dependencies + run shell session in container&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Build management (images)&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;&lt;i&gt;Dockerfile&lt;/i&gt;&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;&lt;i&gt;dockerTools.buildImage {}&lt;br /&gt;&lt;i&gt;dockerTools.buildLayeredImage {}&lt;/i&gt;&lt;/i&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Build management (packages)&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;First class function support&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Implementer's responsibility, can be simulated&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Build environment purity&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Many precautions taken&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Only images provide some reproducibility, implementer's responsibility&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Full source traceability&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Yes (on Linux)&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;No&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style=&quot;border-style: solid; border-width: 1px;&quot;&gt;OS support&lt;/th&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Many UNIX-like systems&lt;/td&gt;&lt;td style=&quot;border-style: solid; border-width: 1px;&quot;&gt;Linux (real system or virtualized)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;I believe the last item in the table deserves a bit of clarification -- Nix works on other operating systems than Linux, e.g. macOS, and can also deploy binaries for those platforms.&lt;br /&gt;&lt;br /&gt;Docker can be used on Windows and macOS, but it still deploys Linux software -- on Windows and macOS containers are deployed to a virtualized Linux environment. Docker containers can only work on Linux, because they heavily rely on Linux-specific concepts: namespaces and cgroups.&lt;br /&gt;&lt;br /&gt;Aside from the functional parts, Nix and Docker also have some fundamental non-functional differences. One of them is usability.&lt;br /&gt;&lt;br /&gt;Although I am a long-time Nix user (since 2007). Docker is very popular because it is well-known and provides quite an optimized user experience. It does not deviate much from the way traditional Linux systems are managed -- this probably explains why so many users incorrectly call containers &quot;virtual machines&quot;, because they manifest themselves as units that provide almost fully functional Linux distributions.&lt;br /&gt;&lt;br /&gt;From &lt;a href=&quot;https://sandervanderburg.blogspot.com/2015/02/a-sales-pitch-explanation-of-nixos.html&quot;&gt;my own experiences&lt;/a&gt;, it is typically more challenging to convince a new audience to adopt Nix -- getting an audience used to the fact that a package build can be modeled as a pure function invocation (in which the function parameters are a package's build inputs) and that a specialized Nix store is used to store all static artifacts, is sometimes difficult.&lt;br /&gt;&lt;br /&gt;Both Nix and Docker support reuse: the former by means of using identical Nix store paths and the latter by using identical layers. For both solutions, these objects can be identified with hash codes.&lt;br /&gt;&lt;br /&gt;In practice, reuse with Docker is not always optimal -- for frequently used services, such as Nginx and Apache HTTP server, is not a common practice to manually derive these images from a Linux distribution base image.&lt;br /&gt;&lt;br /&gt;Instead, most Docker users will obtain specialized Nginx and Apache HTTP images. The &lt;a href=&quot;https://hub.docker.com/_/nginx&quot;&gt;official Docker Nginx images&lt;/a&gt; are constructed from Debian Buster and Alpine Linux, whereas &lt;a href=&quot;https://hub.docker.com/_/httpd&quot;&gt;the official Apache HTTP images&lt;/a&gt; only support Alpine Linux. Sharing common dependencies between these two images will only be possible if we install the Alpine Linux-based images.&lt;br /&gt;&lt;br /&gt;In practice, it happens quite frequently that people run images constructed from all kinds of different base images, making it very difficult to share common dependencies.&lt;br /&gt;&lt;br /&gt;Another impractical aspect of Nix is that it works conveniently for software compiled from source code, but packaging and deploying pre-built binaries is typically a challenge -- ELF binaries typically do not work out of the box and &lt;a href=&quot;https://sandervanderburg.blogspot.com/2015/10/deploying-prebuilt-binary-software-with.html&quot;&gt;need to be patched&lt;/a&gt;, or deployed to &lt;a href=&quot;https://sandervanderburg.blogspot.com/2013/09/composing-fhs-compatible-chroot.html&quot;&gt;an FHS user environment&lt;/a&gt; in which dependencies can be found in their &quot;usual&quot; locations (e.g. &lt;i&gt;/bin&lt;/i&gt;, &lt;i&gt;/lib&lt;/i&gt; etc.).&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Related work&lt;/h2&gt;&lt;br /&gt;In this blog post, I have restricted my analysis to Nix and Docker. Both tools are useful on their own, but they are also the foundations of entire solution eco-systems. I did not elaborate much about solutions in these extended eco-systems.&lt;br /&gt;&lt;br /&gt;For example, Nix does not do any process/service management, but there are Nix-related projects that can address this concern. Most notably: NixOS: a Linux-distribution fully managed by Nix, uses systemd to manage services.&lt;br /&gt;&lt;br /&gt;For Nix users on macOS, there is a project called &lt;a href=&quot;https://github.com/LnL7/nix-darwin&quot;&gt;nix-darwin&lt;/a&gt; that integrates with &lt;i&gt;launchd&lt;/i&gt;, which is the default service manager on macOS.&lt;br /&gt;&lt;br /&gt;There also used to be an interesting cross-over project between Nix and Docker (called &lt;a href=&quot;https://github.com/zefhemel/nix-docker&quot;&gt;nix-docker&lt;/a&gt;) combining the Nix's package management capabilities, with Docker's isolation capabilities, and &lt;a href=&quot;http://supervisord.org&quot;&gt;supervisord&lt;/a&gt;'s ability to manage multiple services in a container -- it takes a configuration file (that looks similar to a NixOS configuration) defining a set of services, fully generates a supervisord configuration (with all required services and dependencies) and deploys them to a container. Unfortunately, the project is no longer maintained.&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://nixery.dev&quot;&gt;Nixery&lt;/a&gt; is a Docker-compatible container registry that is capable of transparently building and serving container images using Nix.&lt;br /&gt;&lt;br /&gt;Docker is also an interesting foundation for an entire eco-system of solutions. Most notably Kubernetes, a container-orchestrating system that works with a variety of container tools including Docker. &lt;i&gt;docker-compose&lt;/i&gt; makes it possible to manage collections of Docker containers and dependencies between containers.&lt;br /&gt;&lt;br /&gt;There are also many solutions available to make building development projects with Docker (and other container technologies) more convenient than my &lt;i&gt;file&lt;/i&gt; package build example. &lt;a href=&quot;https://docs.gitlab.com/ee/ci/&quot;&gt;Gitlab CI&lt;/a&gt;, for example, provides first-class Docker integration. &lt;a href=&quot;https://cloud.google.com/tekton&quot;&gt;Tekton&lt;/a&gt; is a Kubernetes-based framework that can be used to build CI/CD systems.&lt;br /&gt;&lt;br /&gt;There are also quite a few Nix cross-over projects that integrate with the extended containers eco-system, such as Kubernetes and &lt;i&gt;docker-compose&lt;/i&gt;. For example, &lt;a href=&quot;https://github.com/hercules-ci/arion&quot;&gt;arion&lt;/a&gt; can generate &lt;i&gt;docker-compose&lt;/i&gt; configuration files with specialized containers from NixOS modules. &lt;a href=&quot;https://github.com/saschagrunert/kubernix&quot;&gt;KuberNix&lt;/a&gt; can be used to bootstrap a Kubernetes cluster with the Nix package manager, and &lt;a href=&quot;https://github.com/xtruder/kubenix&quot;&gt;Kubenix&lt;/a&gt; can be used to build Kubernetes resources with Nix.&lt;br /&gt;&lt;br /&gt;As explained in my comparisons, package management is not something that Docker supports as a first-class feature, but Docker has been an inspiration for package management solutions as well.&lt;br /&gt;&lt;br /&gt;Most notably, several years ago &lt;a href=&quot;https://sandervanderburg.blogspot.com/2015/04/an-evaluation-and-comparison-of-snappy.html&quot;&gt;I did a comparison between Nix and Ubuntu's Snappy package manager&lt;/a&gt;. The latter deploys every package (and all its required dependencies) as a container.&lt;br /&gt;&lt;br /&gt;In this comparison blog post, I raised a number of concerns about reuse. Snappy does not have any means to share common dependencies between packages, and as a result, Snaps can be quite disk space and memory consuming.&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://flatpak.org&quot;&gt;Flatpak&lt;/a&gt; can be considered an alternative and more open solution to Snappy.&lt;br /&gt;&lt;br /&gt;I still do not understand why these Docker-inspired package management solutions have not used Nix (e.g. storing packages in insolated folders) or Docker (e.g. using layers) as an inspiration to optimize reuse and simplify the construction of packages.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Future work&lt;/h2&gt;&lt;br /&gt;In the next blog post, I will elaborate more about integrating the Nix package manager with tools that can address the process/service management concern.&lt;br /&gt;&lt;br /&gt;</description>
	<pubDate>Wed, 29 Jul 2020 20:57:00 +0000</pubDate>
	<author>noreply@blogger.com (Sander van der Burg)</author>
</item>
<item>
	<title>Cachix: Upstream caches: avoiding pushing paths in cache.nixos.org</title>
	<guid isPermaLink="true">https://blog.cachix.org/posts/2020-07-28-upstream-caches-avoiding-pushing-paths-in-cache-nixos-org/</guid>
	<link>https://blog.cachix.org/posts/2020-07-28-upstream-caches-avoiding-pushing-paths-in-cache-nixos-org/</link>
	<description>One of the most requested features, the so-called upstream caches was released today.
It is enabled by default for all caches, and the owner of the binary cache can disable it via Settings.
When you push store paths to Cachix, querying cache.nixos.org adds overhead of multiples of 100ms, but you save storage and possibly minutes for avoiding the pushing of already available paths.
Queries to cache.nixos.org are also cached, so that subsequent push operations do not have the overhead.</description>
	<pubDate>Tue, 28 Jul 2020 14:30:00 +0000</pubDate>
	<author>support@cachix.org (Domen Kožar)</author>
</item>
<item>
	<title>Cachix: Documentation and More Documentation</title>
	<guid isPermaLink="true">https://blog.cachix.org/posts/2020-07-20-documentation-and-more-documentation/</guid>
	<link>https://blog.cachix.org/posts/2020-07-20-documentation-and-more-documentation/</link>
	<description>Documentation is an important ingredient of a successful software project.
Last few weeks I’ve worked on improving status quo on two fronts:
1) https://nix.dev is an opinionated guide for developers getting things done using the Nix ecosystem.
A few highlights:
 Getting started repository template with a tutorial for using declarative and reproducible developer environments
 Setting up GitHub Actions with Nix
 Nix language anti-patterns to avoid and recommended alternatives</description>
	<pubDate>Mon, 20 Jul 2020 14:45:00 +0000</pubDate>
	<author>support@cachix.org (Domen Kožar)</author>
</item>
<item>
	<title>Tweag I/O: Setting up Buildkite for Nix-based projects using Terraform and GCP</title>
	<guid isPermaLink="true">https://tweag.io/blog/2020-07-08-buildkite-for-nix-ci/</guid>
	<link>https://tweag.io/blog/2020-07-08-buildkite-for-nix-ci/</link>
	<description>&lt;p&gt;In this post I’m going to show how to setup, with Terraform, a Buildkite-based CI using your
own workers that run on GCP. For reference, the complete
Terraform configuration for this post is available in &lt;a href=&quot;https://github.com/tweag/blog-resources/tree/master/buildkite-for-nix-ci&quot;&gt;this
repository&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The setup gives you complete control on how fast your worker are.&lt;/li&gt;
&lt;li&gt;The workers come with Nix pre-installed, so you won’t need to spend
time downloading the same docker container again and again on every
push as would usually happen with most cloud CI providers.&lt;/li&gt;
&lt;li&gt;The workers come with a distributed Nix cache set up. So
authors of CI scripts won’t have to bother about caching at all.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Secrets&lt;/h2&gt;
&lt;p&gt;We are going to need to import two &lt;a href=&quot;https://www.tweag.io/posts/2019-04-03-terraform-provider-secret.html&quot;&gt;secret resources&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-hcl&quot;&gt;&lt;code class=&quot;language-hcl&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;secret_resource&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_agent_token&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;secret_resource&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;nix_signing_key&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To initialize the resources, execute the following from the root directory of your project:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;$ terraform &lt;span class=&quot;token function&quot;&gt;import&lt;/span&gt; secret_resource.&lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;name&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;value&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;buildkite_agent_token&lt;/code&gt; is obtained from the &lt;a href=&quot;https://buildkite.com&quot;&gt;Buildkite site&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;nix_signing_key&lt;/code&gt; can be generated by running:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;nix-store --generate-binary-cache-key &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;your-key-name&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt; key.private key.public&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;language-text&quot;&gt;key.private&lt;/code&gt; file will contain the value for the signing key. I’ll
explain later in the post how to use the contents of the &lt;code class=&quot;language-text&quot;&gt;key.public&lt;/code&gt;
file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Custom NixOS image&lt;/h2&gt;
&lt;p&gt;The next step is to use the &lt;a href=&quot;https://github.com/tweag/terraform-nixos/tree/master/google_image_nixos_custom&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;nixos_image_custom&lt;/code&gt;&lt;/a&gt;
module to create a NixOS image with custom configuration.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-hcl&quot;&gt;&lt;code class=&quot;language-hcl&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;google_storage_bucket&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;nixos_image&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;name&lt;/span&gt;     &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite-nixos-image-bucket-name&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;location&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;EU&quot;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;module&lt;span class=&quot;token type variable&quot;&gt; &quot;nixos_image_custom&quot; &lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;source&lt;/span&gt;      &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;git::https://github.com/tweag/terraform-nixos.git//google_image_nixos_custom?ref=40fedb1fae7df5bd7ad9defdd71eb06b7252810f&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;bucket_name&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;google_storage_bucket&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;nixos_image&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;name&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;nixos_config&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token type variable&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;/nixos-config.nix&quot;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The snippet above first creates a bucket &lt;code class=&quot;language-text&quot;&gt;nixos_image&lt;/code&gt; where the generated
image will be uploaded, then it uses the &lt;code class=&quot;language-text&quot;&gt;nixos_image_custom&lt;/code&gt; module, which
handles generation of the image using the configuration from the
&lt;code class=&quot;language-text&quot;&gt;nixos-config.nix&lt;/code&gt; file. The file is assumed to be in the same directory as
the Terraform configuration, hence &lt;code class=&quot;language-text&quot;&gt;${path.module}/&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Service account and cache bucket&lt;/h2&gt;
&lt;p&gt;To control access to different resources we will also need a service
account:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-hcl&quot;&gt;&lt;code class=&quot;language-hcl&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;google_service_account&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_agent&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;account_id&lt;/span&gt;   &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite-agent&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;display_name&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Buildkite agent&quot;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can use it to set access permissions for the storage bucket that will contain the
Nix cache:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-hcl&quot;&gt;&lt;code class=&quot;language-hcl&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;google_storage_bucket&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;nix_cache_bucket&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;name&lt;/span&gt;     &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;nix-cache-bucket-name&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;location&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;EU&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;force_destroy&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;retention_policy&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;retention_period&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;7889238&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# three months&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;google_storage_bucket_iam_member&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_nix_cache_writer&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;google_storage_bucket&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;nix_cache_bucket&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;name&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;role&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;roles/storage.objectAdmin&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;member&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;serviceAccount:&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;google_service_account&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;buildkite_agent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;email&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;google_storage_bucket_iam_member&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_nix_cache_reader&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;google_storage_bucket&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;nix_cache_bucket&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;name&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;role&lt;/span&gt;   &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;roles/storage.objectViewer&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;member&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;allUsers&quot;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The bucket is configured to automatically delete objects that are older than
3 months. We give the service account the ability to write to and read from
the bucket (the &lt;code class=&quot;language-text&quot;&gt;roles/storage.objectAdmin&lt;/code&gt; role). The rest of the world
gets the ability to read from the bucket (the &lt;code class=&quot;language-text&quot;&gt;roles/storage.objectViewer&lt;/code&gt;
role).&lt;/p&gt;
&lt;h2&gt;NixOS configuration&lt;/h2&gt;
&lt;p&gt;Here is the content of my &lt;code class=&quot;language-text&quot;&gt;nixos-config.nix&lt;/code&gt;. This NixOS configuration
can serve as a starting point for writing your own. The numbered
points refer to the notes below.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-nix&quot;&gt;&lt;code class=&quot;language-nix&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; modulesPath&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; pkgs&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  imports &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token antiquotation variable&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;modulesPath&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;/virtualisation/google-compute-image.nix&quot;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  virtualisation&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;googleComputeImage&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;diskSize &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  virtualisation&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;docker&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;enable &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  services &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    buildkite&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;agents&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;agent &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
      enable &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
      extraConfig &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;''
      tags-from-gcp=true
      ''&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
      tags &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
        os &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;nixos&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
        nix &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
      tokenPath &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;/run/keys/buildkite-agent-token&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (1)&lt;/span&gt;
      runtimePackages &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; pkgs&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;
        bash
        curl
        gcc
        gnutar
        gzip
        ncurses
        nix
        python3
        xz
        &lt;span class=&quot;token comment&quot;&gt;# (2) extend as necessary&lt;/span&gt;
      &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    nix&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;store&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;gcs&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;proxy &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
      nix&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;cache&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;bucket&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;name &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (3)&lt;/span&gt;
        address &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;localhost:3000&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  nix &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    binaryCaches &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;token string&quot;&gt;&quot;https://cache.nixos.org/&quot;&lt;/span&gt;
      &lt;span class=&quot;token string&quot;&gt;&quot;https://storage.googleapis.com/nix-cache-bucket-name&quot;&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (4)&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    binaryCachePublicKeys &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;token string&quot;&gt;&quot;cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY=&quot;&lt;/span&gt;
      &lt;span class=&quot;token string&quot;&gt;&quot;&amp;lt;insert your public signing key here&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (5)&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    extraOptions &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;''
      post-build-hook = /etc/nix/upload-to-cache.sh # (6)
    ''&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  security&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sudo&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;enable &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  services&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;openssh&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;passwordAuthentication &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  security&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sudo&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;wheelNeedsPassword &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This file will be created later by the startup script (see below).&lt;/li&gt;
&lt;li&gt;The collection of packages that are available to the Buildkite script can
be edited here.&lt;/li&gt;
&lt;li&gt;Replace &lt;code class=&quot;language-text&quot;&gt;nix-cache-bucket-name&lt;/code&gt; by the name of the bucket used for the
Nix cache.&lt;/li&gt;
&lt;li&gt;Similarly to (3) replace &lt;code class=&quot;language-text&quot;&gt;nix-cache-bucket-name&lt;/code&gt; in the URL.&lt;/li&gt;
&lt;li&gt;Insert the contents of the &lt;code class=&quot;language-text&quot;&gt;key.public&lt;/code&gt; file you generated earlier.&lt;/li&gt;
&lt;li&gt;The file will be created later by the startup script.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Compute instances and startup script&lt;/h2&gt;
&lt;p&gt;The following snippet sets up an instance group manager which controls
multiple (3 in this example) Buildkite agents. The numbered
points refer to the notes below.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-hcl&quot;&gt;&lt;code class=&quot;language-hcl&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;data &lt;span class=&quot;token type variable&quot;&gt;&quot;template_file&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_nixos_startup&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (1)&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;${path.module}/files/buildkite_nixos_startup.sh&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;

  &lt;span class=&quot;token property&quot;&gt;vars&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;buildkite_agent_token&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;secret_resource&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;buildkite_agent_token&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;nix_signing_key&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;secret_resource&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;nix_signing_key&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;google_compute_instance_template&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_nixos&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;name_prefix&lt;/span&gt;  &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite-nixos-&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;machine_type&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;n1-standard-8&quot;&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;disk&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;boot&lt;/span&gt;         &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;disk_size_gb&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;source_image&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token type variable&quot;&gt;nixos_image_custom&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;self_link&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;token property&quot;&gt;metadata_startup_script&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token type variable&quot;&gt;template_file&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;buildkite_nixos_startup&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;rendered&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;network_interface&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;default&quot;&lt;/span&gt;

    &lt;span class=&quot;token property&quot;&gt;access_config&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;enable-oslogin&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;service_account&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;email&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;google_service_account&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;buildkite_agent&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;email&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;

    &lt;span class=&quot;token property&quot;&gt;scopes&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;token string&quot;&gt;&quot;compute-ro&quot;&lt;/span&gt;,
      &lt;span class=&quot;token string&quot;&gt;&quot;logging-write&quot;&lt;/span&gt;,
      &lt;span class=&quot;token string&quot;&gt;&quot;storage-rw&quot;&lt;/span&gt;,
    &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;scheduling&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;automatic_restart&lt;/span&gt;   &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;false&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;on_host_maintenance&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;TERMINATE&quot;&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;preemptible&lt;/span&gt;         &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (2)&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;lifecycle&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;create_before_destroy&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token boolean&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;resource &lt;span class=&quot;token type variable&quot;&gt;&quot;google_compute_instance_group_manager&quot;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_nixos&quot;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;provider&lt;/span&gt;           &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;google-beta&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;name&lt;/span&gt;               &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite-nixos&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;base_instance_name&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite-nixos&quot;&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;target_size&lt;/span&gt;        &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;3&quot;&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (3)&lt;/span&gt;
  &lt;span class=&quot;token property&quot;&gt;zone&lt;/span&gt;               &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&amp;lt;your-zone&amp;gt;&quot;&lt;/span&gt; &lt;span class=&quot;token comment&quot;&gt;# (4)&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;version&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;name&lt;/span&gt;              &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;buildkite_nixos&quot;&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;instance_template&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;google_compute_instance_template&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;buildkite_nixos&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;self_link&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;update_policy&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;type&lt;/span&gt;                  &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;PROACTIVE&quot;&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;minimal_action&lt;/span&gt;        &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;REPLACE&quot;&lt;/span&gt;
    &lt;span class=&quot;token property&quot;&gt;max_unavailable_fixed&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Notes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The file &lt;code class=&quot;language-text&quot;&gt;files/buildkite_nixos_startup.sh&lt;/code&gt; is shown below.&lt;/li&gt;
&lt;li&gt;Because of the remote Nix cache, the nodes can be preemptible
(short-lived, never lasting longer than 24 hours), which results in much
lower GCP costs.&lt;/li&gt;
&lt;li&gt;Changing &lt;code class=&quot;language-text&quot;&gt;target_size&lt;/code&gt; allows you to scale the system. This is the number
of instances that are controlled by the instance group manager.&lt;/li&gt;
&lt;li&gt;Insert your desired zone here.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, here is the startup script:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-bash&quot;&gt;&lt;code class=&quot;language-bash&quot;&gt;&lt;span class=&quot;token comment&quot;&gt;# workaround https://github.com/NixOS/nixpkgs/issues/42344&lt;/span&gt;
&lt;span class=&quot;token function&quot;&gt;chown&lt;/span&gt; root:keys /run/keys
&lt;span class=&quot;token function&quot;&gt;chmod&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;750&lt;/span&gt; /run/keys
&lt;span class=&quot;token builtin class-name&quot;&gt;umask&lt;/span&gt; 037
&lt;span class=&quot;token builtin class-name&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;&lt;span class=&quot;token variable&quot;&gt;${buildkite_agent_token}&lt;/span&gt;&quot;&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt; /run/keys/buildkite-agent-token
&lt;span class=&quot;token function&quot;&gt;chown&lt;/span&gt; root:keys /run/keys/buildkite-agent-token
&lt;span class=&quot;token builtin class-name&quot;&gt;umask&lt;/span&gt; 077
&lt;span class=&quot;token builtin class-name&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;'&lt;span class=&quot;token variable&quot;&gt;${nix_signing_key}&lt;/span&gt;'&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt; /run/keys/nix_signing_key
&lt;span class=&quot;token function&quot;&gt;chown&lt;/span&gt; root:keys /run/keys/nix-signing-key

&lt;span class=&quot;token function&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;EOF &lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt; /etc/nix/upload-to-cache.sh
&lt;span class=&quot;token comment&quot;&gt;#!/bin/sh&lt;/span&gt;

&lt;span class=&quot;token builtin class-name&quot;&gt;set&lt;/span&gt; -eu
&lt;span class=&quot;token builtin class-name&quot;&gt;set&lt;/span&gt; -f &lt;span class=&quot;token comment&quot;&gt;# disable globbing&lt;/span&gt;
&lt;span class=&quot;token builtin class-name&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;token assign-left variable&quot;&gt;&lt;span class=&quot;token environment constant&quot;&gt;IFS&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;' '&lt;/span&gt;

&lt;span class=&quot;token builtin class-name&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;Uploading paths&quot;&lt;/span&gt; &lt;span class=&quot;token variable&quot;&gt;$OUT_PATHS&lt;/span&gt;
&lt;span class=&quot;token builtin class-name&quot;&gt;exec&lt;/span&gt; nix copy --to http://localhost:3000?secret-key&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;/run/keys/nix_signing_key &lt;span class=&quot;token punctuation&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;token variable&quot;&gt;$OUT_PATHS&lt;/span&gt;
EOF
&lt;span class=&quot;token function&quot;&gt;chmod&lt;/span&gt; +x /etc/nix/upload-to-cache.sh&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This script uses the Nix &lt;a href=&quot;https://www.tweag.io/posts/2019-11-21-untrusted-ci.html&quot;&gt;post build hook&lt;/a&gt; approach for
uploading to the cache without polluting the CI script.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The setup allows us to run Nix builds in an environment where Nix tooling is
available. It also provides a remote Nix cache which does not require
that the
authors of CI scripts set it up or, even, be aware of it at all. We use this setup on
many of Tweag’s projects and found that both mental and performance overheads are
minimal. A typical CI script looks like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-yaml&quot;&gt;&lt;code class=&quot;language-yaml&quot;&gt;&lt;span class=&quot;token key atrule&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token key atrule&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; Build and test
    &lt;span class=&quot;token key atrule&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; nix&lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt;build &lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt;A distributed&lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt;closure &lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt;no&lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt;out&lt;span class=&quot;token punctuation&quot;&gt;-&lt;/span&gt;link&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Builds with up-to-date cache that does not cause re-builds may finish in
literally 1 second.&lt;/p&gt;</description>
	<pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
</item>
<item>
	<title>Tweag I/O: Nix Flakes, Part 2: Evaluation caching</title>
	<guid isPermaLink="true">https://tweag.io/blog/2020-06-25-eval-cache/</guid>
	<link>https://tweag.io/blog/2020-06-25-eval-cache/</link>
	<description>&lt;p&gt;Nix evaluation is often quite slow. In this blog post, we’ll have a
look at a nice advantage of the hermetic evaluation model enforced by
flakes: the ability to cache evaluation results reliably. For a short
introduction to flakes, see our &lt;a href=&quot;https://www.tweag.io/blog/2020-05-25-flakes/&quot;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Why Nix evaluation is slow&lt;/h2&gt;
&lt;p&gt;Nix uses a simple, interpreted, purely functional language to describe
package dependency graphs and NixOS system configurations. So to get
any information about those things, Nix first needs to &lt;em&gt;evaluate&lt;/em&gt; a
substantial Nix program. This involves parsing potentially thousands
of .nix files and running a Turing-complete language.&lt;/p&gt;
&lt;p&gt;For example, the command &lt;code class=&quot;language-text&quot;&gt;nix-env -qa&lt;/code&gt; shows you which packages are
available in Nixpkgs. But this is quite slow and takes a lot of memory:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-shell-session&quot;&gt;&lt;code class=&quot;language-shell-session&quot;&gt;&lt;span class=&quot;token command&quot;&gt;&lt;span class=&quot;token shell-symbol important&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;token bash language-bash&quot;&gt;&lt;span class=&quot;token builtin class-name&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;time&lt;/span&gt; nix-env -qa &lt;span class=&quot;token operator&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;wc&lt;/span&gt; -l&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;token output&quot;&gt;5.09user 0.49system 0:05.59elapsed 99%CPU (0avgtext+0avgdata 1522792maxresident)k
28012&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Evaluating individual packages or configurations can also be slow. For
example, using &lt;code class=&quot;language-text&quot;&gt;nix-shell&lt;/code&gt; to enter a development environment for
&lt;a href=&quot;https://github.com/NixOS/hydra&quot;&gt;Hydra&lt;/a&gt;, we have to wait a bit, even
if all dependencies are present in the Nix store:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-shell-session&quot;&gt;&lt;code class=&quot;language-shell-session&quot;&gt;&lt;span class=&quot;token command&quot;&gt;&lt;span class=&quot;token shell-symbol important&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;token bash language-bash&quot;&gt;&lt;span class=&quot;token builtin class-name&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;time&lt;/span&gt; nix-shell --command &lt;span class=&quot;token string&quot;&gt;'exit 0'&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;token output&quot;&gt;1.34user 0.18system 0:01.69elapsed 89%CPU (0avgtext+0avgdata 434808maxresident)k&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That might be okay for occasional use but a wait of one or more
seconds may well be unacceptably slow in scripts.&lt;/p&gt;
&lt;p&gt;Note that the evaluation overhead is completely independent from the
time it takes to actually build or download a package or
configuration. If something is already present in the Nix store, Nix
won’t build or download it again. But it still needs to re-evaluate
the Nix files to determine &lt;em&gt;which&lt;/em&gt; Nix store paths are needed.&lt;/p&gt;
&lt;h2&gt;Caching evaluation results&lt;/h2&gt;
&lt;p&gt;So can’t we speed things up by &lt;em&gt;caching&lt;/em&gt; evaluation results? After
all, the Nix language is purely functional, so it seems that
re-evaluation should produce the same result, every time. Naively,
maybe we can keep a cache that records that attribute A of file X
evaluates to derivation D (or whatever metadata we want to cache).
Unfortunately, it’s not that simple; cache invalidation is, after all,
one of the &lt;a href=&quot;https://martinfowler.com/bliki/TwoHardThings.html&quot;&gt;only two hard problems in computer
science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reason this didn’t work is that in the past Nix evaluation was not
&lt;em&gt;hermetic&lt;/em&gt;. For example, a &lt;code class=&quot;language-text&quot;&gt;.nix&lt;/code&gt; file can import other Nix files
through relative or absolute paths (such as
&lt;code class=&quot;language-text&quot;&gt;~/.config/nixpkgs/config.nix&lt;/code&gt; for Nixpkgs) or by looking them up in
the Nix search path (&lt;code class=&quot;language-text&quot;&gt;$NIX_PATH&lt;/code&gt;). So unless we perfectly keep track
of &lt;em&gt;all&lt;/em&gt; the files used during evaluation, a cached result might be
inconsistent with the current input.&lt;/p&gt;
&lt;p&gt;(As an aside: for a while, Nix has had an experimental replacement for
&lt;code class=&quot;language-text&quot;&gt;nix-env -qa&lt;/code&gt; called &lt;code class=&quot;language-text&quot;&gt;nix search&lt;/code&gt;, which used an ad hoc cache for
package metadata. It had exactly this cache invalidation problem: it
wasn’t smart enough to figure out whether its cache was up to date
with whatever revision of Nixpkgs you were using. So it had a manual
flag &lt;code class=&quot;language-text&quot;&gt;--update-cache&lt;/code&gt; to allow the user to force cache invalidation.)&lt;/p&gt;
&lt;h2&gt;Flakes to the rescue&lt;/h2&gt;
&lt;p&gt;Flakes solve this problem by ensuring fully hermetic evaluation. When
you evaluate an output attribute of a particular flake (e.g. the
attribute &lt;code class=&quot;language-text&quot;&gt;defaultPackage.x86_64-linux&lt;/code&gt; of the &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt; flake), Nix
disallows access to any files outside that flake or its
dependencies. It also disallows impure or platform-dependent features
such as access to environment variables or the current system type.&lt;/p&gt;
&lt;p&gt;This allows the &lt;code class=&quot;language-text&quot;&gt;nix&lt;/code&gt; command to aggressively cache evaluation results
without fear of cache invalidation problems. Let’s see this in action
by running Firefox from the &lt;code class=&quot;language-text&quot;&gt;nixpkgs&lt;/code&gt; flake. If we do this with an
empty evaluation cache, Nix needs to evaluate the entire dependency
graph of Firefox, which takes a quarter of a second:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;$ command time nix shell nixpkgs#firefox -c firefox --version
Mozilla Firefox 75.0
0.26user 0.05system 0:00.39elapsed 82%CPU (0avgtext+0avgdata 115224maxresident)k&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But if we do it again, it’s almost instantaneous (and takes less
memory):&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;$ command time nix shell nixpkgs#firefox -c firefox --version
Mozilla Firefox 75.0
0.01user 0.01system 0:00.03elapsed 93%CPU (0avgtext+0avgdata 25840maxresident)k&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The cache is implemented using a simple SQLite database that stores the
values of flake output attributes. After the first command above, the
cache looks like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-shell-session&quot;&gt;&lt;code class=&quot;language-shell-session&quot;&gt;&lt;span class=&quot;token command&quot;&gt;&lt;span class=&quot;token shell-symbol important&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;token bash language-bash&quot;&gt;sqlite3 ~/.cache/nix/eval-cache-v1/302043eedfbce13ecd8169612849f6ce789c26365c9aa0e6cfd3a772d746e3ba.sqlite .dump&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;token output&quot;&gt;PRAGMA foreign_keys=OFF;
BEGIN TRANSACTION;
CREATE TABLE Attributes (
    parent      integer not null,
    name        text,
    type        integer not null,
    value       text,
    primary key (parent, name)
);
INSERT INTO Attributes VALUES(0,'',0,NULL);
INSERT INTO Attributes VALUES(1,'packages',3,NULL);
INSERT INTO Attributes VALUES(1,'legacyPackages',0,NULL);
INSERT INTO Attributes VALUES(3,'x86_64-linux',0,NULL);
INSERT INTO Attributes VALUES(4,'firefox',0,NULL);
INSERT INTO Attributes VALUES(5,'type',2,'derivation');
INSERT INTO Attributes VALUES(5,'drvPath',2,'/nix/store/7mz8pkgpl24wyab8nny0zclvca7ki2m8-firefox-75.0.drv');
INSERT INTO Attributes VALUES(5,'outPath',2,'/nix/store/5x1i2gp8k95f2mihd6aj61b5lydpz5dy-firefox-75.0');
INSERT INTO Attributes VALUES(5,'outputName',2,'out');
COMMIT;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In other words, the cache stores all the attributes that &lt;code class=&quot;language-text&quot;&gt;nix shell&lt;/code&gt;
had to evaluate, in particular
&lt;code class=&quot;language-text&quot;&gt;legacyPackages.x86_64-linux.firefox.{type,drvPath,outPath,outputName}&lt;/code&gt;. It
also stores negative lookups, that is, attributes that don’t exist
(such as &lt;code class=&quot;language-text&quot;&gt;packages&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The name of the SQLite database, &lt;code class=&quot;language-text&quot;&gt;302043eedf….sqlite&lt;/code&gt; in this example,
is derived from the contents of the top-level flake. Since the flake’s
lock file contains content hashes of all dependencies, this is enough
to efficiently and completely capture all files that might influence
the evaluation result. (In the future, we’ll optimise this a bit more:
for example, if the flake is a Git repository, we can simply use the
Git revision as the cache name.)&lt;/p&gt;
&lt;p&gt;The &lt;code class=&quot;language-text&quot;&gt;nix search&lt;/code&gt; command has been updated to use the new evaluation
cache instead of its previous ad hoc cache. For example, searching for
Blender is slow the first time:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-shell-session&quot;&gt;&lt;code class=&quot;language-shell-session&quot;&gt;&lt;span class=&quot;token command&quot;&gt;&lt;span class=&quot;token shell-symbol important&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;token bash language-bash&quot;&gt;&lt;span class=&quot;token builtin class-name&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;time&lt;/span&gt; nix search nixpkgs blender&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;token output&quot;&gt;* legacyPackages.x86_64-linux.blender (2.82a)
  3D Creation/Animation/Publishing System
5.55user 0.63system 0:06.17elapsed 100%CPU (0avgtext+0avgdata 1491912maxresident)k&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;but the second time it is pretty fast and uses much less memory:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-shell-session&quot;&gt;&lt;code class=&quot;language-shell-session&quot;&gt;&lt;span class=&quot;token command&quot;&gt;&lt;span class=&quot;token shell-symbol important&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;token bash language-bash&quot;&gt;&lt;span class=&quot;token builtin class-name&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;time&lt;/span&gt; nix search nixpkgs blender&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;token output&quot;&gt;* legacyPackages.x86_64-linux.blender (2.82a)
  3D Creation/Animation/Publishing System
0.41user 0.00system 0:00.42elapsed 99%CPU (0avgtext+0avgdata 21100maxresident)k&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The evaluation cache at this point is about 10.9 MiB in size. The
overhead for creating the cache is fairly modest: with the flag
&lt;code class=&quot;language-text&quot;&gt;--no-eval-cache&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;nix search nixpkgs blender&lt;/code&gt; takes 4.9 seconds.&lt;/p&gt;
&lt;h2&gt;Caching and store derivations&lt;/h2&gt;
&lt;p&gt;There is only one way in which cached results can become “stale”, in a
way. Nix evaluation produces store derivations such as
&lt;code class=&quot;language-text&quot;&gt;/nix/store/7mz8pkgpl24wyab8nny0zclvca7ki2m8-firefox-75.0.drv&lt;/code&gt; as a
side effect. (&lt;code class=&quot;language-text&quot;&gt;.drv&lt;/code&gt; files are essentially a serialization of the
dependency graph of a package.) These store derivations may be
garbage-collected. In that case, the evaluation cache points to a path
that no longer exists. Thus, Nix checks whether the &lt;code class=&quot;language-text&quot;&gt;.drv&lt;/code&gt; file still
exist, and if not, falls back to evaluating normally.&lt;/p&gt;
&lt;h2&gt;Future improvements&lt;/h2&gt;
&lt;p&gt;Currently, the evaluation cache is only created and used
locally. However, Nix could automatically &lt;em&gt;download&lt;/em&gt; precomputed
caches, similar to how it has a binary cache for the contents of store
paths. That is, if we need a cache like &lt;code class=&quot;language-text&quot;&gt;302043eedf….sqlite&lt;/code&gt;, we could
first check if it’s available on &lt;code class=&quot;language-text&quot;&gt;cache.nixos.org&lt;/code&gt; and if so fetch it
from there. In this way, when we run a command such as &lt;code class=&quot;language-text&quot;&gt;nix shell nixpkgs#firefox&lt;/code&gt;, we could even avoid the need to fetch the actual
source of the flake!&lt;/p&gt;
&lt;p&gt;Another future improvement is to populate and use the cache in the
evaluator itself. Currently the cache is populated and cached in the
user interface (that is, the &lt;code class=&quot;language-text&quot;&gt;nix&lt;/code&gt; command). The command &lt;code class=&quot;language-text&quot;&gt;nix shell nixpkgs#firefox&lt;/code&gt; will create a cache entry for &lt;code class=&quot;language-text&quot;&gt;firefox&lt;/code&gt;, but not for
the dependencies of &lt;code class=&quot;language-text&quot;&gt;firefox&lt;/code&gt;; thus a subsequent &lt;code class=&quot;language-text&quot;&gt;nix shell nixpkgs#thunderbird&lt;/code&gt; won’t see a speed improvement even though it
shares most of its dependencies. So it would be nice if the evaluator
had knowledge of the evaluation cache. For example, the evaluation of
thunks that represent attributes like
&lt;code class=&quot;language-text&quot;&gt;nixpkgs.legacyPackages.x86_64-linux.&amp;lt;package name&amp;gt;&lt;/code&gt; could check and
update the cache.&lt;/p&gt;</description>
	<pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
</item>
<item>
	<title>nixbuild.net: Automatic Resource Optimization</title>
	<guid isPermaLink="true">https://blog.nixbuild.net/posts/2020-06-25-automatic-resource-optimization.html</guid>
	<link>https://blog.nixbuild.net/posts/2020-06-25-automatic-resource-optimization.html</link>
	<description>&lt;p&gt;As of today, nixbuild.net will automatically select resources (CPU count and memory amount) for builds submitted to it. Based on historic build data, nixbuild.net calculates a resource allocation that will make your build as performant as possible, while wasting minimal CPU time. This means nixbuild.net users get faster and cheaper builds, while also taking away the user’s burden of figuring out what resource settings to use for each individual build.&lt;/p&gt;

&lt;p&gt;Previously, all builds were assigned 4 CPUs unless the user configured resource selection differently. However, configuring different resource settings for individual builds was difficult, since Nix has no notion of such settings. Additionally, it is really tricky to know wether a build will gain anything from being allocated many CPUs, or if it just makes the build more expensive. It generally requires the user to try out the build with different settings, which is time-consuming for a single build and almost insurmountable for a large set of builds with different characteristics.&lt;/p&gt;
&lt;p&gt;Now, each individual build will be analyzed and can be assigned between 1 and 16 CPUs, depending on how well the build utilizes multiple CPUs. The memory allocation will be adapted to minimize the amount of unused memory.&lt;/p&gt;
&lt;p&gt;The automatic resource optimization has been tested both internally and by a selected number of beta users, and the results have been very positive so far. We’re happy to make this feature available to all nixbuild.net users, since it aligns perfectly with the service’s core idea of being simple, cost-effective and performant.&lt;/p&gt;
&lt;h2 id=&quot;how-does-it-work&quot;&gt;How Does it Work?&lt;/h2&gt;
&lt;p&gt;The automatic resource optimization works in two steps:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;When a Nix derivation is submitted to nixbuild.net, we look for similar derivations that have been built on nixbuild.net before. A heuristic approach is used, where derivations are compared based on package names and version numbers. This approach can be improved in the future, by looking at more parts of the derivations, like dependencies and build scripts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A number of the most recent, most similar derivations are selected. We then analyze the build data of those derivations. Since we have developed a secure sandbox specifically for running Nix builds, we’re also able to collect a lot of data about the builds. One metric that is collected is CPU utilization, and that lets us make predictions about how well a build would scale, performance-wise, if it was given more CPUs.&lt;/p&gt;
&lt;p&gt;We also look at metrics about the historic memory usage, and make sure the new build is allocated enough memory.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
	<pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
	<author>support@nixbuild.net (nixbuild.net)</author>
</item>
<item>
	<title>Tweag I/O: Long-term reproducibility with Nix and Software Heritage</title>
	<guid isPermaLink="true">https://tweag.io/blog/2020-06-18-software-heritage/</guid>
	<link>https://tweag.io/blog/2020-06-18-software-heritage/</link>
	<description>&lt;p&gt;Reproducible builds and deployments — this is the ambition that Nix proclaims for itself.
This ambition comes, however, with a fine print: builds are reproducible &lt;em&gt;only if the original source code still exists&lt;/em&gt;.
Otherwise, Nix can’t download, build and deploy it.
The community maintains binary caches like &lt;code class=&quot;language-text&quot;&gt;cache.nixos.org&lt;/code&gt;, but these don’t preserve anything — caches are ephemeral.
After all, preserving source code is not Nix’s primary mission!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.softwareheritage.org/&quot;&gt;Software Heritage&lt;/a&gt;, on the other hand, aspires to preserve software forever.
Software Heritage is an independent foundation, grounded in academia, supported by public money, and backed by many of the world’s largest software companies.
It can thus reliably pursue the tedious task of collecting and archiving public code and making it available through its website.&lt;/p&gt;
&lt;p&gt;Quite naturally, this situation suggested an opportunity for collaboration:
can we combine Nix’s reproducible build definitions with Software Heritage’s long-term archive?
Will this collaboration bring us closer to the dream of forever replicable and successful builds?
We thought that this was an effort worth pursuing, so a &lt;a href=&quot;https://www.softwareheritage.org/2020/06/18/welcome-nixpgks&quot;&gt;partnership started&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is a story about the challenges that we encountered when trying to make this happen, what we already achieved, and the lessons that we have learned.&lt;/p&gt;
&lt;h2&gt;Archiving all sources in Nixpkgs&lt;/h2&gt;
&lt;p&gt;Whenever a Nix user installs a package that isn’t in the binary cache, Nix tries to download the original source and build it from scratch.
When these sources don’t exist anymore, or don’t correspond anymore to what Nix expects them to be, the experience can be frustrating —
instead of a reproducible build, the user ends up with a “file not found” HTTP error message.&lt;/p&gt;
&lt;p&gt;With a long-term source code archive at hand, we could simply redirect the download request to it and move on with the build process.
In other words, we would like to make Nix fall back on the Software Heritage archive to download the missing source from there.&lt;/p&gt;
&lt;p&gt;For this to work, we must ensure that the source code has been fed to the Software Heritage archive previously. The best way to do it is
simply to tell Software Heritage which source code we would like to have archived. Indeed, Software Heritage’s long term goal is to archive
all source code produced in the world, and is quite eager to get pointed to locations where to get more!&lt;/p&gt;
&lt;p&gt;The first step of this joint effort was to compile a list of all source code URLs required by Nixpkgs, and make them available to Software Heritage.
A &lt;a href=&quot;https://github.com/nix-community/nixpkgs-swh&quot;&gt;Nix community project&lt;/a&gt; is in charge of generating the list of source code URLs required by a Nixpkgs build, and it is available &lt;a href=&quot;https://nix-community.github.io/nixpkgs-swh/sources-unstable.json&quot;&gt;here&lt;/a&gt;.
This list indexes every tarball used by the Nixpkgs &lt;code class=&quot;language-text&quot;&gt;master&lt;/code&gt; branch, with other sources such as patches, JAR’s, or git clones being currently excluded, and hence not archived.&lt;/p&gt;
&lt;p&gt;A source list is a simple JSON file that looks like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;{
  &quot;sources&quot;: [
    {
      &quot;type&quot;: &quot;url&quot;,
      &quot;urls&quot;: [
        &quot;https://ftpmirror.gnu.org/hello/hello-2.10.tar.gz&quot;,
      ],
      &quot;integrity&quot;: &quot;sha256-MeBmE3qWJnbon2nRtlOC3pWn732RS4y5VvQepy4PUWs=&quot;,
    },
    ...
  ],
  &quot;version&quot;: 1,
  &quot;revision&quot;: &quot;cc4e04c26672dd74e5fd0fecb78b435fb55368f7&quot;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;version&lt;/code&gt; is the version of this file’s format,&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;revision&lt;/code&gt; is a Nixpkgs commit ID,&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;type&lt;/code&gt; is the type of the source. Only the &lt;code class=&quot;language-text&quot;&gt;url&lt;/code&gt; type is currently supported,&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;integrity&lt;/code&gt; is the hash of the Nix fixed output derivation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We then implemented a Software Heritage &lt;a href=&quot;https://docs.softwareheritage.org/devel/glossary.html#term-loader&quot;&gt;loader&lt;/a&gt; which fetches this JSON file once per day and archives all listed tarballs in a snapshot.&lt;/p&gt;
&lt;p&gt;You can see an example snapshot &lt;a href=&quot;https://archive.softwareheritage.org/browse/origin/directory/?origin_url=https://nix-community.github.io/nixpkgs-swh/sources-unstable.json&amp;amp;timestamp=2020-06-03T11:25:05Z&quot;&gt;here&lt;/a&gt;.
This snapshot was created the 03 June 2020, points to the Nixpkgs commit &lt;a href=&quot;https://github.com/NixOS/nixpkgs/tree/46fcaf3c8a13f32e2c147fd88f97c4ad2d3b0f27&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;46fcaf3c8a13f32e2c147fd88f97c4ad2d3b0f27&lt;/code&gt;&lt;/a&gt; and contains 21,100 branches, each one corresponding to the tarballs used by this Nixpkgs revision.&lt;/p&gt;
&lt;p&gt;Every day, Software Heritage is now archiving more than 21,000 tarballs used by Nixpkgs.
But how we can plug them back into Nix?&lt;/p&gt;
&lt;h2&gt;Falling back on Software Heritage&lt;/h2&gt;
&lt;p&gt;Two issues need to be addressed to make this happen:&lt;/p&gt;
&lt;p&gt;First, we need to find the correct source, the one that Nix tried to download unsuccessfully, in the Software Heritage archive.
We cannot simply use the original source URL to query the Software Heritage archive because the URL alone doesn’t identify uniquely the content that is behind it.
What if the code that a URL points to has changed over time?
To check this, Nix associates a hash with each downloaded artifact.
Such a build step is called a &lt;a href=&quot;https://nixos.org/nix/manual/#fixed-output-drvs&quot;&gt;fixed output derivation&lt;/a&gt; because Nix verifies that the output hash remains immitigably unchanged.
It thus ensures that the content that it downloads is always the same, and emits an error otherwise.
What we therefore really want is querying Software Heritage with the &lt;em&gt;contents&lt;/em&gt;, that is, the hash of the source code artifact itself.&lt;/p&gt;
&lt;p&gt;Let’s do it then!
Does this mean that the problem is solved?
Unfortunately not, and the reason for this are the design decisions behind Nix, Software Heritage and other package repositories, that are not fully compatible.
And tarballs play a central role in this.&lt;/p&gt;
&lt;h1&gt;Content Hash vs Tarball Hash&lt;/h1&gt;
&lt;p&gt;When Software Heritage archives a tarball, it first unpacks it and then stores all files and directories.
This makes it possible to &lt;em&gt;deduplicate&lt;/em&gt; files: when two tarballs contain the same file it is only stored once.
This is a nice solution, but unfortunately, this new tarball may differ from the original one — for example, if file permissions or timestamps were not preserved in this reassembly procedure.
Since Nix computes the checksum of the tarball, it will detect that it differs from the hash of the original, and has no means to ensure that the tarball downloaded from the Software Heritage archive corresponds to the one it expects.&lt;/p&gt;
&lt;p&gt;Nix, being really stubborn with respect to reproducibility, computes the checksum of the tarball, and detects that it differs from the hash of the original.
Nix simply cannot ensure that the tarball downloaded from the Software Heritage archive corresponds to the one it expects, and rightly so because this could easily lead to a different build.&lt;/p&gt;
&lt;p&gt;Nix already deals with some situations where the hash of a source tarball can change without affecting build reproducibility.
For example, &lt;code class=&quot;language-text&quot;&gt;fetchFromGitHub&lt;/code&gt; unpacks the tarball &lt;em&gt;before&lt;/em&gt; computing the hash of the unpacked contents.
This is because GitHub produces release tarballs on the fly, and a change in the compression algorithm can invalidate the hash expected by Nix.
It happens that when Nix uses the &lt;code class=&quot;language-text&quot;&gt;fetchFromGithub&lt;/code&gt; strategy, it manages to download tarballs from Software Heritage.
Currently, this is the case for about 6,000 sources in Nixpkgs.&lt;/p&gt;
&lt;p&gt;For all other sources, we would have to modify the Nixpkgs fetchers to compute checksums on the unpacked tarball contents.
This seems reasonable, since we want to make sure that the source code is the same, and we don’t care much about the format in it is transferred (which can also be important for security reasons).
However, using the hash of the tarball directly is often more convenient, since it is exposed by many repositories such as &lt;a href=&quot;https://pypi.org&quot;&gt;Pypi&lt;/a&gt; and &lt;a href=&quot;https://hackage.haskell.org&quot;&gt;Hackage&lt;/a&gt;.
When updating a package in Nixpkgs, the maintainer can just pick the checksum provided by the package repository, instead of recalculating it locally.&lt;/p&gt;
&lt;p&gt;Ideally, Nix fetchers and these package repositories would provide checksums of the unpacked &lt;em&gt;contents&lt;/em&gt; of their packages as much as possible.
This would give us the freedom to identify content independently of the way it is packed, transferred and stored.
We haven’t yet decided how to move on, but rest assured that we will continue to work on it.&lt;/p&gt;
&lt;h1&gt;Wrap up&lt;/h1&gt;
&lt;p&gt;Thanks to Software Heritage, a significant part of the sources used by Nixpkgs are now archived forever.
Moreover, about 6,000 out of the 21,000 tarballs used by Nixpkgs can already be used by the future Nix fallback mechanism.
We now want to increase the number of archived source code tarballs, add support for Git sources and start to implement the fallback mechanism in Nix.
At the same time, we aim to increase the number of fixed output derivations whose hash is computed on the unpacked tarball.&lt;/p&gt;
&lt;p&gt;We want build reproducibility to become the standard in the software world.
For this reason, the Software Heritage loader archiving Nixpkgs source code tarballs has been designed to be easily reused by other actors.
For example, the &lt;a href=&quot;https://guix.gnu.org/&quot;&gt;Guix&lt;/a&gt; project already started publishing and archiving their sources using the same component!
Finally, we want to thank &lt;a href=&quot;https://nlnet.nl/&quot;&gt;NLnet&lt;/a&gt; for the funding that makes this work possible.&lt;/p&gt;</description>
	<pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
</item>
<item>
	<title>Mayflower: Windows-on-NixOS, part 2: Make it go fast!</title>
	<guid isPermaLink="true">https://nixos.mayflower.consulting/blog/2020/06/17/windows-vm-performance/</guid>
	<link>https://nixos.mayflower.consulting/blog/2020/06/17/windows-vm-performance/</link>
	<description>This is part 2 of a series of blog posts explaining how we took an existing Windows installation on hardware and moved it into a VM running on top of NixOS. Previously, we discussed how we performed the actual storage migration. In this post, we’ll cover the various performance optimisations we tried, what worked, and what didn’t work.
GPU passthrough Since the machine is, amongst other things, used for gaming, graphics performance is critical.</description>
	<pubDate>Wed, 17 Jun 2020 09:00:00 +0000</pubDate>
</item>
<item>
	<title>Sander van der Burg: Using Disnix as a simple and minimalistic dependency-based process manager</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-1397115249631682228.post-690526319663316035</guid>
	<link>http://sandervanderburg.blogspot.com/2020/06/using-disnix-as-simple-and-minimalistic.html</link>
	<description>In &lt;a href=&quot;https://sandervanderburg.blogspot.com/2020/05/deploying-heterogeneous-service.html&quot;&gt;my previous blog post&lt;/a&gt; I have demonstrated that I can deploy an entire service-oriented system locally with &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/02/disnix-toolset-for-distributed.html&quot;&gt;Disnix&lt;/a&gt; without the need of obtaining any external physical or virtual machines (or even Linux containers).&lt;br /&gt;&lt;br /&gt;The fact that I could do this with relative ease is a benefit of using &lt;a href=&quot;https://sandervanderburg.blogspot.com/2020/02/a-declarative-process-manager-agnostic.html&quot;&gt;my experimental process manager-agnostic deployment framework&lt;/a&gt; that I have developed earlier, allowing you to target a variety of process management solutions with the same declarative deployment specifications.&lt;br /&gt;&lt;br /&gt;Most notably, the fact that the framework can also work with processes that &lt;a href=&quot;https://en.wikipedia.org/wiki/Daemon_(computing)&quot;&gt;daemonize&lt;/a&gt; and let foreground processes automatically daemonize, make it very convenient to do local unprivileged user deployments.&lt;br /&gt;&lt;br /&gt;To refresh your memory: a process that daemonizes spawns another process that keeps running in the background while the invoking process terminates after the initialization is done. Since there is no way for the caller to know the PID of the daemon process, daemons typically follow the convention to write a PID file to disk (containing the daemon's process ID), so that it can eventually be reliably terminated.&lt;br /&gt;&lt;br /&gt;In addition to spawning a daemon process that remains in the background, services should also implement a number of steps to make it &lt;a href=&quot;https://sandervanderburg.blogspot.com/2020/01/writing-well-behaving-daemon-in-c.html&quot;&gt;&lt;b&gt;well-behaving&lt;/b&gt;&lt;/a&gt;, such as resetting signals handlers, clearing privacy sensitive environment variables, and dropping privileges etc.&lt;br /&gt;&lt;br /&gt;In earlier blog posts, I argued that managing foreground processes with a process manager is typically more reliable (e.g. a PID of a foreground process is always known to be right).&lt;br /&gt;&lt;br /&gt;On the other hand, processes that daemonize also have certain advantages:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;They are &lt;b&gt;self contained&lt;/b&gt; -- they do not rely on any external services to operate. This makes it very easy to run a collection of processes for local experimentation.&lt;/li&gt;&lt;li&gt;They have a &lt;b&gt;standard means&lt;/b&gt; to &lt;b&gt;notify&lt;/b&gt; the caller that the service is ready. By convention, the executable that spawns the daemon process is only supposed to terminate when the daemon has been successfully initialized. For example, foreground processes that are managed by &lt;a href=&quot;https://www.freedesktop.org/wiki/Software/systemd&quot;&gt;systemd&lt;/a&gt;, should invoke the non-standard &lt;i&gt;sd_notify()&lt;/i&gt; function to notify systemd that they are ready.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Although these concepts are nice, properly daemonizing a process is the responsibility of the service implementer -- as a consequence, it is not a guarantee that all services will properly implement all steps to make a daemon well-behaving.&lt;br /&gt;&lt;br /&gt;Since the management of daemons is straight forward and self contained, the &lt;a href=&quot;https://sandervanderburg.blogspot.com/2012/11/an-alternative-explaination-of-nix.html&quot;&gt;Nix expression language&lt;/a&gt; provides all kinds of advantages over data-oriented configuration languages (e.g. JSON or YAML) and Disnix has a flexible deployment model that works with a dependency graph and a plugin system that can activate and deactivate all kinds of components, I realized that I could integrate these facilities to make my own simple dependency-based process manager.&lt;br /&gt;&lt;br /&gt;In this blog post, I will describe how this process management approach works.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Specifying a process configuration&lt;/h2&gt;&lt;br /&gt;A simple Nix expression capturing a daemon deployment configuration might look as follows:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;{writeTextFile, mydaemon}:&lt;br /&gt;&lt;br /&gt;writeTextFile {&lt;br /&gt;  name = &quot;mydaemon&quot;;&lt;br /&gt;  text = ''&lt;br /&gt;    process=${mydaemon}/bin/mydaemon&lt;br /&gt;    pidFile=/var/run/mydaemon.pid&lt;br /&gt;  '';&lt;br /&gt;  destination = &quot;/etc/dysnomia/process&quot;;&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above Nix expression generates a textual configuration file:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The &lt;i&gt;process&lt;/i&gt; field specifies the path to executable to start (that in turn spawns a deamon process that keeps running in the background).&lt;/li&gt;&lt;li&gt;The &lt;i&gt;pidFile&lt;/i&gt; field indicates the location of the PID file containing the process ID of the daemon process, so that it can be reliably terminated.&lt;br /&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Most common system services (e.g. the Apache HTTP server, MySQL and PostgreSQL) can daemonize on their own and follow the same conventions. As a result, the deployment system can save you some configuration work by providing reasonable default values:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;If no &lt;i&gt;pidFile&lt;/i&gt; is provided, then the deployment system assumes that the daemon generates a PID file with the same name as the executable and resides in the directory that is commonly used for storing PID files: &lt;i&gt;/var/run&lt;/i&gt;.&lt;/li&gt;&lt;li&gt;If a package provides only a single executable in the &lt;i&gt;bin/&lt;/i&gt; sub folder, then it is also not required to specify a process.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The fact that the configuration system provides reasonable defaults, means that for trivial services we do not have to specify any configuration properties at all -- simply providing a single executable in the package's &lt;i&gt;bin/&lt;/i&gt; sub folder suffices.&lt;br /&gt;&lt;br /&gt;Do these simple configuration facilities really suffice to manage all kinds of system services? The answer is most likely no, because we may also want to manage processes that cannot daemonize on their own, or we may need to initialize some state first before the service can be used.&lt;br /&gt;&lt;br /&gt;To provide these additional facilities, we can create a &lt;b&gt;wrapper&lt;/b&gt; script around the executable and refer to it in the &lt;i&gt;process&lt;/i&gt; field of the deployment specification.&lt;br /&gt;&lt;br /&gt;The following Nix expression generates a deployment configuration for a service that requires state and only runs as a foreground process:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;{stdenv, writeTextFile, writeScript, daemon, myForegroundService}:&lt;br /&gt;&lt;br /&gt;let&lt;br /&gt;  myForegroundServiceWrapper = writeScript {&lt;br /&gt;    name = &quot;myforegroundservice-wrapper&quot;;&lt;br /&gt;    text = ''&lt;br /&gt;      #! ${stdenv.shell} -e&lt;br /&gt;&lt;br /&gt;      mkdir -p /var/lib/myservice&lt;br /&gt;      exec ${daemon}/bin/daemon -U -F /var/run/mydaemon.pid -- \&lt;br /&gt;        ${myForegroundService}/bin/myservice&lt;br /&gt;    '';&lt;br /&gt;  };&lt;br /&gt;in&lt;br /&gt;writeTextFile {&lt;br /&gt;  name = &quot;mydaemon&quot;;&lt;br /&gt;  text = ''&lt;br /&gt;    process=${myForegroundServiceWrapper}&lt;br /&gt;    pidFile=/var/run/mydaemon.pid&lt;br /&gt;  '';&lt;br /&gt;  destination = &quot;/etc/dysnomia/process&quot;;&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;As you may observe by looking at the Nix expression shown above, the Nix expression generates a wrapper script that does the following:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;First, it creates the required state directory: &lt;i&gt;/var/lib/myservice&lt;/i&gt; so that the service can work properly.&lt;/li&gt;&lt;li&gt;Then it invokes libslack's &lt;a href=&quot;http://www.libslack.org/daemon&quot;&gt;&lt;i&gt;daemon&lt;/i&gt;&lt;/a&gt; command to automatically daemonize the service. The &lt;i&gt;daemon&lt;/i&gt; command will automatically store a PID file containing the daemon's process ID, so that the configuration system knows how to terminate it. The value of the &lt;i&gt;-F&lt;/i&gt; parameter passed to the &lt;i&gt;daemon&lt;/i&gt; executable and the &lt;i&gt;pidFile&lt;/i&gt; configuration property are the same.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Typically, in deployment systems that use a data-driven configuration language (such as YAML or JSON) obtaining a wrapped executable is a burden, but in the Nix expression language this is quite convenient -- the language allows you to automatically build packages and other static artifacts such as configuration files and scripts, and pass their corresponding Nix store paths as parameters to configuration files.&lt;br /&gt;&lt;br /&gt;The combination of wrapper scripts and a simple configuration file suffices to manage all kinds of services, but it is fairly low-level -- to automate the deployment process of a system service, you basically need to re-implement the same kinds of configuration properties all over again.&lt;br /&gt;&lt;br /&gt;In the Nix process mangement-framework, I have developed a &lt;strong&gt;high-level&lt;/strong&gt; abstraction function for creating managed processes that can be used to target all kinds of process managers:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;{createManagedProcess, runtimeDir}:&lt;br /&gt;{port}:&lt;br /&gt;&lt;br /&gt;let&lt;br /&gt;  webapp = import ../../webapp;&lt;br /&gt;in&lt;br /&gt;createManagedProcess rec {&lt;br /&gt;  name = &quot;webapp&quot;;&lt;br /&gt;  description = &quot;Simple web application&quot;;&lt;br /&gt;&lt;br /&gt;  # This expression can both run in foreground or daemon mode.&lt;br /&gt;  # The process manager can pick which mode it prefers.&lt;br /&gt;  process = &quot;${webapp}/bin/webapp&quot;;&lt;br /&gt;  daemonArgs = [ &quot;-D&quot; ];&lt;br /&gt;&lt;br /&gt;  environment = {&lt;br /&gt;    PORT = port;&lt;br /&gt;    PID_FILE = &quot;${runtimeDir}/${name}.pid&quot;;&lt;br /&gt;  };&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above Nix expression is a constructor function that generates a configuration for a web application process (with an embedded HTTP server) that returns a static HTML page.&lt;br /&gt;&lt;br /&gt;The &lt;i&gt;createManagedProcess&lt;/i&gt; function abstraction function can be used to generate configuration artifacts for systemd, supervisord, and launchd and various kinds of scripts, such as sysvinit scripts and BSD rc scripts.&lt;br /&gt;&lt;br /&gt;I can also easily adjust the generator infrastructure to generate the configuration files shown earlier (capturing the path of an executable and a PID file) with a wrapper script.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Managing daemons with Disnix&lt;/h2&gt;&lt;br /&gt;As explained in earlier blog posts about Disnix, services in a Disnix deployment model are abstract representations of basically any kind of deployment unit.&lt;br /&gt;&lt;br /&gt;Every service is annotated with a &lt;i&gt;type&lt;/i&gt; field. Disnix consults &lt;a href=&quot;https://sandervanderburg.blogspot.com/2012/03/deployment-of-mutable-components.html&quot;&gt;a plugin system named Dysnomia&lt;/a&gt; to invoke the corresponding plugin that can manage the lifecycle of that service, e.g. by activating or deactivating it.&lt;br /&gt;&lt;br /&gt;Implementing a Dysnomia module for directly managing daemons is quite straight forward -- as an activation step I just have to start the process defined in the configuration file (or the single executable that resides in the &lt;i&gt;bin/&lt;/i&gt; sub folder of the package).&lt;br /&gt;&lt;br /&gt;As a deactivation step (which purpose is to stop a process) I simply need to send a &lt;i&gt;TERM&lt;/i&gt; signal to the PID in the PID file, by running:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ kill $(cat $pidFile)&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h2&gt;Translation to a Disnix deployment specification&lt;/h2&gt;&lt;br /&gt;The last remaining bits in the puzzle is process dependency management and the translation to a Disnix services model so that Disnix can carry out the deployment.&lt;br /&gt;&lt;br /&gt;Deployments managed by the Nix process management framework are driven by so-called &lt;b&gt;processes models&lt;/b&gt; that capture the properties of running process instances, such as:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;{ pkgs ? import  { inherit system; }&lt;br /&gt;, system ? builtins.currentSystem&lt;br /&gt;, stateDir ? &quot;/var&quot;&lt;br /&gt;, runtimeDir ? &quot;${stateDir}/run&quot;&lt;br /&gt;, logDir ? &quot;${stateDir}/log&quot;&lt;br /&gt;, cacheDir ? &quot;${stateDir}/cache&quot;&lt;br /&gt;, tmpDir ? (if stateDir == &quot;/var&quot; then &quot;/tmp&quot; else &quot;${stateDir}/tmp&quot;)&lt;br /&gt;, forceDisableUserChange ? false&lt;br /&gt;, processManager ? &quot;disnix&quot;&lt;br /&gt;}:&lt;br /&gt;&lt;br /&gt;let&lt;br /&gt;  constructors = import ./constructors.nix {&lt;br /&gt;    inherit pkgs stateDir runtimeDir logDir tmpDir forceDisableUserChange processManager;&lt;br /&gt;  };&lt;br /&gt;in&lt;br /&gt;rec {&lt;br /&gt;  webapp = rec {&lt;br /&gt;    port = 5000;&lt;br /&gt;    dnsName = &quot;webapp.local&quot;;&lt;br /&gt;&lt;br /&gt;    pkg = constructors.webapp {&lt;br /&gt;      inherit port;&lt;br /&gt;    };&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  nginxReverseProxy = rec {&lt;br /&gt;    port = 8080;&lt;br /&gt;&lt;br /&gt;    pkg = constructors.nginxReverseProxyHostBased {&lt;br /&gt;      webapps = [ webapp ];&lt;br /&gt;      inherit port;&lt;br /&gt;    } {};&lt;br /&gt;  };&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above Nix expression is a simple example of a processes model defining two running processes:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The &lt;i&gt;webapp&lt;/i&gt; process is the web application process described earlier that runs an embedded HTTP server and serves a static HTML page.&lt;/li&gt;&lt;li&gt;The &lt;i&gt;nginxReverseProxy&lt;/i&gt; is an Nginx web server that acts as a reverse proxy server for the &lt;i&gt;webapp&lt;/i&gt; process. To make this service to work properly, it needs to be activated after the &lt;i&gt;webapp&lt;/i&gt; process is activated. To ensure that the activation is done in the right order, &lt;i&gt;webapp&lt;/i&gt; is passed as a process dependency to the &lt;i&gt;nginxReverseProxyHostBased&lt;/i&gt; constructor function.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;As explained in previous blog posts, Disnix deployments are driven by three kinds of deployment specifications: a &lt;b&gt;services&lt;/b&gt; model that captures the service components of which a system consists, an &lt;b&gt;infrastructure&lt;/b&gt; model that captures all available target machines and their configuration properties and a &lt;b&gt;distribution&lt;/b&gt; model that maps services in the services model to machines in the infrastructure model.&lt;br /&gt;&lt;br /&gt;The processes model and Disnix services model are quite similar -- the latter is actually a superset of the processes model.&lt;br /&gt;&lt;br /&gt;We can translate process instances to Disnix services in a straight forward manner. For example, the &lt;i&gt;nginxReverseProxy&lt;/i&gt; process can be translated into the following Disnix service configuration:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;nginxReverseProxy = rec {&lt;br /&gt;  name = &quot;nginxReverseProxy&quot;;&lt;br /&gt;  port = 8080;&lt;br /&gt;&lt;br /&gt;  pkg = constructors.nginxReverseProxyHostBased {&lt;br /&gt;    webapps = [ webapp ];&lt;br /&gt;    inherit port;&lt;br /&gt;  } {};&lt;br /&gt;&lt;br /&gt;  activatesAfter = {&lt;br /&gt;    inherit webapp;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  type = &quot;process&quot;;&lt;br /&gt;};&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In the above specification, the process configuration has been augmented with the following properties:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;A &lt;i&gt;name&lt;/i&gt; property because this is a mandatory field for every service.&lt;/li&gt;&lt;li&gt;In the process management framework all process instances are managed by the same process manager, but in Disnix services can have all kinds of shapes and formes and require a plugin to manage their life-cycles.&lt;br /&gt;&lt;br /&gt;To allow Disnix to manage daemons, we specify the &lt;i&gt;type&lt;/i&gt; property to refer to our &lt;i&gt;process&lt;/i&gt; Dysnomia module that starts and terminates a daemon from a simple textual specification.&lt;/li&gt;&lt;li&gt;The process dependencies are translated to Disnix inter-dependencies by using the &lt;i&gt;activatesAfter&lt;/i&gt; property.&lt;br /&gt;&lt;br /&gt;In Disnix, inter-dependency parameters serve two purposes -- they provide the inter-dependent services with configuration parameters and they ensure the correct activation ordering.&lt;br /&gt;&lt;br /&gt;The &lt;i&gt;activatesAfter&lt;/i&gt; parameter disregards the first inter-dependency property, because we are already using the process management framework's convention for propagating process dependencies.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;To allow Disnix to carry out the deployment of processes only a services model does not suffice. Since we are only interested in local deployment, we can just provide an infrastructure model with only a localhost target and a distribution model that maps all services to localhost.&lt;br /&gt;&lt;br /&gt;To accomplish this, we can use the same principles for local deployments described in the previous blog post.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;An example deployment scenario&lt;/h2&gt;&lt;br /&gt;I have added a new tool called &lt;i&gt;nixproc-disnix-switch&lt;/i&gt; to the Nix process management framework that automatically converts processes models into Disnix deployment models and invokes Disnix to locally deploy a system.&lt;br /&gt;&lt;br /&gt;The following command will carry out the complete deployment of our webapp example system, shown earlier, using Disnix as a simple dependency-based process manager:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ nixproc-disnix-switch --state-dir /home/sander/var \&lt;br /&gt;  --force-disable-user-change processes.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In addition to using Disnix for deploying processes, we can also use its other features. For example, another application of Disnix I typically find useful is the deployment visualization tool.&lt;br /&gt;&lt;br /&gt;We can also use Disnix to generate a &lt;a href=&quot;https://graphviz.org&quot;&gt;DOT graph&lt;/a&gt; from the deployment architecture of the currently deployed system and generate an image from it:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-visualize &amp;gt; out.dot&lt;br /&gt;$ dot -Tpng out.dot &amp;gt; out.png&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Resulting in the following diagram:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-y3CYMKaDby4/XuFHrkwirKI/AAAAAAAAKHM/pJIIhZeJcsgZUX9_12l3ADjsFKWGY2dvACLcBGAsYHQ/s1600/deploymentarch.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://3.bp.blogspot.com/-y3CYMKaDby4/XuFHrkwirKI/AAAAAAAAKHM/pJIIhZeJcsgZUX9_12l3ADjsFKWGY2dvACLcBGAsYHQ/s1600/deploymentarch.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In the &lt;a href=&quot;https://sandervanderburg.blogspot.com/2019/11/a-nix-based-functional-organization-for.html&quot;&gt;first blog post&lt;/a&gt; that I wrote about the Nix process management framework (in which I explored a functional discipline using sysvinit-scripts as a basis), I was using hand-drawn diagrams to illustrate deployments.&lt;br /&gt;&lt;br /&gt;With the Disnix backend, I can use Disnix's visualization tool to automatically generate these diagrams.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Discussion&lt;/h2&gt;&lt;br /&gt;In this blog post, I have shown that by implementing a few very simple concepts, we can use Disnix as a process management backend for the experimental Nix-based process management framework.&lt;br /&gt;&lt;br /&gt;Although it was fun to develop a simple process management solution, my goal is not to compete with existing process management solutions (such as systemd, launchd or supervisord) -- this solution is primarily designed for simple use cases and local experimentation.&lt;br /&gt;&lt;br /&gt;For production deployments, you probably still want to use a more sophisticated solution. For example, in production scenarios you also want to check the status of running processes and send them reload instructions. These are features that the Disnix backend does not support.&lt;br /&gt;&lt;br /&gt;The Nix process management framework supports a variety of process managers, but none of them can be universally used on all platforms that Disnix can run on. For example, the &lt;i&gt;sysvinit-script&lt;/i&gt; module works conveniently for local deployments but is restricted to Linux only. Likewise the &lt;i&gt;bsdrc-script&lt;/i&gt; module only works on FreeBSD (and theoretically on NetBSD and OpenBSD). &lt;i&gt;supervisord&lt;/i&gt; works on most UNIX-like systems, but is not self contained -- processes rely on the availablity of the supervisord service to run.&lt;br /&gt;&lt;br /&gt;This Disnix-based process management solution is simple and portable to all UNIX-like systems that Disnix has been tested on.&lt;br /&gt;&lt;br /&gt;The &lt;i&gt;process&lt;/i&gt; module described in this blog post is a replacement for the &lt;i&gt;process&lt;/i&gt; module that already exists in the current release of Dysnomia. The reason why I want it to be replaced is that Dysnomia now provides better alternatives to the old process module.&lt;br /&gt;&lt;br /&gt;For example, when it is desired to have your process managed by systemd, then the new &lt;i&gt;systemd-unit&lt;/i&gt; module should be used that is more reliable, supports many more features and has a simpler implementation.&lt;br /&gt;&lt;br /&gt;Furthermore, I made a couple of mistakes in the past. The old process module was originally implemented as a simple module that would start a foreground process in the background, by using the &lt;a href=&quot;https://linux.die.net/man/1/nohup&quot;&gt;&lt;i&gt;nohup&lt;/i&gt;&lt;/a&gt; command. At the time I developed that module, I did not know much about developing daemons, nor about the additional steps daemons need to carry out to make themselves well-behaving.&lt;br /&gt;&lt;br /&gt;&lt;i&gt;nohup&lt;/i&gt; is not a proper solution for daemonizing foreground processes, such as critical system services -- a process might inherit privacy-sensitive environment variables, does not change the current working directory to the root folder and keep external drives mounted, and could also behave unpredictably if signal handlers have been changed from the default behaviour.&lt;br /&gt;&lt;br /&gt;At some point I believed that it is more reliable to use a process manager to manage the lifecycle of a process and adjusted the process module to do that. Originally I used Upstart for this purpose, and later I switched to systemd, with sysvinit-scripts (and the direct appraoch with &lt;i&gt;nohup&lt;/i&gt; as alternative implemenations).&lt;br /&gt;&lt;br /&gt;Basically the &lt;i&gt;process&lt;/i&gt; module provided three kinds of implementations in which none of them provided an optimal deployment experience.&lt;br /&gt;&lt;br /&gt;I made a similar mistake with Dysnomia's &lt;i&gt;wrapper&lt;/i&gt; module. Originally, its only purpose was to delegate the execution of deployment activities to a wrapper script included with the component that needs to be deployed. Because I was using this script mostly to deploy daemons, I have also adjusted the &lt;i&gt;wrapper&lt;/i&gt; module to use an external process manager to manage the lifecycle of the daemon that the &lt;i&gt;wrapper&lt;/i&gt; script might spawn.&lt;br /&gt;&lt;br /&gt;Because of these mistakes and poor separation of functionality, I have decided to deprecate the old &lt;i&gt;process&lt;/i&gt; and &lt;i&gt;wrapper&lt;/i&gt; modules. Since they are frequently used and I do not want to break compatibility with old deployments, they can still be used if Dysnomia is configured in legacy mode, which is the default setting for the time being.&lt;br /&gt;&lt;br /&gt;When using the old modules, Dysnomia will display a warning message explaining you that you should migrate to better alternatives.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Availability&lt;/h2&gt;&lt;br /&gt;The process &lt;a href=&quot;https://github.com/svanderburg/dysnomia&quot;&gt;Dysnomia&lt;/a&gt; module described in this blog post is part of the current development version of Dysnomia and will become available in the next release.&lt;br /&gt;&lt;br /&gt;The &lt;a href=&quot;https://github.com/svanderburg/nix-processmgmt&quot;&gt;Nix process management framework&lt;/a&gt; (which is still a highly-experimental prototype) includes the &lt;i&gt;disnix&lt;/i&gt; backend (described in this blog post), allowing you to automatically translate a processes model to Disnix deployment models and uses Disnix to deploy a system.&lt;br /&gt;&lt;br /&gt;</description>
	<pubDate>Thu, 11 Jun 2020 18:15:00 +0000</pubDate>
	<author>noreply@blogger.com (Sander van der Burg)</author>
</item>
<item>
	<title>Sander van der Burg: Deploying heterogeneous service-oriented systems locally with Disnix</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-1397115249631682228.post-3034296172802127376</guid>
	<link>http://sandervanderburg.blogspot.com/2020/05/deploying-heterogeneous-service.html</link>
	<description>In &lt;a href=&quot;https://sandervanderburg.blogspot.com/2020/04/deploying-container-and-application.html&quot;&gt;the previous blog post&lt;/a&gt;, I have shown a new useful application area that is built on top of the combination of &lt;a href=&quot;https://sandervanderburg.blogspot.com/2020/02/a-declarative-process-manager-agnostic.html&quot;&gt;my experimental Nix-based process management framework&lt;/a&gt; and &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/02/disnix-toolset-for-distributed.html&quot;&gt;Disnix&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Both of these underlying solutions have a number of similarities -- as their names obviously suggest, they both strongly depend on &lt;a href=&quot;https://sandervanderburg.blogspot.com/2012/11/an-alternative-explaination-of-nix.html&quot;&gt;the Nix package manager&lt;/a&gt; to deploy all their package dependencies and static configuration artifacts, such as configuration files.&lt;br /&gt;&lt;br /&gt;Furthermore, they are both driven by models written in the &lt;b&gt;Nix expression language&lt;/b&gt; to automate the deployment processes of entire systems.&lt;br /&gt;&lt;br /&gt;These models are built on a number of simple conventions that are frequently used in the &lt;a href=&quot;https://nixos.org/nixpkgs&quot;&gt;Nix packages repository&lt;/a&gt;:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;All units of which a system consists are defined as Nix expressions declaring a &lt;b&gt;function&lt;/b&gt;. Each function parameter refers to a dependency or configuration property required to construct the unit from its sources.&lt;/li&gt;&lt;li&gt;To compose a particular variant of a unit, we must &lt;b&gt;invoke&lt;/b&gt; the function that builds and configures the unit with parameters providing the dependencies and configuration properties that the unit needs.&lt;/li&gt;&lt;li&gt;To make all units conveniently &lt;b&gt;accessible&lt;/b&gt; from a &lt;b&gt;single location&lt;/b&gt;, the content of the configuration units is typically blended into a symlink tree called &lt;a href=&quot;https://sandervanderburg.blogspot.com/2013/09/managing-user-environments-with-nix.html&quot;&gt;Nix profiles&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Besides these commonalities, their main difference is that the process management framework is specifically designed as a solution for systems that are composed out of &lt;b&gt;running processes&lt;/b&gt; (i.e. &lt;a href=&quot;https://sandervanderburg.blogspot.com/2020/01/writing-well-behaving-daemon-in-c.html&quot;&gt;daemons&lt;/a&gt; in UNIX terminology).&lt;br /&gt;&lt;br /&gt;This framework makes it possible to construct multiple instances of running processes, isolate their resources (by avoiding conflicting resource configuration properties), and manage running process with a variety of process management solutions, such as sysvinit scripts, BSD rc scripts, systemd, launchd and supervisord.&lt;br /&gt;&lt;br /&gt;The process management framework is quite useful for single machine deployments and local experimentation, but it does not do any &lt;b&gt;distributed&lt;/b&gt; deployment and &lt;b&gt;heterogeneous service deployment&lt;/b&gt; -- it cannot (at least not conveniently) deploy units that are not daemons, such as databases, Java web applications deployed to a Servlet container, PHP applications deployed to a PHP-enabled web server etc.&lt;br /&gt;&lt;br /&gt;Disnix is a solution to automate the deployment processes of service-oriented systems -- distributed systems that are composed of components, using a variety of technologies, into a network of machines.&lt;br /&gt;&lt;br /&gt;To accomplish full automation, Disnix integrates and combines a number of activities and tools, such as Nix for package management and &lt;a href=&quot;https://sandervanderburg.blogspot.com/2015/07/deploying-state-with-disnix.html&quot;&gt;Dysnomia for state management&lt;/a&gt; (Dysnomia takes care of the activation, deactivation steps for services, and can optionally manage snapshots and restores of state). Dysnomia provides a plugin system that makes it possible to manage a variety of component types, including processes and databases.&lt;br /&gt;&lt;br /&gt;Disnix and Dysnomia can also include the features of the Nix process management framework for the deployment of services that are running processes, if desired.&lt;br /&gt;&lt;br /&gt;The scope of Disnix is quite broad in comparison to the process management framework, but it can also be used to automate all kinds of &lt;b&gt;sub problems&lt;/b&gt;. For example, it can also be used as &lt;a href=&quot;https://sandervanderburg.blogspot.com/2016/06/using-disnix-as-remote-package-deployer.html&quot;&gt;a remote package deployment solution&lt;/a&gt; to build and deploy packages in a network of heterogeneous machines (e.g. Linux and macOS).&lt;br /&gt;&lt;br /&gt;After comparing the properties of both deployment solutions, I have identified another interesting sub use case for Disnix -- deploying heterogeneous service-oriented systems (that are composed out of components using a variety of technologies) locally for experimentation purposes.&lt;br /&gt;&lt;br /&gt;In this blog post, I will describe how Disnix can be used for local deployments.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Motivating example: deploying a Java-based web application and web service system&lt;/h2&gt;&lt;br /&gt;One of the examples I have shown in the previous blog post, is an over engineered Java-based web application and web service system which only purpose is to display the string: &quot;Hello world!&quot;.&lt;br /&gt;&lt;br /&gt;The &quot;Hello&quot; string is returned by the &lt;i&gt;HelloService&lt;/i&gt; and consumed by another service called &lt;i&gt;HelloWorldService&lt;/i&gt; that composes the sentence &quot;Hello world!&quot; from the first message. The &lt;i&gt;HelloWorld&lt;/i&gt; web application is the front-end responsible for displaying the sentence to the end user.&lt;br /&gt;&lt;br /&gt;When deploying the system to a single target machine, it could have the following deployment architecture:&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-SoTrSg5o6xo/Xs1xv4DbQ_I/AAAAAAAAKFc/7gxV3KcSsaMgS1ss-_gA7UANHXrG6g8XwCLcBGAsYHQ/s1600/deploymentarch-local.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://3.bp.blogspot.com/-SoTrSg5o6xo/Xs1xv4DbQ_I/AAAAAAAAKFc/7gxV3KcSsaMgS1ss-_gA7UANHXrG6g8XwCLcBGAsYHQ/s640/deploymentarch-local.png&quot; width=&quot;500&quot; /&gt;&lt;/a&gt;&lt;br /&gt;In the architecture diagram shown above, ovals denote services, arrows inter-dependency relationships (requiring that a service gets activated before another), the dark grey colored boxes container environments, and the light grey colored box a machine (which is only one machine in the above example).&lt;br /&gt;&lt;br /&gt;As you may notice, only one service in the diagram shown above is a daemon, namely Apache Tomcat (&lt;i&gt;simpleAppservingTomcat&lt;/i&gt;) that can be managed by the experimental Nix process management framework.&lt;br /&gt;&lt;br /&gt;The remainder of the services have a different kind of form -- the web application front-end (&lt;i&gt;HelloWorld&lt;/i&gt;) is a Java web application that is embedded in Catalina, the Servlet container that comes with Apache Tomcat. The web services are Axis2 archives that are deployed to the Axis2 container (that in turn is a web application managed by Apache Tomcat).&lt;br /&gt;&lt;br /&gt;In the previous blog post, I have shown that we can deploy and distribute these services over a small network of machines.&lt;br /&gt;&lt;br /&gt;It is also possible to completely deploy this system locally, without any external physical or virtual machines, and network connectivity.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Configuring the client interface for local deployment&lt;/h2&gt;&lt;br /&gt;To execute deployment tasks remotely, Disnix invokes an external process that is called a &lt;b&gt;client interface&lt;/b&gt;. By default, Disnix uses the &lt;i&gt;disnix-ssh-client&lt;/i&gt; that remotely executes commands via SSH and transfers data via SCP.&lt;br /&gt;&lt;br /&gt;It is also possible to use alternative client interfaces so that different communication protocols and methods can be used. For example, there is also an external package that provides a SOAP client &lt;i&gt;disnix-soap-client&lt;/i&gt; and a NixOps client (&lt;i&gt;disnix-nixops-client&lt;/i&gt;).&lt;br /&gt;&lt;br /&gt;Communication with a local Disnix service instance can also be done with a client interface. For example, configuring the following environment variable:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ export DISNIX_CLIENT_INTERFACE=disnix-client&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;instructs the Disnix tools to use the &lt;a href=&quot;https://www.freedesktop.org/wiki/Software/dbus/&quot;&gt;D-Bus&lt;/a&gt; client to communicate with a local Disnix service instance.&lt;br /&gt;&lt;br /&gt;It is also possible to bypass the local Disnix service and directly execute all deployment activities with the following interface:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ export DISNIX_CLIENT_INTERFACE=disnix-runactivity&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The &lt;i&gt;disnix-runactivity&lt;/i&gt; client interface is particularly useful for single-user/unprivileged user deployments. In the former case, you need a Disnix D-Bus daemon running in the background that authorizes the user to execute deployments. For the latter, nothing is required beyond a single user Nix installation.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Deploying the example system locally&lt;/h2&gt;&lt;br /&gt;As explained in earlier blog posts about Disnix, deployments are driven by three kinds of deployment specifications: a &lt;b&gt;services&lt;/b&gt; model capturing all the services of which a system consists and how they depend on each other, an &lt;b&gt;infrastructure&lt;/b&gt; model captures all available target machines and their relevant configuration properties (including so-called container services that can host application services) and the &lt;b&gt;distribution&lt;/b&gt; model maps services in the services model to target machines in the infrastructure model (and container services that a machine may provide).&lt;br /&gt;&lt;br /&gt;Normally, Disnix deploys services to remote machines defined in the infrastructure model. For local deployments, we simply need to provide an infrastructure model with only one entry:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;{&lt;br /&gt;  localhost.properties.hostname = &quot;localhost&quot;;&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In the distribution model, we must map all services to the &lt;i&gt;localhost&lt;/i&gt; target:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;{infrastructure}:&lt;br /&gt;&lt;br /&gt;{&lt;br /&gt;  simpleAppservingTomcat = [ infrastructure.localhost ];&lt;br /&gt;  axis2 = [ infrastructure.localhost ];&lt;br /&gt;&lt;br /&gt;  HelloService = [ infrastructure.localhost ];&lt;br /&gt;  HelloWorldService = [ infrastructure.localhost ];&lt;br /&gt;  HelloWorld = [ infrastructure.localhost ];&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;With the above infrastructure and distribution model that facilitates local deployment, and the services model of the example system shown above, we can deploy the entire system on our local machine:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-size: 90%; overflow: auto;&quot;&gt;$ disnix-env -s services.nix -i infrastructure-local.nix -d distribution-local.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h2&gt;Deploying the example system locally as an unprivileged user&lt;/h2&gt;&lt;br /&gt;The deployment scenario shown earlier supports local deployment, but still requires super-user privileges. For example, to deploy Apache Tomcat, we must have write access to the state directory: &lt;i&gt;/var&lt;/i&gt; to configure Apache Tomcat's state and deploy the Java web application archives. An unprivileged user typically lacks the permissions to perform modifications in the &lt;i&gt;/var&lt;/i&gt; directory.&lt;br /&gt;&lt;br /&gt;One of they key features of the Nix process management framework is that it makes all state directories are configurable. State directories can be changed in such a way that also unprivileged users can deploy services (e.g. by changing the state directory to a sub folder in the user's home directory).&lt;br /&gt;&lt;br /&gt;Disnix service models can also define these process management configuration parameters:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;{ pkgs, system, distribution, invDistribution&lt;br /&gt;, stateDir ? &quot;/var&quot;&lt;br /&gt;, runtimeDir ? &quot;${stateDir}/run&quot;&lt;br /&gt;, logDir ? &quot;${stateDir}/log&quot;&lt;br /&gt;, cacheDir ? &quot;${stateDir}/cache&quot;&lt;br /&gt;, tmpDir ? (if stateDir == &quot;/var&quot; then &quot;/tmp&quot; else &quot;${stateDir}/tmp&quot;)&lt;br /&gt;, forceDisableUserChange ? false&lt;br /&gt;, processManager ? &quot;systemd&quot;&lt;br /&gt;}:&lt;br /&gt;&lt;br /&gt;let&lt;br /&gt;  processType =&lt;br /&gt;    if processManager == null then &quot;managed-process&quot;&lt;br /&gt;    else if processManager == &quot;sysvinit&quot; then &quot;sysvinit-script&quot;&lt;br /&gt;    else if processManager == &quot;systemd&quot; then &quot;systemd-unit&quot;&lt;br /&gt;    else if processManager == &quot;supervisord&quot; then &quot;supervisord-program&quot;&lt;br /&gt;    else if processManager == &quot;bsdrc&quot; then &quot;bsdrc-script&quot;&lt;br /&gt;    else if processManager == &quot;cygrunsrv&quot; then &quot;cygrunsrv-service&quot;&lt;br /&gt;    else if processManager == &quot;launchd&quot; then &quot;launchd-daemon&quot;&lt;br /&gt;    else throw &quot;Unknown process manager: ${processManager}&quot;;&lt;br /&gt;&lt;br /&gt;  constructors = import ../../../nix-processmgmt/examples/service-containers-agnostic/constructors.nix {&lt;br /&gt;    inherit pkgs stateDir runtimeDir logDir cacheDir tmpDir forceDisableUserChange processManager;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  customPkgs = import ../top-level/all-packages.nix {&lt;br /&gt;    inherit system pkgs stateDir;&lt;br /&gt;  };&lt;br /&gt;in&lt;br /&gt;rec {&lt;br /&gt;  simpleAppservingTomcat = constructors.simpleAppservingTomcat {&lt;br /&gt;    httpPort = 8080;&lt;br /&gt;    type = processType;&lt;br /&gt;  };&lt;br /&gt;  ...&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above Nix expression shows a partial Nix services model for the Java example system. The first four function parameters: &lt;i&gt;pkgs&lt;/i&gt;, &lt;i&gt;system&lt;/i&gt;, &lt;i&gt;distribution&lt;/i&gt;, and &lt;i&gt;invDistribution&lt;/i&gt; are standard Disnix service model parameters.&lt;br /&gt;&lt;br /&gt;The remainder of the parameters are specific to the process management framework -- they allow you to change the state directories, force disable user changing (this is useful for unprivileged user deployments) and the process manager it should use for daemons.&lt;br /&gt;&lt;br /&gt;I have added a new command-line parameter (&lt;i&gt;--extra-params&lt;/i&gt;) to the Disnix tools that can be used to propagate values for these additional parameters.&lt;br /&gt;&lt;br /&gt;With the following command-line instruction, we change the base directory of the state directories to the user's home directory, force disable user changing (only a privileged user can do this), and change the process manager to sysvinit scripts:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-size: 90%; overflow: auto;&quot;&gt;$ disnix-env -s services.nix -i infrastructure-local.nix -d distribution-local.nix \&lt;br /&gt;  --extra-params '{&lt;br /&gt;  stateDir = &quot;/home/sander/var&quot;;&lt;br /&gt;  processManager = &quot;sysvinit&quot;;&lt;br /&gt;  forceDisableUserChange = true;&lt;br /&gt;}'&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;With the above command, we can deploy the example system completely as an unprivileged user, without requiring any process/service manager to manage Apache Tomcat.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Working with predeployed container services&lt;/h2&gt;&lt;br /&gt;In our examples so far, we have deployed systems that are entirely self contained. However, it is also possible to deploy services to container services that have already been deployed by other means. For example, it is also possible to install Apache Tomcat with your host system's distribution and use Dysnomia to integrate with that.&lt;br /&gt;&lt;br /&gt;To allow Disnix to deploy services to these containers, we need an infrastructure model that knows its properties. We can automatically generate an infrastructure model from the Dysnomia container configuration files, by running:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-capture-infra infrastructure.nix &amp;gt; \&lt;br /&gt;  infrastructure-captured.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;and using the captured infrastructure model to locally deploy the system:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-size: 90%; overflow: auto;&quot;&gt;$ disnix-env -s services.nix -i infrastructure-captured.nix -d distribution-local.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h2&gt;Undeploying a system&lt;/h2&gt;&lt;br /&gt;For local experimentation, it is probably quite common that you want to completely undeploy the system as soon as you no longer need it. Normally, this should be done by writing an empty distribution model and redeploying the system with that empty distribution model, but that is still a bit of a hassle.&lt;br /&gt;&lt;br /&gt;In the latest development version of Disnix, an undeploy can be done with the following command-line instruction:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-env --undeploy -i infrastructure.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h2&gt;Availability&lt;/h2&gt;&lt;br /&gt;The &lt;i&gt;--extra-params&lt;/i&gt; and &lt;i&gt;--undeploy&lt;/i&gt; Disnix command-line options are part of the current development version of Disnix and will become available in the next release.&lt;br /&gt;&lt;br /&gt;</description>
	<pubDate>Tue, 26 May 2020 21:49:00 +0000</pubDate>
	<author>noreply@blogger.com (Sander van der Burg)</author>
</item>
<item>
	<title>Tweag I/O: Nix Flakes, Part 1: An introduction and tutorial</title>
	<guid isPermaLink="true">https://tweag.io/blog/2020-05-25-flakes/</guid>
	<link>https://tweag.io/blog/2020-05-25-flakes/</link>
	<description>&lt;p&gt;This is the first in a series of blog posts intended to provide a
gentle introduction to
&lt;a href=&quot;https://github.com/NixOS/rfcs/pull/49&quot;&gt;&lt;em&gt;flakes&lt;/em&gt;&lt;/a&gt;, a new Nix feature
that improves reproducibility, composability and usability in the Nix
ecosystem. This blog post describes why flakes were introduced, and
give a short tutorial on how to use them.&lt;/p&gt;
&lt;p&gt;Flakes were developed at Tweag and funded by Target Corporation and
Tweag.&lt;/p&gt;
&lt;h2&gt;What problems do flakes solve?&lt;/h2&gt;
&lt;p&gt;Once upon a time, Nix pioneered reproducible builds: it tries hard to
ensure that two builds of the same derivation graph produce an
identical result. Unfortunately, the evaluation of Nix files into such
a derivation graph isn’t nearly as reproducible, despite the language
being nominally purely functional.&lt;/p&gt;
&lt;p&gt;For example, Nix files can access arbitrary files (such as
&lt;code class=&quot;language-text&quot;&gt;~/.config/nixpkgs/config.nix&lt;/code&gt;), environment variables, Git
repositories, files in the Nix search path (&lt;code class=&quot;language-text&quot;&gt;$NIX_PATH&lt;/code&gt;), command-line
arguments (&lt;code class=&quot;language-text&quot;&gt;--arg&lt;/code&gt;) and the system type (&lt;code class=&quot;language-text&quot;&gt;builtins.currentSystem&lt;/code&gt;). In
other words, &lt;em&gt;evaluation isn’t as hermetic as it could be&lt;/em&gt;. In practice, ensuring reproducible evaluation of things like NixOS system configurations requires special care.&lt;/p&gt;
&lt;p&gt;Furthermore, there is no &lt;em&gt;standard way to compose Nix-based
projects&lt;/em&gt;. It’s rare that everything you need is in Nixpkgs; consider
for instance projects that use Nix as a build tool, or NixOS system
configurations. Typical ways to compose Nix files are to rely on the
Nix search path (e.g. &lt;code class=&quot;language-text&quot;&gt;import &amp;lt;nixpkgs&amp;gt;&lt;/code&gt;) or to use &lt;code class=&quot;language-text&quot;&gt;fetchGit&lt;/code&gt; or
&lt;code class=&quot;language-text&quot;&gt;fetchTarball&lt;/code&gt;. The former has poor reproducibility, while the latter
provides a bad user experience because of the need to manually update
Git hashes to update dependencies.&lt;/p&gt;
&lt;p&gt;There is also no easy way to &lt;em&gt;deliver&lt;/em&gt; Nix-based projects to
users. Nix has a “channel” mechanism (essentially a tarball containing
Nix files), but it’s not easy to create channels and they are not
composable. Finally, Nix-based projects lack a standardized structure.
There are some conventions (e.g. &lt;code class=&quot;language-text&quot;&gt;shell.nix&lt;/code&gt; or &lt;code class=&quot;language-text&quot;&gt;release.nix&lt;/code&gt;) but
they don’t cover many common use cases; for instance, there is no
way to discover the NixOS modules provided by a repository.&lt;/p&gt;
&lt;p&gt;Flakes are a solution to these problems. A flake is simply a source
tree (such as a Git repository) containing a file named &lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt;
that provides a standardized interface to Nix artifacts such as
packages or NixOS modules. Flakes can have dependencies on other
flakes, with a “lock file” pinning those dependencies to exact
revisions to ensure reproducible evaluation.&lt;/p&gt;
&lt;p&gt;The flake file format and semantics are described in a &lt;a href=&quot;https://github.com/NixOS/rfcs/pull/49&quot;&gt;NixOS
RFC&lt;/a&gt;, which is currently the
best reference on flakes.&lt;/p&gt;
&lt;h2&gt;Trying out flakes&lt;/h2&gt;
&lt;p&gt;Flakes are currently implemented in &lt;a href=&quot;https://github.com/NixOS/nix/tree/flakes&quot;&gt;an experimental branch of
Nix&lt;/a&gt;. If you want to play
with flakes, you can get this version of Nix from Nixpkgs:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix-shell -I nixpkgs=channel:nixos-20.03 -p nixFlakes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since flakes are an experimental feature, you also need to add the
following line to &lt;code class=&quot;language-text&quot;&gt;~/.config/nix/nix.conf&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;experimental-features = nix-command flakes&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or pass the flag &lt;code class=&quot;language-text&quot;&gt;--experimental-features 'nix-command flakes'&lt;/code&gt;
whenever you call the &lt;code class=&quot;language-text&quot;&gt;nix&lt;/code&gt; command.&lt;/p&gt;
&lt;h2&gt;Using flakes&lt;/h2&gt;
&lt;p&gt;To see flakes in action, let’s start
with a simple Unix package named &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt; (a FUSE filesystem that
automatically fetches debug symbols from the Internet). It lives in a
GitHub repository at &lt;code class=&quot;language-text&quot;&gt;https://github.com/edolstra/dwarffs&lt;/code&gt;; it is a
flake because it contains a file named
&lt;a href=&quot;https://github.com/edolstra/dwarffs/blob/master/flake.nix&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt;&lt;/a&gt;. We
will look at the contents of this file later, but in short, it tells
Nix what the flake provides (such as Nix packages, NixOS modules or CI
tests).&lt;/p&gt;
&lt;p&gt;The following command fetches the &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt; Git repository, builds its
&lt;em&gt;default package&lt;/em&gt; and runs it.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix shell github:edolstra/dwarffs -c dwarffs --version
dwarffs 0.1.20200406.cd7955a&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The command above isn’t very &lt;em&gt;reproducible&lt;/em&gt;: it fetches the most
recent version of &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt;, which could change over time. But it’s
easy to ask Nix to build a specific revision:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix shell github:edolstra/dwarffs/cd7955af31698c571c30b7a0f78e59fd624d0229 ...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Nix tries very hard to ensure that the result of building a flake from
such a URL is always the same. This requires it to restrict a number
of things that Nix projects could previously do. For example, the
&lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt; project requires a number of dependencies (such as a C++
compiler) that it gets from the Nix Packages collection (Nixpkgs). In
the past, you might use the &lt;code class=&quot;language-text&quot;&gt;NIX_PATH&lt;/code&gt; environment variable to allow
your project to find Nixpkgs. In the world of flakes, this is no
longer allowed: flakes have to declare their dependencies explicitly,
and these dependencies have to be &lt;em&gt;locked&lt;/em&gt; to specific revisions.&lt;/p&gt;
&lt;p&gt;In order to do so, &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt;’s &lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt; file declares an explicit
dependency on Nixpkgs, which is also a
&lt;a href=&quot;https://github.com/NixOS/nixpkgs/blob/master/flake.nix&quot;&gt;flake&lt;/a&gt;.
We can see the dependencies of a flake as follows:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix flake list-inputs github:edolstra/dwarffs
github:edolstra/dwarffs/d11b181af08bfda367ea5cf7fad103652dc0409f
├───nix: github:NixOS/nix/3aaceeb7e2d3fb8a07a1aa5a21df1dca6bbaa0ef
│   └───nixpkgs: github:NixOS/nixpkgs/b88ff468e9850410070d4e0ccd68c7011f15b2be
└───nixpkgs: github:NixOS/nixpkgs/b88ff468e9850410070d4e0ccd68c7011f15b2be&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So the &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt; flake depends on a &lt;em&gt;specific&lt;/em&gt; version of the
&lt;code class=&quot;language-text&quot;&gt;nixpkgs&lt;/code&gt; flake (as well as the &lt;code class=&quot;language-text&quot;&gt;nix&lt;/code&gt; flake). As a result, building
&lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt; will always produce the same result. We didn’t specify this
version in &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt;’s &lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt;. Instead, it’s recorded in a &lt;em&gt;lock
file&lt;/em&gt; named
&lt;a href=&quot;https://github.com/edolstra/dwarffs/blob/master/flake.lock&quot;&gt;flake.lock&lt;/a&gt;
that is generated automatically by Nix and committed to the &lt;code class=&quot;language-text&quot;&gt;dwarffs&lt;/code&gt;
repository.&lt;/p&gt;
&lt;h2&gt;Flake outputs&lt;/h2&gt;
&lt;p&gt;Another goal of flakes is to provide a standard structure for
discoverability within Nix-based projects. Flakes can provide
arbitrary Nix values, such as packages, NixOS modules or library
functions. These are called its &lt;em&gt;outputs&lt;/em&gt;. We can see the outputs of a
flake as follows:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix flake show github:edolstra/dwarffs
github:edolstra/dwarffs/d11b181af08bfda367ea5cf7fad103652dc0409f
├───checks
│   ├───aarch64-linux
│   │   └───build: derivation 'dwarffs-0.1.20200409'
│   ├───i686-linux
│   │   └───build: derivation 'dwarffs-0.1.20200409'
│   └───x86_64-linux
│       └───build: derivation 'dwarffs-0.1.20200409'
├───defaultPackage
│   ├───aarch64-linux: package 'dwarffs-0.1.20200409'
│   ├───i686-linux: package 'dwarffs-0.1.20200409'
│   └───x86_64-linux: package 'dwarffs-0.1.20200409'
├───nixosModules
│   └───dwarffs: NixOS module
└───overlay: Nixpkgs overlay&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While a flake can have arbitrary outputs, some of them, if they exist,
have a special meaning to certain Nix commands and therefore must have
a specific type. For example, the output &lt;code class=&quot;language-text&quot;&gt;defaultPackage.&amp;lt;system&amp;gt;&lt;/code&gt;
must be a derivation; it’s what &lt;code class=&quot;language-text&quot;&gt;nix build&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;nix shell&lt;/code&gt; will build
by default unless you specify another output. The &lt;code class=&quot;language-text&quot;&gt;nix&lt;/code&gt; CLI allows you to
specify another output through a syntax reminiscent of URL fragments:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix build github:edolstra/dwarffs#checks.aarch64-linux.build&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By the way, the standard &lt;code class=&quot;language-text&quot;&gt;checks&lt;/code&gt; output specifies a set of
derivations to be built by a continuous integration system such as
Hydra. Because flake evaluation is hermetic and the lock file locks
all dependencies, it’s guaranteed that the &lt;code class=&quot;language-text&quot;&gt;nix build&lt;/code&gt; command above
will evaluate to the same result as the one in the CI system.&lt;/p&gt;
&lt;h2&gt;The flake registry&lt;/h2&gt;
&lt;p&gt;Flake locations are specified using a URL-like syntax such as
&lt;code class=&quot;language-text&quot;&gt;github:edolstra/dwarffs&lt;/code&gt; or
&lt;code class=&quot;language-text&quot;&gt;git+https://github.com/NixOS/patchelf&lt;/code&gt;. But because such URLs would
be rather verbose if you had to type them all the time on the command
line, there also is a &lt;a href=&quot;https://raw.githubusercontent.com/NixOS/flake-registry/master/flake-registry.json&quot;&gt;flake
registry&lt;/a&gt;
that maps symbolic identifiers such as &lt;code class=&quot;language-text&quot;&gt;nixpkgs&lt;/code&gt; to actual locations
like &lt;code class=&quot;language-text&quot;&gt;https://github.com/NixOS/nixpkgs&lt;/code&gt;. So the following are (by
default) equivalent:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix shell nixpkgs#cowsay -c cowsay Hi!
$ nix shell github:NixOS/nixpkgs#cowsay -c cowsay Hi!&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It’s possible to override the registry locally. For example, you can
override the &lt;code class=&quot;language-text&quot;&gt;nixpkgs&lt;/code&gt; flake to your own Nixpkgs tree:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix registry add nixpkgs ~/my-nixpkgs&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or pin it to a specific revision:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix registry add nixpkgs github:NixOS/nixpkgs/5272327b81ed355bbed5659b8d303cf2979b6953&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Writing your first flake&lt;/h2&gt;
&lt;p&gt;Unlike Nix channels, creating a flake is pretty simple: you just add a
&lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt; and possibly a &lt;code class=&quot;language-text&quot;&gt;flake.lock&lt;/code&gt; to your project’s repository. As
an example, suppose we want to create our very own Hello World and
distribute it as a flake. Let’s create this project first:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ git init hello
$ cd hello
$ echo 'int main() { printf(&quot;Hello World&quot;); }' &amp;gt; hello.c
$ git add hello.c&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To turn this Git repository into a flake, we add a file named
&lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt; at the root of the repository with the following contents:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-nix&quot;&gt;&lt;code class=&quot;language-nix&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  description &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;A flake for building Hello World&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  inputs&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;nixpkgs&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;url &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token url&quot;&gt;github:NixOS/nixpkgs/nixos-20.03&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  outputs &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; nixpkgs &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;

    defaultPackage&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;x86_64&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;linux &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;
      &lt;span class=&quot;token comment&quot;&gt;# Notice the reference to nixpkgs here.&lt;/span&gt;
      &lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;import&lt;/span&gt; nixpkgs &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; system &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;x86_64-linux&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
      stdenv&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;mkDerivation &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
        name &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;hello&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
        src &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
        buildPhase &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;gcc -o hello ./hello.c&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
        installPhase &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;mkdir -p $out/bin; install -t $out/bin hello&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The command &lt;code class=&quot;language-text&quot;&gt;nix flake init&lt;/code&gt; creates a basic &lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt; for you.&lt;/p&gt;
&lt;p&gt;Note that any file that is not tracked by Git is invisible during Nix
evaluation, in order to ensure hermetic evaluation. Thus, you need to
make &lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt; visible to Git:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ git add flake.nix&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let’s see if it builds!&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix build
warning: creating lock file '/home/eelco/Dev/hello/flake.lock'
warning: Git tree '/home/eelco/Dev/hello' is dirty

$ ./result/bin/hello
Hello World&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or equivalently:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix shell -c hello
Hello World&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It’s also possible to get an interactive development environment in
which all the dependencies (like GCC) and shell variables and
functions from the derivation are in scope:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix dev-shell
$ eval &quot;$buildPhase&quot;
$ ./hello
Hello World&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So what does all that stuff in &lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt; mean?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code class=&quot;language-text&quot;&gt;description&lt;/code&gt; attribute is a one-line description shown by &lt;code class=&quot;language-text&quot;&gt;nix flake info&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code class=&quot;language-text&quot;&gt;inputs&lt;/code&gt; attribute specifies other flakes that this flake
depends on. These are fetched by Nix and passed as arguments to the
&lt;code class=&quot;language-text&quot;&gt;outputs&lt;/code&gt; function.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code class=&quot;language-text&quot;&gt;outputs&lt;/code&gt; attribute is the heart of the flake: it’s a function
that produces an attribute set. The function arguments are the
flakes specified in &lt;code class=&quot;language-text&quot;&gt;inputs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code class=&quot;language-text&quot;&gt;self&lt;/code&gt; argument denotes &lt;em&gt;this&lt;/em&gt; flake. Its primarily useful for
referring to the source of the flake (as in &lt;code class=&quot;language-text&quot;&gt;src = self;&lt;/code&gt;) or to
other outputs (e.g. &lt;code class=&quot;language-text&quot;&gt;self.defaultPackage.x86_64-linux&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;The attributes produced by &lt;code class=&quot;language-text&quot;&gt;outputs&lt;/code&gt; are arbitrary values, except
that (as we saw above) there are some standard outputs such as
&lt;code class=&quot;language-text&quot;&gt;defaultPackage.${system}&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Every flake has some metadata, such as &lt;code class=&quot;language-text&quot;&gt;self.lastModifiedDate&lt;/code&gt;,
which is used to generate a version string like &lt;code class=&quot;language-text&quot;&gt;hello-20191015&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You may have noticed that the dependency specification
&lt;code class=&quot;language-text&quot;&gt;github:NixOS/nixpkgs/nixos-20.03&lt;/code&gt; is imprecise: it says that we want
to use the &lt;code class=&quot;language-text&quot;&gt;nixos-20.03&lt;/code&gt; branch of Nixpkgs, but doesn’t say which Git
revision. This seems bad for reproducibility. However, when we ran
&lt;code class=&quot;language-text&quot;&gt;nix build&lt;/code&gt;, Nix automatically generated a lock file that precisely
states which revision of &lt;code class=&quot;language-text&quot;&gt;nixpkgs&lt;/code&gt; to use:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ cat flake.lock
{
  &quot;nodes&quot;: {
    &quot;nixpkgs&quot;: {
      &quot;info&quot;: {
        &quot;lastModified&quot;: 1587398327,
        &quot;narHash&quot;: &quot;sha256-mEKkeLgUrzAsdEaJ/1wdvYn0YZBAKEG3AN21koD2AgU=&quot;
      },
      &quot;locked&quot;: {
        &quot;owner&quot;: &quot;NixOS&quot;,
        &quot;repo&quot;: &quot;nixpkgs&quot;,
        &quot;rev&quot;: &quot;5272327b81ed355bbed5659b8d303cf2979b6953&quot;,
        &quot;type&quot;: &quot;github&quot;
      },
      &quot;original&quot;: {
        &quot;owner&quot;: &quot;NixOS&quot;,
        &quot;ref&quot;: &quot;nixos-20.03&quot;,
        &quot;repo&quot;: &quot;nixpkgs&quot;,
        &quot;type&quot;: &quot;github&quot;
      }
    },
    &quot;root&quot;: {
      &quot;inputs&quot;: {
        &quot;nixpkgs&quot;: &quot;nixpkgs&quot;
      }
    }
  },
  &quot;root&quot;: &quot;root&quot;,
  &quot;version&quot;: 5
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Any subsequent build of this flake will use the version of &lt;code class=&quot;language-text&quot;&gt;nixpkgs&lt;/code&gt;
recorded in the lock file. If you add new inputs to &lt;code class=&quot;language-text&quot;&gt;flake.nix&lt;/code&gt;, when
you run any command such as &lt;code class=&quot;language-text&quot;&gt;nix build&lt;/code&gt;, Nix will automatically add
corresponding locks to &lt;code class=&quot;language-text&quot;&gt;flake.lock&lt;/code&gt;. However, it won’t replace
existing locks. If you want to update a locked input to the latest
version, you need to ask for it:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix flake update --update-input nixpkgs
$ nix build&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To wrap things up, we can now commit our project and push it to
GitHub, after making sure that everything is in order:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix flake check
$ git commit -a -m 'Initial version'
$ git remote add origin git@github.com:edolstra/hello.git
$ git push -u origin master&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Other users can then use this flake:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;&lt;pre class=&quot;language-console&quot;&gt;&lt;code class=&quot;language-console&quot;&gt;$ nix shell github:edolstra/hello -c hello&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;In the next blog post, we’ll talk about typical uses of flakes, such
as managing NixOS system configurations, distributing Nixpkgs overlays
and NixOS modules, and CI integration.&lt;/p&gt;</description>
	<pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
</item>
<item>
	<title>Sander van der Burg: Deploying container and application services with Disnix</title>
	<guid isPermaLink="false">tag:blogger.com,1999:blog-1397115249631682228.post-7320527361984147016</guid>
	<link>http://sandervanderburg.blogspot.com/2020/04/deploying-container-and-application.html</link>
	<description>As described in many previous blog posts, &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/02/disnix-toolset-for-distributed.html&quot;&gt;Disnix&lt;/a&gt;'s purpose is to deploy &lt;b&gt;service-oriented systems&lt;/b&gt; -- systems that can be decomposed into inter-connected service components, such as databases, web services, web applications and processes -- to networks of machines.&lt;br /&gt;&lt;br /&gt;To use Disnix effectively, two requirements must be met:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;A system must be &lt;b&gt;decomposed&lt;/b&gt; into independently deployable services, and these services must be packaged with &lt;a href=&quot;https://sandervanderburg.blogspot.com/2012/11/an-alternative-explaination-of-nix.html&quot;&gt;Nix&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Services may require other services that provide environments with essential facilities to run them. In Disnix terminology, these environments are called &lt;b&gt;containers&lt;/b&gt;. For example, to host a MySQL database, Disnix requires a MySQL DBMS as a container, to run a Java web application archive you need a Java Servlet container, such as Apache Tomcat, and to run a &lt;a href=&quot;https://en.wikipedia.org/wiki/Daemon_(computing)&quot;&gt;daemon&lt;/a&gt; it needs a process manager, such as &lt;a href=&quot;https://www.freedesktop.org/wiki/Software/systemd/&quot;&gt;systemd&lt;/a&gt;, &lt;a href=&quot;https://www.launchd.info/&quot;&gt;launchd&lt;/a&gt; or &lt;a href=&quot;http://supervisord.org/&quot;&gt;supervisord&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;Disnix was originally designed to only deploy the (functional) application components (called &lt;strong&gt;services&lt;/strong&gt; in Disnix terminology) of which a service-oriented systems consists, but it was not designed to handle the deployment of any underlying container services.&lt;br /&gt;&lt;br /&gt;In &lt;a href=&quot;https://sandervanderburg.blogspot.com/2013/05/a-reference-architecture-for.html&quot;&gt;my PhD thesis&lt;/a&gt;, I called Disnix's problem domain &lt;b&gt;service deployment&lt;/b&gt;. Another problem domain that I identified was &lt;b&gt;infrastructure deployment&lt;/b&gt; that concerns the deployment of machine configurations, including container services.&lt;br /&gt;&lt;br /&gt;The fact that these problem domains are separated means that, if we want to fully deploy a service-oriented system from scratch, we basically need to do infrastructure deployment first, e.g. install a collection of machines with system software and these container services, such as MySQL and Apache Tomcat, and once that is done, we can use these machines as deployment targets for Disnix.&lt;br /&gt;&lt;br /&gt;There are a variety of solutions available to automate infrastructure deployment. Most notably, &lt;a href=&quot;http://github.com/nixos/nixops&quot;&gt;NixOps&lt;/a&gt; can be used to automatically deploy networks of &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/01/nixos-purely-functional-linux.html&quot;&gt;NixOS&lt;/a&gt; configurations, and (if desired) automatically instantiate virtual machines in a cloud/&lt;a href=&quot;https://en.wikipedia.org/wiki/Infrastructure_as_a_service&quot;&gt;IaaS&lt;/a&gt; environment, such as &lt;a href=&quot;https://aws.amazon.com/ec2/&quot;&gt;Amazon EC2&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Although &lt;a href=&quot;https://sandervanderburg.blogspot.com/2015/03/on-nixops-disnix-service-deployment-and.html&quot;&gt;combining NixOps for infrastructure deployment with Disnix for service deployment&lt;/a&gt; works great in many scenarios, there are still a number of concerns that are not adequately addressed:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Infrastructure and service deployment are still two (somewhat) &lt;b&gt;separated processes&lt;/b&gt;. Although I have developed an extension toolset (called DisnixOS) to combine Disnix with the deployment concepts of NixOS and NixOps, we still need to run two kinds of deployment procedures. Ideally, it would be nice to fully automate the entire deployment process with only one command.&lt;/li&gt;&lt;li&gt;Although NixOS (and NixOps that extends NixOS' concepts to networks of machines and the cloud) do a great job in fully automating the deployments of machines, we can only reap their benefits if we can permit ourselves use to NixOS, which is a particular &lt;b&gt;Linux distribution&lt;/b&gt; flavour -- sometimes you may need to deploy services to conventional Linux distributions, or different kinds of operating systems (after all, one of the reasons to use service-oriented systems is to be able to use a diverse set of technologies).&lt;br /&gt;&lt;br /&gt;The Nix package manager also works on other operating systems than Linux, such macOS, but there is no Nix-based deployment automation solution that can universally deploy infrastructure components to other operating systems (the only other infrastructure deployment solution that provides similar functionality to NixOS is the the &lt;a href=&quot;https://github.com/LnL7/nix-darwin&quot;&gt;nix-darwin&lt;/a&gt; repository, that can only be used on macOS).&lt;/li&gt;&lt;li&gt;The NixOS module system does &lt;b&gt;not&lt;/b&gt; facilitate the deployment of &lt;b&gt;multiple instances&lt;/b&gt; of infrastructure components. Although this is probably a very uncommon use case, it is also possible to run two MySQL DBMS services on one machine and use both of them as Disnix deployment targets for databases.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;In a Disnix-context, services have no specific meaning or shape and can basically represent anything -- a satellite tool providing a plugin system (called &lt;a href=&quot;https://sandervanderburg.blogspot.com/2015/07/deploying-state-with-disnix.html&quot;&gt;Dysnomia&lt;/a&gt;) takes care of most of their deployment steps, such as their activation and deactivation.&lt;br /&gt;&lt;br /&gt;A couple of years ago, I have demonstrated with a proof of concept implementation that &lt;a href=&quot;https://sandervanderburg.blogspot.com/2016/06/deploying-containers-with-disnix-as.html&quot;&gt;we can use Disnix and Dysnomia's features to deploy infrastructure components&lt;/a&gt;. This deployment approach is also capable of deploying multiple instances of container services to one machine.&lt;br /&gt;&lt;br /&gt;Recently, I have revisited that idea again and extended it so that we can now deploy a service-oriented system including most underlying container services with a single command-line instruction.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;About infrastructure deployment solutions&lt;/h2&gt;&lt;br /&gt;As described in the introduction, Disnix's purpose is service deployment and not infrastructure deployment. In the past, I have been using a variety of solutions to manage the underlying infrastructure of service-oriented systems:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;In the very beginning, while working on my master thesis internship (in which I built the first prototype version of Disnix), there was not much automation at all -- for most of my testing activities I &lt;b&gt;manually&lt;/b&gt; created VirtualBox virtual machines and manually installed NixOS on them, with all essential container servers, such as Apache Tomcat and MySQL, because these were the container services that my target system required.&lt;br /&gt;&lt;br /&gt;Even after some decent Nix-based automated solutions appeared, I still ended up doing manual deployments for non-NixOS machines. For example, I still remember &lt;a href=&quot;https://sandervanderburg.blogspot.com/2015/11/deploying-services-to-heterogeneous.html&quot;&gt;the steps I had to perform to prepare myself for the demo I gave at NixCon 2015&lt;/a&gt;, in which I configured a small heterogeneous network consisting of an Ubuntu, NixOS, and Windows machine. It took me many hours of preparation time to get the demo right.&lt;/li&gt;&lt;li&gt;Some time later, for a research paper about &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/02/using-nixos-for-declarative-deployment.html&quot;&gt;declarative deployment and testing&lt;/a&gt;, we have developed a tool called &lt;i&gt;nixos-deploy-network&lt;/i&gt; that deploys NixOS configurations in a network of machines and is driven by a networked NixOS configuration file.&lt;/li&gt;&lt;li&gt;Around the same time, I have also developed a similar tool called: &lt;i&gt;disnixos-deploy-network&lt;/i&gt; that uses Disnix's deployment mechanisms to remotely deploy a network of NixOS configurations. It was primarily developed to show that Disnix's plugin system: Dysnomia, could also treat entire NixOS configurations as services.&lt;/li&gt;&lt;li&gt;When NixOps appeared (initially it was called Charon), I have also created facilities in the DisnixOS toolset to integrate with it -- for example DisnixOS can automatically convert a NixOps configuration to a Disnix infrastructure model.&lt;/li&gt;&lt;li&gt;And finally, I have created a proof of concept implementation that shows that Disnix can also treat every container service as a Disnix service and deploy it.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The idea behind the last approach is that we deploy two systems in sequential order with Disnix -- the former consisting of the container services and the latter of the application services.&lt;br /&gt;&lt;br /&gt;For example, if we want to deploy a system that consists of a number of Java web applications and MySQL databases, such as the infamous &lt;a href=&quot;https://github.com/svanderburg/disnix-stafftracker-java-example&quot;&gt;Disnix StaffTracker example application (Java version)&lt;/a&gt;, then we must first deploy a system with Disnix that provides the containers: the MySQL DBMS and Apache Tomcat:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-env -s services-containers.nix \&lt;br /&gt;  -i infrastructure-bare.nix \&lt;br /&gt;  -d distribution-containers.nix \&lt;br /&gt;  --profile containers&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;As described in earlier blog posts about Disnix, deployments are driven by three configuration files -- the &lt;b&gt;services&lt;/b&gt; model captures all distributable components of which the system consists (called services in a Disnix-context), the &lt;b&gt;infrastructure&lt;/b&gt; model captures all target machines in the network and their relevant properties, and the &lt;b&gt;distribution&lt;/b&gt; model specifies the mappings of services in the services model to the target machines (and container services already available on the machines in the network).&lt;br /&gt;&lt;br /&gt;All the container services in the services model provide above refer to systemd services, that in addition to running Apache Tomcat and MySQL, also do the following:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;They bundle a &lt;strong&gt;Dysnomia plugin&lt;/strong&gt; that can be used to manage the life-cycles of Java web applications and MySQL databases.&lt;/li&gt;&lt;li&gt;They bundle a &lt;strong&gt;Dysnomia container&lt;/strong&gt; configuration file capturing all relevant container configuration properties, such as the MySQL TCP port the daemon listens to, and the Tomcat web application deployment directory.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;For example, the Nix expression that configures Apache Tomcat has roughly the following structure:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;font-size: 90%; overflow: auto;&quot;&gt;&lt;br /&gt;{stdenv, dysnomia, httpPort, catalinaBaseDir, instanceSuffix ? &quot;&quot;}:&lt;br /&gt;&lt;br /&gt;stdenv.mkDerivation {&lt;br /&gt;  name = &quot;simpleAppservingTomcat&quot;;&lt;br /&gt;  ...&lt;br /&gt;  postInstall = ''&lt;br /&gt;    # Add Dysnomia container configuration file for a Tomcat web application&lt;br /&gt;    mkdir -p $out/etc/dysnomia/containers&lt;br /&gt;    cat &amp;gt; $out/etc/dysnomia/containers/tomcat-webapplication${instanceSuffix} &amp;lt;&amp;lt;EOF&lt;br /&gt;    tomcatPort=${toString httpPort}&lt;br /&gt;    catalinaBaseDir=${catalinaBaseDir}&lt;br /&gt;    EOF&lt;br /&gt;&lt;br /&gt;    # Copy the Dysnomia module that manages an Apache Tomcat web application&lt;br /&gt;    mkdir -p $out/libexec/dysnomia&lt;br /&gt;    ln -s ${dysnomia}/libexec/dysnomia/tomcat-webapplication $out/libexec/dysnomia&lt;br /&gt;  '';&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;First, the Nix expression will build and configure Apache Tomcat (this is left out of the example to keep it short). After Apache Tomcat has been built and configured, the Nix expression generates the container configuration file and copies the &lt;i&gt;tomcat-webapplication&lt;/i&gt; Dysnomia module from the Dysnomia toolset.&lt;br /&gt;&lt;br /&gt;The &lt;i&gt;disnix-env&lt;/i&gt; command-line instruction shown earlier, deploys container services to target machines in the network, using a bare infrastructure model that does not provide any container services except the init system (which is systemd on NixOS). The &lt;i&gt;profile&lt;/i&gt; parameter specifies a Disnix &lt;strong&gt;profile&lt;/strong&gt; to tell the tool that we are deploying a different kind of system than the default.&lt;br /&gt;&lt;br /&gt;If the command above succeeds, then we have all required container services at our disposal. The deployment architecture of the resulting system may look as follows:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-s0BNMwOC1d0/XqdCcmJYJsI/AAAAAAAAKBY/Xyln6YE1Xqs-eOhpEImsE2TJZ7gmfT-8gCLcBGAsYHQ/s1600/deploymentarch-containers.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://4.bp.blogspot.com/-s0BNMwOC1d0/XqdCcmJYJsI/AAAAAAAAKBY/Xyln6YE1Xqs-eOhpEImsE2TJZ7gmfT-8gCLcBGAsYHQ/s640/deploymentarch-containers.png&quot; width=&quot;520&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In the above diagram, the light grey colored boxes correspond to machines in a network, the dark grey boxes to container environments, and white ovals to services.&lt;br /&gt;&lt;br /&gt;As you may observe, we have deployed three services -- to the &lt;i&gt;test1&lt;/i&gt; machine we have deployed an Apache Tomcat service (that itself is managed by systemd), and to the &lt;i&gt;test2&lt;/i&gt; machine we have deployed both Apache Tomcat and the MySQL server (both their lifecycles are managed with systemd).&lt;br /&gt;&lt;br /&gt;We can run the following command to generate a new infrastructure model that provides the properties of these newly deployed container services:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-capture-infra infrastructure-bare.nix &amp;gt; infrastructure.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;As shown earlier, the retrieved infrastructure model provides all relevant configuration properties of the MySQL and Apache Tomcat containers that we have just deployed, because they expose their configuration properties via container configuration files.&lt;br /&gt;&lt;br /&gt;By using the retrieved infrastructure model and running the following command, we can deploy our web application and database components:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-env -s services.nix \&lt;br /&gt;  -i infrastructure.nix \&lt;br /&gt;  -d distribution.nix \&lt;br /&gt;  --profile services&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In the above command-line invocation, the services model contains all application components, and the distribution model maps these application components to the corresponding target machines and their containers.&lt;br /&gt;&lt;br /&gt;As with the previous &lt;i&gt;disnix-env&lt;/i&gt; command invocation, we provide a &lt;i&gt;--profile&lt;/i&gt; parameter to tell Disnix that we are deploying a different system. If we would use the same profile parameter as in the previous example, then Disnix will undeploy the container services and tries to upgrade the system with the application services, which will obviously fail.&lt;br /&gt;&lt;br /&gt;If the above command succeeds, then we have successfully deployed both the container and application services that our example system requires, resulting in a fully functional and activated system with a deployment architecture that may have the following structure:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-8EJWISAWP1o/XqdG_AWLYuI/AAAAAAAAKBk/aZBsQYwsy-c9huv78D03D1RFfQ41KACqQCLcBGAsYHQ/s1600/deploymentarch-services.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;235&quot; src=&quot;https://3.bp.blogspot.com/-8EJWISAWP1o/XqdG_AWLYuI/AAAAAAAAKBk/aZBsQYwsy-c9huv78D03D1RFfQ41KACqQCLcBGAsYHQ/s640/deploymentarch-services.png&quot; width=&quot;520&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;As may you may observe by looking at the diagram above, we have deployed a system that consists of a number of MySQL databases, Java web services and Java web applications.&lt;br /&gt;&lt;br /&gt;The diagram uses the same notational conventions used in the previous diagram. The arrows denote inter-dependency relationships, telling Disnix that one service depends on another, and that dependency should be deployed first.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Exposing services as containers&lt;/h2&gt;&lt;br /&gt;The Disnix service container deployment approach that I just described works, but it is not an integrated solution -- it has a limitation that is comparable to the infrastructure and services deployment separation that I have explained earlier. It requires you to run two deployments: one for the containers and one for the services.&lt;br /&gt;&lt;br /&gt;In the blog post that I wrote a couple of years ago, I also explained that in order to fully automate the entire process with a single command, this might eventually lead to &quot;a layered deployment approach&quot; -- the idea was to combine several system deployment processes into one. For example, you might want to deploy a service manager in the first layer, the container services for application components in the second, and in the third the application components themselves.&lt;br /&gt;&lt;br /&gt;I also argued that it is probably not worth spending a lot of effort in automating multiple deployment layers -- for nearly all systems that I deployed there were only two &quot;layers&quot; that I need to keep track of -- the infrastructure layer providing container services, and a service layer providing the application services. NixOps sufficed as a solution to automate the infrastructure parts for most of my use cases, except for deployment to non-NixOS machines, and deploying multiple instances of container services, which is a very uncommon use case.&lt;br /&gt;&lt;br /&gt;However, I got inspired to revisit this problem again after I completed my work described in &lt;a href=&quot;https://sandervanderburg.blogspot.com/2020/02/a-declarative-process-manager-agnostic.html&quot;&gt;the previous blog post&lt;/a&gt; -- in my previous blog post, I have created a process manager-agnostic service management framework that works with a variety of process managers on a variety of operating systems.&lt;br /&gt;&lt;br /&gt;Combining this framework with Disnix, makes it possible to also easily deploy container services (most of them are daemons) to non-NixOS machines, including non-Linux machines, such as macOS and FreeBSD from the same declarative specifications.&lt;br /&gt;&lt;br /&gt;Moreover, this framework also provides facilities to easily deploy multiple instances of the same service to the same machine.&lt;br /&gt;&lt;br /&gt;Revisiting this problem also made me think about the &quot;layered approach&quot; again, and after some thinking I have dropped the idea. The problem of using layers is that:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;We need to develop &lt;b&gt;another tool&lt;/b&gt; that integrates the deployment processes of all layers into one. In addition to the fact that we need to implement more automation, this introduces many additional technical challenges -- for example, if we want to deploy three layers and the deployment of the second fails, how are we going to do a rollback?&lt;/li&gt;&lt;li&gt;A layered approach is somewhat &quot;&lt;b&gt;imperative&lt;/b&gt;&quot; -- each layer deploys services that include Dysnomia modules and Dysnomia container configuration files. The Disnix service on each target machine performs a lookup in the Nix profile that contains all packages of the containers layer to find the required Dysnomia modules and container configuration files.&lt;br /&gt;&lt;br /&gt;Essentially, Dysnomia modules and container configurations are stored in a global namespace. This means the order in which the deployment of the layers is executed is important and that each layer can imperatively modify the behaviour of each Dysnomia module.&lt;/li&gt;&lt;li&gt;Because we need to deploy the system on layer-by-layer basis, we cannot for example, deploy multiple services in another layer that have no dependency in parallel, making a deployment process &lt;b&gt;slower&lt;/b&gt; than it should be.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;After some thinking, I came up with a much simpler approach -- I have introduced a new concept to the Disnix services model that makes it possible to &lt;b&gt;annotate&lt;/b&gt; services with a specification of the &lt;b&gt;container services&lt;/b&gt; that it provides. This information can be used by application services that need to deploy to this container service.&lt;br /&gt;&lt;br /&gt;For example, we can annotate the Apache Tomcat service in the Disnix services model as follows:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;{ pkgs, system, distribution, invDistribution&lt;br /&gt;, stateDir ? &quot;/var&quot;&lt;br /&gt;, runtimeDir ? &quot;${stateDir}/run&quot;&lt;br /&gt;, logDir ? &quot;${stateDir}/log&quot;&lt;br /&gt;, cacheDir ? &quot;${stateDir}/cache&quot;&lt;br /&gt;, tmpDir ? (if stateDir == &quot;/var&quot; then &quot;/tmp&quot; else &quot;${stateDir}/tmp&quot;)&lt;br /&gt;, forceDisableUserChange ? false&lt;br /&gt;, processManager ? &quot;systemd&quot;&lt;br /&gt;}:&lt;br /&gt;&lt;br /&gt;let&lt;br /&gt;  constructors = import ../../../nix-processmgmt/examples/services-agnostic/constructors.nix {&lt;br /&gt;    inherit pkgs stateDir runtimeDir logDir cacheDir tmpDir forceDisableUserChange processManager;&lt;br /&gt;  };&lt;br /&gt;in&lt;br /&gt;rec {&lt;br /&gt;  simpleAppservingTomcat = rec {&lt;br /&gt;    name = &quot;simpleAppservingTomcat&quot;;&lt;br /&gt;    pkg = constructors.simpleAppservingTomcat {&lt;br /&gt;      inherit httpPort;&lt;br /&gt;      commonLibs = [ &quot;${pkgs.mysql_jdbc}/share/java/mysql-connector-java.jar&quot; ];&lt;br /&gt;    };&lt;br /&gt;    httpPort = 8080;&lt;br /&gt;    catalinaBaseDir = &quot;/var/tomcat/webapps&quot;;&lt;br /&gt;    type = &quot;systemd-unit&quot;;&lt;br /&gt;    providesContainers = {&lt;br /&gt;      tomcat-webapplication = {&lt;br /&gt;        httpPort = 8080;&lt;br /&gt;        catalinaBaseDir = &quot;/var/tomcat/webapps&quot;;&lt;br /&gt;      };&lt;br /&gt;    };&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  GeolocationService = {&lt;br /&gt;    name = &quot;GeolocationService&quot;;&lt;br /&gt;    pkg = customPkgs.GeolocationService;&lt;br /&gt;    dependsOn = {};&lt;br /&gt;    type = &quot;tomcat-webapplication&quot;;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  ...&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In the above example, the &lt;i&gt;simpleAppservingTomcat&lt;/i&gt; service refers to an Apache Tomcat server that serves Java web applications for one particular virtual host. The &lt;i&gt;providesContainers&lt;/i&gt; property tells Disnix that the service is a container provider, providing a container named: &lt;i&gt;tomcat-webapplication&lt;/i&gt; with the following properties:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;For HTTP traffic, Apache Tomcat should listen on TCP port 8080&lt;/li&gt;&lt;li&gt;The Java web application archives (WAR files) should be deployed to the Catalina Servlet container. By copying the WAR files to the &lt;i&gt;/var/tomcat/webapps&lt;/i&gt; directory, they should be automatically hot-deployed.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The other service in the services model (&lt;i&gt;GeolocationService&lt;/i&gt;) is a Java web application that should be deployed to a Apache Tomcat container service.&lt;br /&gt;&lt;br /&gt;If in a Disnix distribution model, we map the Apache Tomcat service (&lt;i&gt;simpleAppservingTomcat&lt;/i&gt;) and the Java web application (&lt;i&gt;GeolocationService&lt;/i&gt;) to the same machine:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;{infrastructure}:&lt;br /&gt;&lt;br /&gt;{&lt;br /&gt;  simpleAppservingTomcat = [ infrastructure.test1 ];&lt;br /&gt;  GeolocationService = [ infrastructure.test1 ];&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;Disnix will automatically search for a suitable container service provider for each service.&lt;br /&gt;&lt;br /&gt;In the above scenario, Disnix knows that &lt;i&gt;simpleAppservingTomcat&lt;/i&gt; provides a &lt;i&gt;tomcat-webapplication&lt;/i&gt; container. The &lt;i&gt;GeolocationService&lt;/i&gt; uses the type: &lt;i&gt;tomcat-webapplication&lt;/i&gt; indicating that it needs to deployed to a Apache Tomcat servlet container.&lt;br /&gt;&lt;br /&gt;Because these services have been deployed to the same machine Disnix will make sure that Apache Tomcat gets activated before the &lt;i&gt;GeolocationService&lt;/i&gt;, and uses the Dysnomia module that is bundled with the &lt;i&gt;simpleAppservingTomcat&lt;/i&gt; to handle the deployment of the Java web application.&lt;br /&gt;&lt;br /&gt;Furthermore, the properties that &lt;i&gt;simpleAppservingTomcat&lt;/i&gt; exposes in the &lt;i&gt;providesContainers&lt;/i&gt; attribute set, are automatically propagated as container parameters to the &lt;i&gt;GeolocationService&lt;/i&gt; Nix expression, so that it knows where the WAR file should be copied to, to automatically hot-deploy the service.&lt;br /&gt;&lt;br /&gt;If Disnix does not detect a service that provides a required container deployed to the same machine, then it will fall back to its original behaviour -- it automatically propagates the properties of a container in the infrastructure model, and assumes the the container service is already deployed by an infrastructure deployment solution.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Simplifications&lt;/h2&gt;&lt;br /&gt;The notation used for the &lt;i&gt;simpleAppservingTomcat&lt;/i&gt; service (shown earlier) refers to an attribute set. An attribute set also makes it possible to specify multiple container instances. However, it is far more common that we only need one single container instance.&lt;br /&gt;&lt;br /&gt;Moreover, there is some redundancy -- we need to specify certain properties in two places. Some properties can both belong to a service, as well as the container properties that we want to propagate to the services that require it.&lt;br /&gt;&lt;br /&gt;We can also use a shorter notation to expose only one single container:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;simpleAppservingTomcat = rec {&lt;br /&gt;  name = &quot;simpleAppservingTomcat&quot;;&lt;br /&gt;  pkg = constructors.simpleAppservingTomcat {&lt;br /&gt;    inherit httpPort;&lt;br /&gt;    commonLibs = [ &quot;${pkgs.mysql_jdbc}/share/java/mysql-connector-java.jar&quot; ];&lt;br /&gt;  };&lt;br /&gt;  httpPort = 8080;&lt;br /&gt;  catalinaBaseDir = &quot;/var/tomcat/webapps&quot;;&lt;br /&gt;  type = &quot;systemd-unit&quot;;&lt;br /&gt;  providesContainer = &quot;tomcat-webapplication&quot;;&lt;br /&gt;};&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In the above example, we have rewritten the service configuration of &lt;i&gt;simpleAppserviceTomcat&lt;/i&gt; to use the &lt;i&gt;providesContainer&lt;/i&gt; attribute referring to a string. This shorter notation will automatically expose all non-reserved service properties as container properties.&lt;br /&gt;&lt;br /&gt;For our example above, this means that it will automatically expose &lt;i&gt;httpPort&lt;/i&gt;, and &lt;i&gt;catalinaBaseDir&lt;/i&gt; and ignores the remaining properties -- these remaining properties have a specific purpose for the Disnix deployment system.&lt;br /&gt;&lt;br /&gt;Although the notation above simplifies things considerably, the above example still contains a bit of redundancy -- some of the container properties that we want to expose to application services, also need to be propagated to the constructor function requiring us to specify the same properties twice.&lt;br /&gt;&lt;br /&gt;We can eliminate this redundancy by encapsulating the creation of the service properties attribute set a constructor function. With a constructor function, we can simply write:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;simpleAppservingTomcat = constructors.simpleAppservingTomcat {&lt;br /&gt;  httpPort = 8080;&lt;br /&gt;  commonLibs = [ &quot;${pkgs.mysql_jdbc}/share/java/mysql-connector-java.jar&quot; ];&lt;br /&gt;  type = &quot;systemd-unit&quot;;&lt;br /&gt;};&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h2&gt;Example: deploying container and application services as one system&lt;/h2&gt;&lt;br /&gt;By applying the techniques described in the previous section to the StaffTracker example (e.g. distributing a &lt;i&gt;simpleAppservingTomcat&lt;/i&gt; and &lt;i&gt;mysql&lt;/i&gt; to the same machines that host Java web applications and MySQL databases), we can deploy the StaffTracker system including all its required container services with a single command-line instruction:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-env -s services-with-containers.nix \&lt;br /&gt;  -i infrastructure-bare.nix \&lt;br /&gt;  -d distribution-with-containers.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The corresponding deployment architecture visualization may look as follows:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-SCCzN8g3iHk/XqfyGdXiipI/AAAAAAAAKBw/MW2eE7eaj1w3Mm3AxNWiwGLjh0E9v5jewCLcBGAsYHQ/s1600/deploymentarch-combined.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://3.bp.blogspot.com/-SCCzN8g3iHk/XqfyGdXiipI/AAAAAAAAKBw/MW2eE7eaj1w3Mm3AxNWiwGLjh0E9v5jewCLcBGAsYHQ/s640/deploymentarch-combined.png&quot; width=&quot;520&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;As you may notice, the above diagram looks very similar to the previously shown deployment architecture diagram of the services layer.&lt;br /&gt;&lt;br /&gt;What has been added are the container services -- the ovals with the double borders denote services that are also container providers. The labels describe both the name of the service and the containers that it provides (behind the arrow &lt;i&gt;-&amp;gt;&lt;/i&gt;).&lt;br /&gt;&lt;br /&gt;Furthermore, all the services that are hosted inside a particular container environment (e.g. &lt;i&gt;tomcat-webapplication&lt;/i&gt;) have a local inter-dependency on the corresponding container provider service (e.g. &lt;i&gt;simpleAppservingTomcat&lt;/i&gt;), causing Disnix to activate Apache Tomcat before the web applications that are hosted inside it.&lt;br /&gt;&lt;br /&gt;Another thing you might notice, is that we have not completely eliminated the dependency on an infrastructure deployment solution -- the MySQL DBMS and Apache Tomcat service are deployed as &lt;i&gt;systemd-unit&lt;/i&gt; requiring the presence of systemd on the target system. Systemd should be provided as part of the target Linux distribution, and cannot be managed by Disnix because it runs as PID 1.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Example: deploying multiple container service instances and application services&lt;/h2&gt;&lt;br /&gt;One of my motivating reasons to use Disnix as a deployment solution for container services is to be able to deploy multiple instances of them to the same machine. This can also be done in a combined container and application services deployment approach.&lt;br /&gt;&lt;br /&gt;To allow, for example, to have two instance of Apache Tomcat to co-exist on one machine, we must configure them in such a way their resources do not conflict:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;{ pkgs, system, distribution, invDistribution&lt;br /&gt;, stateDir ? &quot;/var&quot;&lt;br /&gt;, runtimeDir ? &quot;${stateDir}/run&quot;&lt;br /&gt;, logDir ? &quot;${stateDir}/log&quot;&lt;br /&gt;, cacheDir ? &quot;${stateDir}/cache&quot;&lt;br /&gt;, tmpDir ? (if stateDir == &quot;/var&quot; then &quot;/tmp&quot; else &quot;${stateDir}/tmp&quot;)&lt;br /&gt;, forceDisableUserChange ? false&lt;br /&gt;, processManager ? &quot;systemd&quot;&lt;br /&gt;}:&lt;br /&gt;&lt;br /&gt;let&lt;br /&gt;  constructors = import ../../../nix-processmgmt/examples/service-containers-agnostic/constructors.nix {&lt;br /&gt;    inherit pkgs stateDir runtimeDir logDir cacheDir tmpDir forceDisableUserChange processManager;&lt;br /&gt;  };&lt;br /&gt;in&lt;br /&gt;rec {&lt;br /&gt;  simpleAppservingTomcat-primary = constructors.simpleAppservingTomcat {&lt;br /&gt;    instanceSuffix = &quot;-primary&quot;;&lt;br /&gt;    httpPort = 8080;&lt;br /&gt;    httpsPort = 8443;&lt;br /&gt;    serverPort = 8005;&lt;br /&gt;    ajpPort = 8009;&lt;br /&gt;    commonLibs = [ &quot;${pkgs.mysql_jdbc}/share/java/mysql-connector-java.jar&quot; ];&lt;br /&gt;    type = &quot;systemd-unit&quot;;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  simpleAppservingTomcat-secondary = constructors.simpleAppservingTomcat {&lt;br /&gt;    instanceSuffix = &quot;-secondary&quot;;&lt;br /&gt;    httpPort = 8081;&lt;br /&gt;    httpsPort = 8444;&lt;br /&gt;    serverPort = 8006;&lt;br /&gt;    ajpPort = 8010;&lt;br /&gt;    commonLibs = [ &quot;${pkgs.mysql_jdbc}/share/java/mysql-connector-java.jar&quot; ];&lt;br /&gt;    type = &quot;systemd-unit&quot;;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  ...&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;The above partial services model defines two Apache Tomcat instances, that have been configured to listen to different TCP ports (for example the primary Tomcat instance listens to HTTP traffic on port 8080, whereas the secondary instance listens on port 8081), and serving web applications from a different deployment directories. Because their properties do not conflict, they can co-exist on the same machine.&lt;br /&gt;&lt;br /&gt;With the following distribution model, we can deploy multiple container providers to the same machine and distribute application services to them:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;{infrastructure}:&lt;br /&gt;&lt;br /&gt;{&lt;br /&gt;  # Container providers&lt;br /&gt;&lt;br /&gt;  mysql-primary = [ infrastructure.test1 ];&lt;br /&gt;  mysql-secondary = [ infrastructure.test1 ];&lt;br /&gt;  simpleAppservingTomcat-primary = [ infrastructure.test2 ];&lt;br /&gt;  simpleAppservingTomcat-secondary = [ infrastructure.test2 ];&lt;br /&gt;&lt;br /&gt;  # Application components&lt;br /&gt;&lt;br /&gt;  GeolocationService = {&lt;br /&gt;    targets = [&lt;br /&gt;      { target = infrastructure.test2;&lt;br /&gt;        container = &quot;tomcat-webapplication-primary&quot;;&lt;br /&gt;      }&lt;br /&gt;    ];&lt;br /&gt;  };&lt;br /&gt;  RoomService = {&lt;br /&gt;    targets = [&lt;br /&gt;      { target = infrastructure.test2;&lt;br /&gt;        container = &quot;tomcat-webapplication-secondary&quot;;&lt;br /&gt;      }&lt;br /&gt;    ];&lt;br /&gt;  };&lt;br /&gt;  StaffTracker = {&lt;br /&gt;    targets = [&lt;br /&gt;      { target = infrastructure.test2;&lt;br /&gt;        container = &quot;tomcat-webapplication-secondary&quot;;&lt;br /&gt;      }&lt;br /&gt;    ];&lt;br /&gt;  };&lt;br /&gt;  staff = {&lt;br /&gt;    targets = [&lt;br /&gt;      { target = infrastructure.test1;&lt;br /&gt;        container = &quot;mysql-database-secondary&quot;;&lt;br /&gt;      }&lt;br /&gt;    ];&lt;br /&gt;  };&lt;br /&gt;  zipcodes = {&lt;br /&gt;    targets = [&lt;br /&gt;      { target = infrastructure.test1;&lt;br /&gt;        container = &quot;mysql-database-primary&quot;;&lt;br /&gt;      }&lt;br /&gt;    ];&lt;br /&gt;  };&lt;br /&gt;  ...&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In the first four lines of the distribution model shown above, we distribute the container providers. As you may notice, we distribute two MySQL instances that should co-exist on machine &lt;i&gt;test1&lt;/i&gt; and two Apache Tomcat instances that should co-exist on machine &lt;i&gt;test2&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;In the remainder of the distribution model, we map Java web applications and MySQL databases to these container providers. As explained in the previous blog post about deploying multiple container service instances, if no container is specified in the distribution model, Disnix will auto map the service to the container that has the same name as the service's &lt;i&gt;type&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;In the above example, we have two instances of each container service with a different name. As a result, we need to use the more verbose notation for distribution mappings to instruct Disnix to which container provider we want to deploy the service.&lt;br /&gt;&lt;br /&gt;Deploying the system with the following command-line instruction:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-env -s services-with-multicontainers.nix \&lt;br /&gt;  -i infrastructure-bare.nix \&lt;br /&gt;  -d distribution-with-multicontainers.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;results in a running system that may has the following deployment architecture:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kn5TjiUpuT4/XqgAf1M9r_I/AAAAAAAAKB8/CBkVKFmlu_YbUWeedLVLWQvwgiMgyaVoQCLcBGAsYHQ/s1600/deploymentarch-multicontainers.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://1.bp.blogspot.com/-kn5TjiUpuT4/XqgAf1M9r_I/AAAAAAAAKB8/CBkVKFmlu_YbUWeedLVLWQvwgiMgyaVoQCLcBGAsYHQ/s640/deploymentarch-multicontainers.png&quot; width=&quot;520&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;As you may notice, we have MySQL databases and Java web application distributed over mutiple container providers residing on the same machine. All services belong to the same system, deployed by a single Disnix command.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;A more extreme example: multiple process managers&lt;/h2&gt;&lt;br /&gt;By exposing services as container providers in Disnix, my original requirements were met. Because the facilities are very flexible, I also discovered that there is much more I could do.&lt;br /&gt;&lt;br /&gt;For example, on more primitive systems that do not have systemd, I could also extend the services and distribution models in such a way that I can deploy supervisord as a process manager first (as a &lt;i&gt;sysvinit-script&lt;/i&gt; that does not require any process manager service), then use supervisord to manage MySQL and Apache Tomcat, and then use the Dysnomia plugin system to deploy the databases and Java web applications to these container services managed by supervisord:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-SnxhfSHR7ZI/XqgXfmpV9uI/AAAAAAAAKCI/bepnVXFy09I42Dt_-t5wur-QWdb4-DJRACLcBGAsYHQ/s1600/deploymentarch-extreme.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://3.bp.blogspot.com/-SnxhfSHR7ZI/XqgXfmpV9uI/AAAAAAAAKCI/bepnVXFy09I42Dt_-t5wur-QWdb4-DJRACLcBGAsYHQ/s640/deploymentarch-extreme.png&quot; width=&quot;520&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;As you may notice, the deployment architecture above looks similar to the first combined deployment example, with &lt;i&gt;supervisord&lt;/i&gt; added as an extra container provider service.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;More efficient reuse: expose any kind of service as container provider&lt;/h2&gt;&lt;br /&gt;In addition to managed processes (which the MySQL DBMS and Apache Tomcat services are), any kind of Disnix service can act as a container provider.&lt;br /&gt;&lt;br /&gt;An example of such a non-process managed container provider could be &lt;a href=&quot;http://axis.apache.org/axis2&quot;&gt;Apache Axis2&lt;/a&gt;. In the StaffTracker example, all data access is provided by web services. These web services are implemented as Java web applications (WAR files) embedding an Apache Axis2 container that embeds an Axis2 Application Archive (AAR file) providing the web service implementation.&lt;br /&gt;&lt;br /&gt;Every web application that is a web service includes its own implementation of Apache Axis2.&lt;br /&gt;&lt;br /&gt;It is also possible to deploy a single Axis2 web application to Apache Tomcat, and treat each Axis2 Application Archive as a separate deployment unit using the &lt;i&gt;axis2-webservice&lt;/i&gt; identifier as a container provider for any service of the type: &lt;i&gt;axis2-webservice&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;pre style=&quot;overflow: auto;&quot;&gt;{ pkgs, system, distribution, invDistribution&lt;br /&gt;, stateDir ? &quot;/var&quot;&lt;br /&gt;, runtimeDir ? &quot;${stateDir}/run&quot;&lt;br /&gt;, logDir ? &quot;${stateDir}/log&quot;&lt;br /&gt;, cacheDir ? &quot;${stateDir}/cache&quot;&lt;br /&gt;, tmpDir ? (if stateDir == &quot;/var&quot; then &quot;/tmp&quot; else &quot;${stateDir}/tmp&quot;)&lt;br /&gt;, forceDisableUserChange ? false&lt;br /&gt;, processManager ? &quot;systemd&quot;&lt;br /&gt;}:&lt;br /&gt;&lt;br /&gt;let&lt;br /&gt;  constructors = import ../../../nix-processmgmt/examples/service-containers-agnostic/constructors.nix {&lt;br /&gt;    inherit pkgs stateDir runtimeDir logDir cacheDir tmpDir forceDisableUserChange processManager;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  customPkgs = import ../top-level/all-packages.nix {&lt;br /&gt;    inherit system pkgs stateDir;&lt;br /&gt;  };&lt;br /&gt;in&lt;br /&gt;rec {&lt;br /&gt;### Container providers&lt;br /&gt;&lt;br /&gt;  simpleAppservingTomcat = constructors.simpleAppservingTomcat {&lt;br /&gt;    httpPort = 8080;&lt;br /&gt;    commonLibs = [ &quot;${pkgs.mysql_jdbc}/share/java/mysql-connector-java.jar&quot; ];&lt;br /&gt;    type = &quot;systemd-unit&quot;;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  axis2 = customPkgs.axis2 {};&lt;br /&gt;&lt;br /&gt;### Web services&lt;br /&gt;&lt;br /&gt;  HelloService = {&lt;br /&gt;    name = &quot;HelloService&quot;;&lt;br /&gt;    pkg = customPkgs.HelloService;&lt;br /&gt;    dependsOn = {};&lt;br /&gt;    type = &quot;axis2-webservice&quot;;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  HelloWorldService = {&lt;br /&gt;    name = &quot;HelloWorldService&quot;;&lt;br /&gt;    pkg = customPkgs.HelloWorldService;&lt;br /&gt;    dependsOn = {&lt;br /&gt;      inherit HelloService;&lt;br /&gt;    };&lt;br /&gt;    type = &quot;axis2-webservice&quot;;&lt;br /&gt;  };&lt;br /&gt;&lt;br /&gt;  ...&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;In the above partial services model, we have defined two container providers:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;simpleAppservingTomcat&lt;/i&gt; that provides a Servlet container in which Java web applications (WAR files) can be hosted.&lt;/li&gt;&lt;li&gt;The &lt;i&gt;axis2&lt;/i&gt; service is a Java web application that acts as a container provider for Axis2 web services.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The remaining services are Axis2 web services that can be embedded inside the shared Axis2 container.&lt;br /&gt;&lt;br /&gt;If we deploy the above example system, e.g.:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;$ disnix-env -s services-optimised.nix \&lt;br /&gt;  -i infrastructure-bare.nix \&lt;br /&gt;  -d distribution-optimised.nix&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;may result in the following deployment architecture:&lt;br /&gt;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-5bpdPjW4BWg/Xqgw2C4e6nI/AAAAAAAAKCU/wxjonZMdIio-BBWWJB5PRb0PzM4V-W1agCLcBGAsYHQ/s1600/deploymentarch-optimised.png&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;https://1.bp.blogspot.com/-5bpdPjW4BWg/Xqgw2C4e6nI/AAAAAAAAKCU/wxjonZMdIio-BBWWJB5PRb0PzM4V-W1agCLcBGAsYHQ/s640/deploymentarch-optimised.png&quot; width=&quot;520&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;As may be observed when looking at the above architecture diagram, the web services deployed to the &lt;i&gt;test2&lt;/i&gt; machine, use a shared Axis2 container, that is embedded as a Java web application inside Apache Tomcat.&lt;br /&gt;&lt;br /&gt;The above system has a far better degree of reuse, because it does not use redundant copies of Apache Axis2 for each web service.&lt;br /&gt;&lt;br /&gt;Although it is possible to have a deployment architecture with a shared Axis2 container, this shared approach is not always desirable to use. For example, database connections managed by Apache Tomcat are shared between all web services embedded in an Axis2 container, which is not always desirable from a security point of view.&lt;br /&gt;&lt;br /&gt;Moreover, an unstable web service embedded in an Axis2 container might also tear the container down causing the other web services to crash as well. Still, the deployment system does not make it difficult to use a shared approach, when it is desired.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;br /&gt;With this new feature addition to Disnix, that can expose services as container providers, it becomes possible to deploy both container services and application services as one integrated system.&lt;br /&gt;&lt;br /&gt;Furthermore, it also makes it possible to:&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Deploy multiple instances of container services and deploy services to them.&lt;/li&gt;&lt;li&gt;For process-based service containers, we can combine the process manager-agostic framework described in the previous blog post, so that we can use them with any process manager on any operating system that it supports.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;The fact that Disnix can now also deploy containers does not mean that it no longer relies on external infrastructure deployment solutions anymore. For example, you still need target machines at your disposal that have Nix and Disnix installed and need to be remotely connectable, e.g. through SSH. For this, you still require an external infrastructure deployment solution, such as NixOps.&lt;br /&gt;&lt;br /&gt;Furthermore, not all container services can be managed by Disnix. For example, systemd, that runs as a system's PID 1, cannot be installed by Disnix. Instead, it must already be provided by the target system's Linux distribution (In NixOS' case it is Nix that deploys it, but it is not managed by Disnix).&lt;br /&gt;&lt;br /&gt;And there may also be other reasons why you may still want to use separated deployment processes for container and service deployment. For example, you may want to &lt;a href=&quot;https://sandervanderburg.blogspot.com/2011/10/deploying-net-services-with-disnix.html&quot;&gt;deploy to container services that cannot be managed by Nix/Disnix&lt;/a&gt;, or you may work in an organization in which two different teams take care of the infrastructure and the services.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Availability&lt;/h2&gt;&lt;br /&gt;The new features described in this blog post are part of the current development versions of Dysnomia and Disnix that can be obtained from &lt;a href=&quot;http://github.com/svanderburg&quot;&gt;my GitHub page&lt;/a&gt;. These features will become generally available in the next release.&lt;br /&gt;&lt;br /&gt;Moreover, I have extended all my public Disnix examples with container deployment support (including the Java-based StaffTracker and composition examples shown in this blog post). These changes currently reside in the &lt;i&gt;servicesascontainers&lt;/i&gt; Git branches.&lt;br /&gt;&lt;br /&gt;The &lt;a href=&quot;https://github.com/svanderburg/nix-processmgmt&quot;&gt;nix-processmgmt&lt;/a&gt; repository contains shared constructor functions for all kinds of system services, e.g. MySQL, Apache HTTP server, PostgreSQL and Apache Tomcat. These functions can be reused amongst all kinds of Disnix projects.&lt;br /&gt;&lt;br /&gt;</description>
	<pubDate>Thu, 30 Apr 2020 20:39:00 +0000</pubDate>
	<author>noreply@blogger.com (Sander van der Burg)</author>
</item>
<item>
	<title>Craige McWhirter: Building Daedalus Flight on NixOS</title>
	<guid isPermaLink="true">http://mcwhirter.com.au//craige/blog/2020/Building_Daedalus_Flight_on_NixOS/</guid>
	<link>http://mcwhirter.com.au//craige/blog/2020/Building_Daedalus_Flight_on_NixOS/</link>
	<description>&lt;p&gt;&lt;img alt=&quot;NixOS Daedalus Gears by Craige McWhirter&quot; src=&quot;http://mcwhirter.com.au/files/NixOS_Daedalus_Gears.png&quot; title=&quot;NixOS Daedalus Gears by Craige McWhirter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://daedaluswallet.io/en/flight/&quot;&gt;Daedalus Flight&lt;/a&gt; was recently released
and this is how you can build and run this version of
&lt;a href=&quot;https://daedaluswallet.io/&quot;&gt;Deadalus&lt;/a&gt; on &lt;a href=&quot;https://nixos.org/&quot;&gt;NixOS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you want to speed the build process up, you can add the
&lt;a href=&quot;https://iohk.io/&quot;&gt;IOHK&lt;/a&gt; &lt;a href=&quot;https://nixos.org/nix/&quot;&gt;Nix&lt;/a&gt; cache to your own NixOS configuration:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://source.mcwhirter.io/craige/mio-ops/src/branch/master/roles/iohk.nix&quot;&gt;iohk.nix&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;nix&quot;&gt;nix.binaryCaches = [
  &quot;https://cache.nixos.org&quot;
  &quot;https://hydra.iohk.io&quot;
];
nix.binaryCachePublicKeys = [
  &quot;hydra.iohk.io:f/Ea+s+dFdN+3Y/G+FDgSq+a5NEWhJGzdjvKNGv0/EQ=&quot;
];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you haven't already, you can clone the &lt;a href=&quot;https://github.com/input-output-hk/daedalus&quot;&gt;Daedalus
repo&lt;/a&gt; and specifically the
1.0.0 tagged commit:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone --branch 1.0.0 https://github.com/input-output-hk/daedalus.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you've cloned the repo and checked you're on the 1.0.0 tagged commit,
you can build Daedalus flight with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ nix build -f . daedalus --argstr cluster mainnet_flight
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the build completes, you're ready to launch Daedalus Flight:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./result/bin/daedalus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To verify that you have in fact built Daedalus Flight, first head to the
&lt;code&gt;Daedalus&lt;/code&gt; menu then &lt;code&gt;About Daedalus&lt;/code&gt;. You should see a title such as
&quot;DAEDALUS 1.0.0&quot;. The second check, is to press &lt;code&gt;[Ctl]+d&lt;/code&gt; to access &lt;code&gt;Daedalus
Diagnostocs&lt;/code&gt; and your &lt;code&gt;Daedalus state directory&lt;/code&gt; should have &lt;code&gt;mainnet_flight&lt;/code&gt;
at the end of the path.&lt;/p&gt;

&lt;p&gt;If you've got these, give yourself a pat on the back and grab yourself a
refreshing bevvy while you wait for blocks to sync.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Daedalus FC1 screenshot&quot; src=&quot;http://mcwhirter.com.au/files/Daedalus_FC1.png&quot; title=&quot;Daedalus FC1 screenshot&quot; /&gt;&lt;/p&gt;</description>
	<pubDate>Thu, 23 Apr 2020 23:28:59 +0000</pubDate>
</item>
<item>
	<title>nixbuild.net: Binary Cache Support</title>
	<guid isPermaLink="true">https://blog.nixbuild.net/posts/2020-04-18-binary-cache-support.html</guid>
	<link>https://blog.nixbuild.net/posts/2020-04-18-binary-cache-support.html</link>
	<description>&lt;p&gt;Up until now, nixbuild.net has not supported directly fetching build dependencies from binary caches like &lt;a href=&quot;https://cache.nixos.org&quot;&gt;cache.nixos.org&lt;/a&gt; or &lt;a href=&quot;https://cachix.org&quot;&gt;Cachix&lt;/a&gt;. All build dependencies have instead been uploaded from the user’s local machine to nixbuild.net the first time they’ve been needed.&lt;/p&gt;
&lt;p&gt;Today, this bottleneck has been removed, since nixbuild.net now can fetch build dependencies directly from binary caches, without taxing users’ upload bandwidth.&lt;/p&gt;

&lt;p&gt;By default, the official Nix binary cache (&lt;a href=&quot;https://cache.nixos.org&quot;&gt;cache.nixos.org&lt;/a&gt;) is added to all nixbuild.net accounts, but a nixbuild.net user can freely decide on which caches that should be queried for build dependencies (including &lt;a href=&quot;https://cachix.org&quot;&gt;Cachix&lt;/a&gt; caches).&lt;/p&gt;
&lt;p&gt;An additional benefit of the new support for binary caches is that users that trust the same binary caches automatically share build dependencies from those caches. This means that if one user’s build has triggered a download from for example cache.nixos.org, the next user that comes along and needs the same build dependency doesn’t have to spend time on downloading that dependency.&lt;/p&gt;
&lt;p&gt;For more information on how to use binary caches with nixbuild.net, see the &lt;a href=&quot;https://docs.nixbuild.net/getting-started/&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;</description>
	<pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
	<author>support@nixbuild.net (nixbuild.net)</author>
</item>
<item>
	<title>Graham Christensen: Erase your darlings</title>
	<guid isPermaLink="false">http://grahamc.com//blog/erase-your-darlings</guid>
	<link>http://grahamc.com/blog/erase-your-darlings</link>
	<description>&lt;p&gt;I erase my systems at every boot.&lt;/p&gt;

&lt;p&gt;Over time, a system collects state on its root partition. This state
lives in assorted directories like &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;/var&lt;/code&gt;, and represents
every under-documented or out-of-order step in bringing up the
services.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Right, run &lt;code class=&quot;highlighter-rouge&quot;&gt;myapp-init&lt;/code&gt;.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;These small, inconsequential “oh, oops” steps are the pieces that get
lost and don’t appear in your runbooks.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Just download ca-certificates to … to fix …”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Each of these quick fixes leaves you doomed to repeat history in three
years when you’re finally doing that dreaded RHEL 7 to RHEL 8 upgrade.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Oh, &lt;code class=&quot;highlighter-rouge&quot;&gt;touch /etc/ipsec.secrets&lt;/code&gt; or the l2tp tunnel won’t work.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;immutable-infrastructure-gets-us-so-close&quot;&gt;Immutable infrastructure gets us &lt;em&gt;so&lt;/em&gt; close&lt;/h3&gt;

&lt;p&gt;Immutable infrastructure is a wonderfully effective method of
eliminating so many of these forgotten steps. Leaning in to the pain
by deleting and replacing your servers on a weekly or monthly basis
means you are constantly testing and exercising your automation and
runbooks.&lt;/p&gt;

&lt;p&gt;The nugget here is the regular and indiscriminate removal of system
state. Destroying the whole server doesn’t leave you much room to
forget the little tweaks you made along the way.&lt;/p&gt;

&lt;p&gt;These techniques work great when you meet two requirements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;you can provision and destroy servers with an API call&lt;/li&gt;
  &lt;li&gt;the servers aren’t inherently stateful&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;long-running-servers&quot;&gt;Long running servers&lt;/h4&gt;

&lt;p&gt;There are lots of cases in which immutable infrastructure &lt;em&gt;doesn’t&lt;/em&gt;
work, and the dirty secret is &lt;strong&gt;those servers need good tools the
most.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Long-running servers cause long outages. Their runbooks are outdated
and incomplete. They accrete tweaks and turn in to an ossified,
brittle snowflake — except its arms are load-bearing.&lt;/p&gt;

&lt;p&gt;Let’s bring the ideas of immutable infrastructure to these systems
too. Whether this system is embedded in a stadium’s jumbotron, in a
datacenter, or under your desk, we &lt;em&gt;can&lt;/em&gt; keep the state under control.&lt;/p&gt;

&lt;h4 id=&quot;fhs-isnt-enough&quot;&gt;FHS isn’t enough&lt;/h4&gt;

&lt;p&gt;The hard part about applying immutable techniques to long running
servers is knowing exactly where your application state ends and the
operating system, software, and configuration begin.&lt;/p&gt;

&lt;p&gt;This is hard because legacy operating systems and the Filesystem
Hierarchy Standard poorly separate these areas of concern. For
example, &lt;code class=&quot;highlighter-rouge&quot;&gt;/var/lib&lt;/code&gt; is for state information, but how much of this do
you actually care about tracking? What did you configure in &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt; on
purpose?&lt;/p&gt;

&lt;p&gt;The answer is probably not a lot.&lt;/p&gt;

&lt;p&gt;You may not care, but all of this accumulation of junk is a tarpit.
Everything becomes harder: replicating production, testing changes,
undoing mistakes.&lt;/p&gt;

&lt;h3 id=&quot;new-computer-smell&quot;&gt;New computer smell&lt;/h3&gt;

&lt;p&gt;Getting a new computer is this moment of cleanliness. The keycaps
don’t have oils on them, the screen is perfect, and the hard drive
is fresh and unspoiled — for about an hour or so.&lt;/p&gt;

&lt;p&gt;Let’s get back to that.&lt;/p&gt;

&lt;h2 id=&quot;how-is-this-possible&quot;&gt;How is this possible?&lt;/h2&gt;

&lt;p&gt;NixOS can boot with only two directories: &lt;code class=&quot;highlighter-rouge&quot;&gt;/boot&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;/nix&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/nix&lt;/code&gt; contains read-only system configurations, which are specified
by your &lt;code class=&quot;highlighter-rouge&quot;&gt;configuration.nix&lt;/code&gt; and are built and tracked as system
generations. These never change. Once the files are created in &lt;code class=&quot;highlighter-rouge&quot;&gt;/nix&lt;/code&gt;,
the only way to change the config’s contents is to build a new system
configuration with the contents you want.&lt;/p&gt;

&lt;p&gt;Any configuration or files created on the drive outside of &lt;code class=&quot;highlighter-rouge&quot;&gt;/nix&lt;/code&gt; is
state and cruft. We can lose everything outside of &lt;code class=&quot;highlighter-rouge&quot;&gt;/nix&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;/boot&lt;/code&gt;
and have a healthy system. My technique is to explicitly opt in and
&lt;em&gt;choose&lt;/em&gt; which state is important, and only keep that.&lt;/p&gt;

&lt;p&gt;How this is possible comes down to the boot sequence.&lt;/p&gt;

&lt;p&gt;For NixOS, the bootloader follows the same basic steps as a standard
Linux distribution: the kernel starts with an initial ramdisk, and the
initial ramdisk mounts the system disks.&lt;/p&gt;

&lt;p&gt;And here is where the similarities end.&lt;/p&gt;

&lt;h3 id=&quot;nixoss-early-startup&quot;&gt;NixOS’s early startup&lt;/h3&gt;

&lt;p&gt;NixOS configures the bootloader to pass some extra information: a
specific system configuration. This is the secret to NixOS’s
bootloader rollbacks, and also the key to erasing our disk on each
boot. The parameter is named &lt;code class=&quot;highlighter-rouge&quot;&gt;systemConfig&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;On every startup the very early boot stage knows what the system’s
configuration should be: the entire system configuration is stored in
the read-only &lt;code class=&quot;highlighter-rouge&quot;&gt;/nix/store&lt;/code&gt;, and the directory passed through
&lt;code class=&quot;highlighter-rouge&quot;&gt;systemConfig&lt;/code&gt; has a reference to the config. Early boot then
manipulates &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;/run&lt;/code&gt; to match the chosen setup. Usually this
involves swapping out a few symlinks.&lt;/p&gt;

&lt;p&gt;If &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt; simply doesn’t exist, however, early boot &lt;em&gt;creates&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt;
and moves on like it were any other boot. It also &lt;em&gt;creates&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;/var&lt;/code&gt;,
&lt;code class=&quot;highlighter-rouge&quot;&gt;/dev&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;/home&lt;/code&gt;, and any other core directories that must be present.&lt;/p&gt;

&lt;p&gt;Simply speaking, an empty &lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt; is &lt;em&gt;not surprising&lt;/em&gt; to NixOS. In fact,
the NixOS netboot, EC2, and installation media all start out this way.&lt;/p&gt;

&lt;h2 id=&quot;opting-out&quot;&gt;Opting out&lt;/h2&gt;

&lt;p&gt;Before we can opt in to saving data, we must opt out of saving data
&lt;em&gt;by default&lt;/em&gt;. I do this by setting up my filesystem in a way that
lets me easily and safely erase the unwanted data, while preserving
the data I do want to keep.&lt;/p&gt;

&lt;p&gt;My preferred method for this is using a ZFS dataset and rolling it
back to a blank snapshot before it is mounted. A partition of any
other filesystem would work just as well too, running &lt;code class=&quot;highlighter-rouge&quot;&gt;mkfs&lt;/code&gt; at boot,
or something similar. If you have a lot of RAM, you could skip the
erase step and make &lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt; a tmpfs.&lt;/p&gt;

&lt;h3 id=&quot;opting-out-with-zfs&quot;&gt;Opting out with ZFS&lt;/h3&gt;
&lt;p&gt;When installing NixOS, I partition my disk with two partitions, one
for the boot partition, and another for a ZFS pool. Then I create and
mount a few datasets.&lt;/p&gt;

&lt;p&gt;My root dataset:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# zfs create -p -o mountpoint=legacy rpool/local/root
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Before I even mount it, I &lt;strong&gt;create a snapshot while it is totally
blank&lt;/strong&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# zfs snapshot rpool/local/root@blank
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then mount it:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mount -t zfs rpool/local/root /mnt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then I mount the partition I created for the &lt;code class=&quot;highlighter-rouge&quot;&gt;/boot&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mkdir /mnt/boot
# mount /dev/the-boot-partition /mnt/boot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Create and mount a dataset for &lt;code class=&quot;highlighter-rouge&quot;&gt;/nix&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# zfs create -p -o mountpoint=legacy rpool/local/nix
# mkdir /mnt/nix
# mount -t zfs rpool/local/nix /mnt/nix
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And a dataset for &lt;code class=&quot;highlighter-rouge&quot;&gt;/home&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# zfs create -p -o mountpoint=legacy rpool/safe/home
# mkdir /mnt/home
# mount -t zfs rpool/safe/home /mnt/home
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And finally, a dataset explicitly for state I want to persist between
boots:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# zfs create -p -o mountpoint=legacy rpool/safe/persist
# mkdir /mnt/persist
# mount -t zfs rpool/safe/persist /mnt/persist
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; in my systems, datasets under &lt;code class=&quot;highlighter-rouge&quot;&gt;rpool/local&lt;/code&gt; are never backed
up, and datasets under &lt;code class=&quot;highlighter-rouge&quot;&gt;rpool/safe&lt;/code&gt; are.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And now safely erasing the root dataset on each boot is very easy:
after devices are made available, roll back to the blank snapshot:&lt;/p&gt;

&lt;div class=&quot;language-nix highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;boot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;initrd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;postDeviceCommands&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;mkAfter&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;    zfs rollback -r rpool/local/root@blank&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;  ''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I then finish the installation as normal. If all goes well, your
next boot will start with an empty root partition but otherwise be
configured exactly as you specified.&lt;/p&gt;

&lt;h2 id=&quot;opting-in&quot;&gt;Opting in&lt;/h2&gt;

&lt;p&gt;Now that I’m keeping no state, it is time to specify what I do want
to keep. My choices here are different based on the role of the
system: a laptop has different state than a server.&lt;/p&gt;

&lt;p&gt;Here are some different pieces of state and how I preserve them. These
examples largely use reconfiguration or symlinks, but using ZFS
datasets and mount points would work too.&lt;/p&gt;

&lt;h4 id=&quot;wireguard-private-keys&quot;&gt;Wireguard private keys&lt;/h4&gt;

&lt;p&gt;Create a directory under &lt;code class=&quot;highlighter-rouge&quot;&gt;/persist&lt;/code&gt; for the key:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mkdir -p /persist/etc/wireguard/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And use Nix’s wireguard module to generate the key there:&lt;/p&gt;

&lt;div class=&quot;language-nix highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;networking&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;wireguard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;interfaces&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;wg0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;generatePrivateKeyFile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;privateKeyFile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/persist/etc/wireguard/wg0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;networkmanager-connections&quot;&gt;NetworkManager connections&lt;/h4&gt;

&lt;p&gt;Create a directory under &lt;code class=&quot;highlighter-rouge&quot;&gt;/persist&lt;/code&gt;, mirroring the &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt; structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mkdir -p /persist/etc/NetworkManager/system-connections
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And use Nix’s &lt;code class=&quot;highlighter-rouge&quot;&gt;etc&lt;/code&gt; module to set up the symlink:&lt;/p&gt;

&lt;div class=&quot;language-nix highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;etc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;NetworkManager/system-connections&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/persist/etc/NetworkManager/system-connections/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;bluetooth-devices&quot;&gt;Bluetooth devices&lt;/h4&gt;

&lt;p&gt;Create a directory under &lt;code class=&quot;highlighter-rouge&quot;&gt;/persist&lt;/code&gt;, mirroring the &lt;code class=&quot;highlighter-rouge&quot;&gt;/var&lt;/code&gt; structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mkdir -p /persist/var/lib/bluetooth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then use systemd’s tmpfiles.d rules to create a symlink from
&lt;code class=&quot;highlighter-rouge&quot;&gt;/var/lib/bluetooth&lt;/code&gt; to my persisted directory:&lt;/p&gt;

&lt;div class=&quot;language-nix highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;systemd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;tmpfiles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;rules&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;L /var/lib/bluetooth - - - - /persist/var/lib/bluetooth&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;ssh-host-keys&quot;&gt;SSH host keys&lt;/h4&gt;

&lt;p&gt;Create a directory under &lt;code class=&quot;highlighter-rouge&quot;&gt;/persist&lt;/code&gt;, mirroring the &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc&lt;/code&gt; structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mkdir -p /persist/etc/ssh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And use Nix’s openssh module to create and use the keys in that
directory:&lt;/p&gt;

&lt;div class=&quot;language-nix highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;openssh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;enable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;hostKeys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/persist/ssh/ssh_host_ed25519_key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;ed25519&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;/persist/ssh/ssh_host_rsa_key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;nv&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;acme-certificates&quot;&gt;ACME certificates&lt;/h4&gt;

&lt;p&gt;Create a directory under &lt;code class=&quot;highlighter-rouge&quot;&gt;/persist&lt;/code&gt;, mirroring the &lt;code class=&quot;highlighter-rouge&quot;&gt;/var&lt;/code&gt; structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# mkdir -p /persist/var/lib/acme
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then use systemd’s tmpfiles.d rules to create a symlink from
&lt;code class=&quot;highlighter-rouge&quot;&gt;/var/lib/acme&lt;/code&gt; to my persisted directory:&lt;/p&gt;

&lt;div class=&quot;language-nix highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;systemd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;tmpfiles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;rules&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;L /var/lib/acme - - - - /persist/var/lib/acme&quot;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;answering-the-question-what-am-i-about-to-lose&quot;&gt;Answering the question “what am I about to lose?”&lt;/h3&gt;

&lt;p&gt;I found this process a bit scary for the first few weeks: was I losing
important data each reboot? No, I wasn’t.&lt;/p&gt;

&lt;p&gt;If you’re worried and want to know what state you’ll lose on the next
boot, you can list the files on your root filesystem and see if you’re
missing something important:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# tree -x /
├── bin
│   └── sh -&amp;gt; /nix/store/97zzcs494vn5k2yw-dash-0.5.10.2/bin/dash
├── boot
├── dev
├── etc
│   ├── asound.conf -&amp;gt; /etc/static/asound.conf
... snip ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ZFS can give you a similar answer:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# zfs diff rpool/local/root@blank
M	/
+	/nix
+	/etc
+	/root
+	/var/lib/is-nix-channel-up-to-date
+	/etc/pki/fwupd
+	/etc/pki/fwupd-metadata
... snip ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;your-stateless-future&quot;&gt;Your stateless future&lt;/h2&gt;

&lt;p&gt;You may bump in to new state you meant to be preserving. When I’m
adding new services, I think about the state it is writing and whether
I care about it or not. If I care, I find a way to redirect its state
to &lt;code class=&quot;highlighter-rouge&quot;&gt;/persist&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Take care to reboot these machines on a somewhat regular basis. It
will keep things agile, proving your system state is tracked
correctly.&lt;/p&gt;

&lt;p&gt;This technique has given me the “new computer smell” on every boot
without the datacenter full of hardware, and even on systems that do
carry important state. I have deployed this strategy to systems in the
large and small: build farm servers, database servers, my NAS and home
server, my raspberry pi garage door opener, and laptops.&lt;/p&gt;

&lt;p&gt;NixOS enables powerful new deployment models in so many ways, allowing
for systems of all shapes and sizes to be managed properly and
consistently. I think this model of ephemeral roots is yet
another example of this flexibility and power. I would like to see
this partitioning scheme become a reference architecture and take us
out of this eternal tarpit of legacy.&lt;/p&gt;</description>
	<pubDate>Mon, 13 Apr 2020 00:00:00 +0000</pubDate>
</item>

</channel>
</rss>
